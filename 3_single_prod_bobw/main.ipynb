{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND DEPENDENCIES\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np                    # Numerical computations and arrays\n",
    "import random                        # Random number generation\n",
    "import matplotlib.pyplot as plt      # Plotting and visualization\n",
    "import scipy.stats as stats          # Statistical distributions and functions\n",
    "from scipy import optimize          # Optimization algorithms\n",
    "from collections import Counter     # For counting frequency distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331bc84",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT: NON-STATIONARY STOCHASTIC PRICING\n",
    "# =============================================================================\n",
    "\n",
    "class NonStationaryStochasticPricingEnvironment:\n",
    "    \"\"\"\n",
    "    Non-stationary stochastic environment for dynamic pricing problems.\n",
    "    \n",
    "    Customer valuations follow time-varying distributions that change every round,\n",
    "    creating a challenging online learning scenario where the agent must adapt\n",
    "    to shifting customer preferences while managing inventory constraints.\n",
    "    \n",
    "    Key Features:\n",
    "    - Non-stationary customer valuations (distributions change over time)\n",
    "    - Stochastic demand with configurable noise\n",
    "    - Supports discrete price sets\n",
    "    - Tracks round progression automatically\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, valuation_distributions, prices, demand_noise_std=0.03):\n",
    "        \"\"\"\n",
    "        Initialize the non-stationary pricing environment.\n",
    "        \n",
    "        Args:\n",
    "            valuation_distributions: List of scipy.stats distributions representing \n",
    "                                   customer valuations for each round\n",
    "            prices: Array of available discrete price options\n",
    "            demand_noise_std: Standard deviation of noise in demand probability (default: 0.03)\n",
    "        \"\"\"\n",
    "        self.valuation_dist = valuation_distributions\n",
    "        self.noise_std = demand_noise_std\n",
    "        self.current_round = 0\n",
    "        self.prices = prices\n",
    "\n",
    "    def demand_probability(self, price):\n",
    "        \"\"\"\n",
    "        Calculate the probability that a customer purchases at the given price.\n",
    "        \n",
    "        The probability is computed as P(customer_valuation >= price) plus noise.\n",
    "        This represents customers buying when their willingness-to-pay exceeds price.\n",
    "        \n",
    "        Args:\n",
    "            price: The price at which to evaluate demand probability\n",
    "            \n",
    "        Returns:\n",
    "            float: Purchase probability in [0, 1]\n",
    "        \"\"\"\n",
    "        # Get current round's valuation distribution\n",
    "        current_dist = self.valuation_dist[self.current_round]\n",
    "\n",
    "        # Base probability: P(valuation >= price)\n",
    "        base_prob = 1 - current_dist.cdf(price)\n",
    "        \n",
    "        # Add stochastic noise to create realistic demand uncertainty\n",
    "        noise = np.random.normal(0, self.noise_std)\n",
    "        \n",
    "        # Ensure probability remains in valid range [0, 1]\n",
    "        prob = np.clip(base_prob + noise, 0, 1)\n",
    "        return prob\n",
    "\n",
    "    def simulate_round(self, price):\n",
    "        \"\"\"\n",
    "        Simulate one pricing round with a customer interaction.\n",
    "        \n",
    "        This method:\n",
    "        1. Draws a random customer valuation from current round's distribution\n",
    "        2. Determines if customer purchases (valuation >= price)\n",
    "        3. Calculates revenue from the interaction\n",
    "        4. Computes counterfactual outcomes for all possible prices\n",
    "        5. Advances to next round\n",
    "        \n",
    "        Args:\n",
    "            price: The price offered to the customer\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (sale_made, revenue, sale_made_full, revenue_full, valuation)\n",
    "                - sale_made: Binary indicator if sale occurred\n",
    "                - revenue: Revenue from this transaction\n",
    "                - sale_made_full: Array of sale indicators for all prices\n",
    "                - revenue_full: Array of revenues for all prices\n",
    "                - valuation: The actual customer valuation sampled\n",
    "        \"\"\"\n",
    "        # Get current round's valuation distribution\n",
    "        current_dist = self.valuation_dist[self.current_round]\n",
    "\n",
    "        # Sample customer valuation from current distribution\n",
    "        valuation = current_dist.rvs()\n",
    "        \n",
    "        # Customer purchases if their valuation >= offered price\n",
    "        sale_made = 1 if valuation >= price else 0\n",
    "        \n",
    "        # Revenue is price if sale was made, 0 otherwise\n",
    "        revenue = sale_made * price\n",
    "        \n",
    "        # Compute counterfactual outcomes for all possible prices\n",
    "        # This helps the agent understand what would have happened\n",
    "        sale_made_full = (valuation >= self.prices).astype(int)\n",
    "        revenue_full = self.prices * sale_made_full\n",
    "        \n",
    "        # Advance to next round (important for non-stationarity)\n",
    "        self.current_round += 1\n",
    "\n",
    "        return sale_made, revenue, sale_made_full, revenue_full, valuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa861eb",
   "metadata": {},
   "source": [
    "# Environment Configuration Notes\n",
    "\n",
    "The environment uses different probability distributions for each round to create a non-stationary setting where customer valuations change over time. This creates a challenging online learning scenario.\n",
    "\n",
    "When defining the environment configuration, a lambda function is used for the mean and standard deviation. A vector of distributions is created to be passed to the environment. It may even make sense to change the distribution (and not just make them normal), to create a sharper change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88e9a9",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eedac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HEDGE ALGORITHM: EXPERT-BASED ONLINE LEARNING\n",
    "# =============================================================================\n",
    "\n",
    "class HedgeAgent:\n",
    "    \"\"\"\n",
    "    Implementation of the Hedge algorithm for online learning with expert advice.\n",
    "    \n",
    "    The Hedge algorithm maintains weights over a set of experts (actions) and\n",
    "    updates these weights based on observed losses. It provides strong theoretical\n",
    "    guarantees for regret minimization in adversarial settings.\n",
    "    \n",
    "    Key Properties:\n",
    "    - Exponentially weighted average of experts\n",
    "    - Regret bound: O(‚àöT log K) where K is number of experts, T is time horizon\n",
    "    - Robust to adversarial losses\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, K, learning_rate):\n",
    "        \"\"\"\n",
    "        Initialize the Hedge algorithm.\n",
    "        \n",
    "        Args:\n",
    "            K: Number of experts (actions/arms)\n",
    "            learning_rate: Step size for weight updates (typically O(‚àö(log K / T)))\n",
    "        \"\"\"\n",
    "        \n",
    "        self.K = K                                   # Number of experts/actions\n",
    "        self.learning_rate = learning_rate           # Learning rate parameter\n",
    "        self.weights = np.ones(K, dtype=float)       # Expert weights (start uniform)\n",
    "        self.x_t = np.ones(K, dtype=float) / K       # Probability distribution over experts\n",
    "        self.a_t = None                              # Last selected action\n",
    "        self.t = 0                                   # Current time step\n",
    "\n",
    "    def pull_arm(self):\n",
    "        \"\"\"\n",
    "        Select an expert (action) according to current probability distribution.\n",
    "        \n",
    "        Uses the exponentially weighted average strategy where experts with\n",
    "        lower cumulative losses receive higher selection probabilities.\n",
    "        \n",
    "        Returns:\n",
    "            int: Index of selected expert/action\n",
    "        \"\"\"\n",
    "        # Compute probability distribution from current weights\n",
    "        self.x_t = self.weights / np.sum(self.weights)\n",
    "        \n",
    "        # Sample action according to probability distribution\n",
    "        self.a_t = np.random.choice(np.arange(self.K), p=self.x_t)\n",
    "        \n",
    "        return self.a_t\n",
    "    \n",
    "    def update(self, l_t):\n",
    "        \"\"\"\n",
    "        Update expert weights based on observed losses.\n",
    "        \n",
    "        Uses exponential update rule: w_{i,t+1} = w_{i,t} * exp(-Œ∑ * l_{i,t})\n",
    "        where Œ∑ is the learning rate and l_{i,t} is the loss of expert i at time t.\n",
    "        \n",
    "        Args:\n",
    "            l_t: Array of losses for each expert at current time step\n",
    "        \"\"\"\n",
    "        # Exponential weight update: experts with higher losses get lower weights\n",
    "        self.weights *= np.exp(-self.learning_rate * l_t)\n",
    "        \n",
    "        # Increment time counter\n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452010fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXP3 ALGORITHM: EXPERT-BASED ONLINE LEARNING\n",
    "# =============================================================================\n",
    "\n",
    "class Exp3Agent:\n",
    "    \"\"\"\n",
    "    Implementation of the EXP3 algorithm for online learning with expert advice.\n",
    "    \n",
    "    The EXP3 algorithm is designed for adversarial multi-armed bandit problems,\n",
    "    where the agent only observes the loss of the chosen action. It combines\n",
    "    exploration and exploitation using a probability distribution over actions.\n",
    "    \n",
    "    Key Properties:\n",
    "    - Exponentially weighted average of experts with exploration\n",
    "    - Regret bound: O(‚àö(T K log K)) where K is number of experts, T is time horizon\n",
    "    - Suitable for adversarial settings with partial feedback\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, K, learning_rate, exploration_rate = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the EXP3 algorithm.\n",
    "        \n",
    "        Args:\n",
    "            K: Number of experts (actions/arms)\n",
    "            learning_rate: Step size for weight updates (typically O(‚àö(log K / (K T))))\n",
    "            exploration_rate: Probability of exploring random actions (typically O(‚àö(K log K / T)))\n",
    "        \"\"\"\n",
    "        \n",
    "        self.K = K                                   # Number of experts/actions\n",
    "        self.learning_rate = learning_rate           # Learning rate parameter\n",
    "        self.exploration_rate = exploration_rate     # Exploration parameter\n",
    "        self.weights = np.ones(K, dtype=float)       # Expert weights (start uniform)\n",
    "        self.x_t = np.ones(K, dtype=float) / K       # Probability distribution over experts\n",
    "        self.a_t = None                              # Last selected action\n",
    "        self.t = 0                                   # Current time step\n",
    "        self.prob_distribution = None                # Current probability distribution\n",
    "\n",
    "    def pull_arm(self):\n",
    "        \"\"\"\n",
    "        Select an expert (action) according to current probability distribution.\n",
    "        \n",
    "        Combines exploitation (based on weights) and exploration (uniform random).\n",
    "        \n",
    "        Returns:\n",
    "            int: Index of selected expert/action\n",
    "        \"\"\"\n",
    "        # Compute probability distribution with exploration\n",
    "        self.prob_distribution = ((1 - self.exploration_rate) * \n",
    "                             (self.weights / np.sum(self.weights)) + \n",
    "                             (self.exploration_rate / self.K))\n",
    "        \n",
    "        # Sample action according to probability distribution\n",
    "        self.a_t = np.random.choice(np.arange(self.K), p=self.prob_distribution)\n",
    "        \n",
    "        return self.a_t\n",
    "    \n",
    "    def update(self, losses):\n",
    "        \"\"\"\n",
    "        Update expert weights based on observed loss from chosen action.\n",
    "        \n",
    "        Uses importance-weighted loss to estimate losses for all experts.\n",
    "        \n",
    "        Args:\n",
    "            losses: Losses received from the chosen action\n",
    "        \"\"\"\n",
    "        # Estimate loss for chosen action using importance weighting\n",
    "        estimated_loss = np.zeros(self.K)\n",
    "        \n",
    "        # Importance-weighted loss estimate for the chosen action\n",
    "        estimated_loss[self.a_t] = losses[self.a_t] / self.prob_distribution[self.a_t]\n",
    "        \n",
    "        # Exponential weight update: experts with higher estimated losses get lower weights\n",
    "        self.weights *= np.exp(-self.learning_rate * estimated_loss)\n",
    "        \n",
    "        # Increment time counter\n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f9f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRIMAL-DUAL AGENT: ONLINE LEARNING WITH CONSTRAINTS\n",
    "# =============================================================================\n",
    "\n",
    "class PrimalDualAgent:\n",
    "    \"\"\"\n",
    "    Primal-Dual agent for dynamic pricing with inventory constraints.\n",
    "    \n",
    "    This agent combines the Hedge algorithm with Lagrangian multipliers to handle\n",
    "    constrained online optimization problems. It's particularly suited for:\n",
    "    - Non-stationary environments\n",
    "    - Resource/inventory constraints\n",
    "    - Revenue maximization under uncertainty\n",
    "    \n",
    "    Algorithm Overview:\n",
    "    1. Use Hedge for exploration over price options\n",
    "    2. Maintain Lagrangian multiplier for inventory constraint\n",
    "    3. Construct Lagrangian: L = revenue - Œª * (sales_rate - target_rate)\n",
    "    4. Update both Hedge weights and multiplier based on feedback\n",
    "    \n",
    "    Theoretical Guarantees:\n",
    "    - Regret bound: O(‚àöT log K) for stationary environments\n",
    "    - Constraint violation: O(‚àöT) on average\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prices, valuation, P, T, eta, algorithm = 'exp3'):\n",
    "        \"\"\"\n",
    "        Initialize the Primal-Dual agent.\n",
    "        \n",
    "        Args:\n",
    "            prices: Array of available price options\n",
    "            valuation: Customer valuation distributions (for compatibility)\n",
    "            P: Total inventory (budget constraint)\n",
    "            T: Time horizon (total number of rounds)\n",
    "            eta: Learning rate for Lagrangian multiplier updates\n",
    "        \"\"\"\n",
    "        # Basic parameters\n",
    "        self.prices = prices                         # Available price options\n",
    "        self.K = len(prices)                        # Number of price options\n",
    "        self.valuation = valuation                  # Valuation distributions (compatibility)\n",
    "        self.P = P                                  # Total inventory constraint\n",
    "        self.T = T                                  # Time horizon\n",
    "        self.eta = eta                              # Learning rate for multiplier\n",
    "        \n",
    "        # Constraint management\n",
    "        self.rho = self.P / self.T                 # Target selling rate (œÅ = P/T)\n",
    "        self.lmbd = 1                              # Lagrangian multiplier (Œª)\n",
    "        \n",
    "        # Online learning components\n",
    "        hedge_lr = np.sqrt(np.log(self.K) / T)     # Optimal Hedge learning rate\n",
    "        exp3_lr = np.sqrt( np.log(self.K) / (self.K * T))  # Optimal EXP3 learning rate\n",
    "        self.hedge = HedgeAgent(self.K, hedge_lr)  # Hedge algorithm for exploration\n",
    "        self.exp3 = Exp3Agent(self.K, exp3_lr)  # EXP3 algorithm for exploration\n",
    "        \n",
    "        # State tracking\n",
    "        self.t = 0                                 # Current time step\n",
    "        self.remaining_inventory = P               # Inventory remaining\n",
    "        self.bid_index = 0                         # Last selected price index\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.N_pulls = np.zeros(len(prices))       # Number of times each price used\n",
    "        self.reward = np.zeros(self.K)             # Cumulative rewards per price\n",
    "\n",
    "        # Choosen algorithm\n",
    "        self.algorithm = algorithm                  # Algorithm to use: 'hedge' or 'exp3\n",
    "        \n",
    "        # History for analysis\n",
    "        self.history = {\n",
    "            'prices': [],        # Selected prices over time\n",
    "            'rewards': [],       # Rewards received\n",
    "            'purchases': [],     # Purchase indicators\n",
    "            'inventory': []      # Remaining inventory levels\n",
    "        }\n",
    "\n",
    "    def bid(self):\n",
    "        \"\"\"\n",
    "        Select a price using the Hedge algorithm with inventory awareness.\n",
    "        \n",
    "        If inventory is exhausted, automatically selects lowest price (index 0)\n",
    "        to minimize regret while avoiding further sales.\n",
    "        \n",
    "        Returns:\n",
    "            float: Selected price value\n",
    "        \"\"\"\n",
    "        # If no inventory remaining, select lowest price to avoid sales\n",
    "        if self.remaining_inventory < 1:\n",
    "            self.bid_index = 0\n",
    "            return self.prices[0]\n",
    "        \n",
    "        # Use Hedge algorithm to select price index\n",
    "        if(self.algorithm == 'hedge'):\n",
    "            self.bid_index = self.hedge.pull_arm()\n",
    "        else:\n",
    "            self.bid_index = self.exp3.pull_arm()\n",
    "            \n",
    "        return self.prices[self.bid_index]\n",
    "    \n",
    "    def update(self, f_t, c_t, f_t_full, c_t_full):\n",
    "        \"\"\"\n",
    "        Update the agent based on observed outcomes using primal-dual approach.\n",
    "        \n",
    "        This method implements the core primal-dual update:\n",
    "        1. Construct Lagrangian for all actions: L_i = f_i - Œª * (c_i - œÅ)\n",
    "        2. Update Hedge algorithm with normalized Lagrangian\n",
    "        3. Update Lagrangian multiplier based on constraint violation\n",
    "        4. Update inventory and statistics\n",
    "        \n",
    "        Args:\n",
    "            f_t: Observed revenue for selected action\n",
    "            c_t: Observed purchase indicator for selected action  \n",
    "            f_t_full: Expected revenues for all actions (counterfactual)\n",
    "            c_t_full: Expected purchase probabilities for all actions\n",
    "        \"\"\"\n",
    "        # Step 1: Construct Lagrangian for all price options\n",
    "        # L_i = revenue_i - Œª * (sales_rate_i - target_rate)\n",
    "        # This balances revenue maximization with constraint satisfaction\n",
    "        L_full = f_t_full - self.lmbd * (c_t_full - self.rho)\n",
    "        \n",
    "        # Step 2: Normalize Lagrangian to [0,1] for Hedge algorithm\n",
    "        # Hedge expects losses in [0,1], so we need proper normalization\n",
    "        max_possible_revenue = np.max(self.prices)\n",
    "        min_lagrangian = -self.lmbd  # When f=0, c=1 (worst case)\n",
    "        max_lagrangian = max_possible_revenue  # When f=max_price, c=0 (best case)\n",
    "        \n",
    "        # Avoid division by zero in edge cases\n",
    "        if max_lagrangian > min_lagrangian:\n",
    "            normalized_L = (L_full - min_lagrangian) / (max_lagrangian - min_lagrangian)\n",
    "        else:\n",
    "            normalized_L = np.ones_like(L_full) * 0.5\n",
    "            \n",
    "        # Step 3: Update Hedge with losses (1 - normalized_L since we maximize)\n",
    "        losses = 1 - normalized_L\n",
    "        if(self.algorithm == 'hedge'):\n",
    "            self.hedge.update(losses)\n",
    "        else:   \n",
    "            self.exp3.update(losses)\n",
    "\n",
    "        # Step 4: Update Lagrangian multiplier using projected gradient ascent\n",
    "        # Œª_{t+1} = Proj_{[0, 1/œÅ]}(Œª_t - Œ∑ * (œÅ - c_t))\n",
    "        # The projection ensures Œª stays in feasible range\n",
    "        constraint_violation = self.rho - c_t  # Positive if under-selling\n",
    "        self.lmbd = np.clip(\n",
    "            self.lmbd - self.eta * constraint_violation, \n",
    "            a_min=0,                    # Œª ‚â• 0 (dual feasibility)\n",
    "            a_max=1/self.rho           # Œª ‚â§ 1/œÅ (reasonable upper bound)\n",
    "        )\n",
    "        \n",
    "        # Step 5: Update inventory and performance tracking\n",
    "        self.remaining_inventory -= c_t\n",
    "        self.N_pulls[self.bid_index] += 1\n",
    "        self.reward += f_t_full  # Accumulate counterfactual rewards\n",
    "\n",
    "        # Step 6: Record history for analysis\n",
    "        self.history['prices'].append(self.prices[self.bid_index])\n",
    "        self.history['rewards'].append(f_t)\n",
    "        self.history['purchases'].append(c_t)\n",
    "        self.history['inventory'].append(self.remaining_inventory)\n",
    "\n",
    "        # Increment time counter\n",
    "        self.t += 1\n",
    "    \n",
    "    def get_reward(self):\n",
    "        \"\"\"Get cumulative rewards for all actions.\"\"\"\n",
    "        return self.reward\n",
    "    \n",
    "    def get_argmax_reward(self):\n",
    "        \"\"\"Get the index of the best-performing price.\"\"\"\n",
    "        return np.argmax(self.reward)\n",
    "    \n",
    "    def get_max_reward(self):\n",
    "        \"\"\"Get the maximum cumulative reward achieved.\"\"\"\n",
    "        best_idx = np.argmax(self.reward)\n",
    "        return self.reward[best_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d676aad",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8596f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIMULATION RUNNER: PRIMAL-DUAL PRICING EXPERIMENT\n",
    "# =============================================================================\n",
    "\n",
    "def run_simulator(T, valuation_dist, env_config, agent_params, n_simulations=1, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the Primal-Dual pricing simulation with comprehensive tracking.\n",
    "    \n",
    "    This function orchestrates the interaction between the non-stationary environment\n",
    "    and the primal-dual agent over T rounds, tracking all relevant metrics for\n",
    "    performance analysis and regret calculation.\n",
    "    \n",
    "    Args:\n",
    "        T: Number of rounds to simulate\n",
    "        valuation_dist: List of customer valuation distributions (one per round)\n",
    "        env_config: Environment configuration parameters\n",
    "        agent_params: Agent initialization parameters\n",
    "        n_simulations: Number of independent simulation runs (default: 1)\n",
    "        verbose: Whether to print progress and statistics\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - selected_prices: List of prices chosen by agent each round\n",
    "            - revenues: List of revenues earned each round\n",
    "            - sales: List of binary sale indicators each round\n",
    "            - cumulative_revenue: List of cumulative revenue over time\n",
    "            - best_price: Best performing price according to agent\n",
    "            - best_reward: Maximum cumulative reward achieved\n",
    "            - first_inventory_empty: Round when inventory first depleted (if any)\n",
    "            - total_revenue: Final total revenue\n",
    "            - agent: Trained agent instance for analysis\n",
    "            - actual_valuations: List of actual customer valuations observed\n",
    "    \"\"\"\n",
    "    # Initialize result containers\n",
    "    selected_prices = []\n",
    "    revenues = []\n",
    "    sales = []\n",
    "    cumulative_revenue = []\n",
    "    total_revenue = 0\n",
    "    best_price = []\n",
    "    best_reward = []\n",
    "    first_inventory_empty = None\n",
    "    actual_valuations = []  # NEW: Track actual customer valuations\n",
    "\n",
    "    for sim in range(n_simulations):\n",
    "        if verbose:\n",
    "            print(f\"\\n=== Running Primal-Dual Pricing Simulation #{sim + 1} ===\")\n",
    "            print(f\"Time horizon: {T:,} rounds\")\n",
    "            print(f\"Inventory constraint: {agent_params['P']:,} units\")\n",
    "            print(f\"Non-stationary environment: {len(valuation_dist):,} distributions\")\n",
    "\n",
    "        # Initialize environment and agent\n",
    "        env = NonStationaryStochasticPricingEnvironment(\n",
    "            valuation_distributions=valuation_dist,\n",
    "            prices=agent_params['prices']\n",
    "        )\n",
    "        agent = PrimalDualAgent(**agent_params)\n",
    "\n",
    "        # Main simulation loop\n",
    "        for t in range(T):\n",
    "            # Track inventory depletion\n",
    "            if agent.remaining_inventory < 1 and first_inventory_empty is None:\n",
    "                first_inventory_empty = t\n",
    "                if verbose and t % 1000 == 0:\n",
    "                    print(f\"Inventory empty for the first time at round {t}\")\n",
    "\n",
    "            # Agent selects price based on current state\n",
    "            price = agent.bid()\n",
    "            price_idx = agent.bid_index\n",
    "\n",
    "            # Simulate customer interaction in environment\n",
    "            if agent.remaining_inventory <= 0:\n",
    "                # No inventory: force no sale and zero revenue\n",
    "                sale_made = False\n",
    "                revenue = 0\n",
    "                # Still need counterfactuals for learning and valuation for fairness\n",
    "                _, _, sale_made_full, revenue_full, valuation = env.simulate_round(price)\n",
    "            else:\n",
    "                # Normal interaction: simulate customer decision\n",
    "                sale_made, revenue, sale_made_full, revenue_full, valuation = env.simulate_round(price)\n",
    "\n",
    "            # Store the actual customer valuation for fair theoretical optimum comparison\n",
    "            actual_valuations.append(valuation)\n",
    "\n",
    "            # Update agent with observed outcomes and counterfactuals\n",
    "            agent.update(revenue, sale_made, revenue_full, sale_made_full)\n",
    "\n",
    "            # Record results for analysis\n",
    "            selected_prices.append(price)\n",
    "            revenues.append(revenue)\n",
    "            sales.append(sale_made)\n",
    "            total_revenue += revenue\n",
    "            cumulative_revenue.append(total_revenue)\n",
    "\n",
    "            # Progress reporting\n",
    "            if verbose and (t + 1) % (T//10) == 0:\n",
    "                current_performance = total_revenue / (t + 1)\n",
    "                print(f\"Round {t + 1:,}: Avg revenue = {current_performance:.4f}, \"\n",
    "                      f\"Inventory = {agent.remaining_inventory}, \"\n",
    "                      f\"Sales rate = {sum(sales)/(t+1)*100:.1f}%\")\n",
    "\n",
    "        # Record final agent statistics\n",
    "        best_reward_value = agent.get_max_reward()\n",
    "        best_reward.append(best_reward_value)\n",
    "        best_price_value = agent_params['prices'][agent.get_argmax_reward()]\n",
    "        best_price.append(best_price_value)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== Simulation #{sim + 1} Results ===\")\n",
    "            print(f\"Total revenue: {total_revenue:.2f}\")\n",
    "            print(f\"Average revenue per round: {total_revenue / T:.6f}\")\n",
    "            print(f\"Total sales: {sum(sales):,} units\")\n",
    "            print(f\"Sales rate: {sum(sales)/T*100:.1f}%\")\n",
    "            print(f\"Inventory utilization: {(agent_params['P'] - agent.remaining_inventory)/agent_params['P']*100:.1f}%\")\n",
    "            print(f\"Agent's best price: {best_price[-1]:.3f}\")\n",
    "            print(f\"First inventory empty: {first_inventory_empty if first_inventory_empty else 'Never'}\")\n",
    "\n",
    "    return {\n",
    "        'selected_prices': selected_prices,\n",
    "        'revenues': revenues,\n",
    "        'sales': sales,\n",
    "        'cumulative_revenue': cumulative_revenue,\n",
    "        'best_price': best_price,\n",
    "        'best_reward': best_reward,\n",
    "        'first_inventory_empty': first_inventory_empty,\n",
    "        'total_revenue': total_revenue,\n",
    "        'agent': agent,\n",
    "        'actual_valuations': actual_valuations  # NEW: Include actual valuations\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e63f048",
   "metadata": {},
   "source": [
    "RUN THE SIMULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b77733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIMULATION CONFIGURATION AND EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "# Environment configuration parameters\n",
    "env_config = {\n",
    "    'valuation_mean': 0.5,        # Base mean customer valuation\n",
    "    'valuation_std': 0.05,        # Base standard deviation for valuations\n",
    "    'demand_noise_std': 0.005     # Noise in demand probability calculation\n",
    "}\n",
    "\n",
    "# Core simulation parameters\n",
    "T = 10000                        # Time horizon (number of rounds)\n",
    "inventory = int(T * 0.1)         # Initial inventory: 10% of time horizon\n",
    "eta = 0.1                        # Learning rate for Lagrangian multiplier\n",
    "\n",
    "print(\"=== SIMULATION CONFIGURATION ===\")\n",
    "print(f\"Time horizon (T): {T:,} rounds\")\n",
    "print(f\"Inventory constraint (P): {inventory:,} units\")\n",
    "print(f\"Inventory-to-time ratio: {inventory/T:.1%}\")\n",
    "print(f\"Lagrangian learning rate (Œ∑): {eta}\")\n",
    "\n",
    "# === CREATE NON-STATIONARY CUSTOMER VALUATIONS ===\n",
    "# Generate different customer preference distributions for each round\n",
    "# This creates the non-stationary environment where customer valuations shift over time\n",
    "\n",
    "np.random.seed(42)  # For reproducibility of experiments\n",
    "\n",
    "# Generate time-varying parameters for customer valuations\n",
    "means = np.random.uniform(0.4, 0.6, size=T)    # Mean valuations vary between 0.4-0.6\n",
    "stds = np.random.uniform(0.1, 0.2, size=T)     # Standard deviations vary between 0-0.1\n",
    "\n",
    "# Create list of normal distributions (one for each round)\n",
    "valuation_dists = [\n",
    "    stats.norm(loc=means[t], scale=stds[t]) \n",
    "    for t in range(T)\n",
    "]\n",
    "\n",
    "print(f\"\\nNon-stationarity characteristics:\")\n",
    "print(f\"Mean valuation range: [{np.min(means):.3f}, {np.max(means):.3f}]\")\n",
    "print(f\"Std deviation range: [{np.min(stds):.3f}, {np.max(stds):.3f}]\")\n",
    "print(f\"Distribution changes: Every round (maximum non-stationarity)\")\n",
    "\n",
    "# === AGENT CONFIGURATION ===\n",
    "# Define the set of discrete price options available to the agent\n",
    "price_options = np.arange(0.1, 0.9, 1/7)  # 7 evenly spaced prices from 0.1 to ~0.8\n",
    "\n",
    "agent_params = {\n",
    "    'prices': price_options,     # Available price options\n",
    "    'valuation': valuation_dists,  # Customer valuation distributions\n",
    "    'P': inventory,              # Inventory constraint\n",
    "    'T': T,                      # Time horizon\n",
    "    'eta': eta,                  # Learning rate for Lagrangian multiplier\n",
    "    'algorithm': 'exp3'          # Algorithm to use: 'hedge' or 'exp3'\n",
    "}\n",
    "\n",
    "print(f\"\\n=== AGENT CONFIGURATION ===\")\n",
    "print(f\"Number of price options: {len(agent_params['prices'])}\")\n",
    "print(f\"Price range: [{agent_params['prices'][0]:.3f}, {agent_params['prices'][-1]:.3f}]\")\n",
    "print(f\"Price granularity: {(agent_params['prices'][1] - agent_params['prices'][0]):.3f}\")\n",
    "print(f\"Target selling rate (œÅ): {inventory/T:.4f} units/round\")\n",
    "\n",
    "# Theoretical analysis preview\n",
    "hedge_lr = np.sqrt(np.log(np.size(price_options)) / T)\n",
    "print(f\"Hedge learning rate: {hedge_lr:.6f}\")\n",
    "print(f\"Expected regret bound: O(‚àöT log K) ‚âà {np.sqrt(T * np.log(np.size(price_options))):.0f}\")\n",
    "\n",
    "# === RUN SIMULATION ===\n",
    "print(f\"\\n=== EXECUTING SIMULATION ===\")\n",
    "print(\"Starting primal-dual pricing experiment...\")\n",
    "\n",
    "results = run_simulator(\n",
    "    T=T,\n",
    "    valuation_dist=valuation_dists,\n",
    "    env_config=env_config,\n",
    "    agent_params=agent_params,\n",
    "    n_simulations=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# === EXTRACT AND SUMMARIZE RESULTS ===\n",
    "selected_prices = results['selected_prices']\n",
    "revenues = results['revenues']\n",
    "sales = results['sales']\n",
    "cumulative_revenue = results['cumulative_revenue']\n",
    "best_price = results['best_price']\n",
    "first_inventory_empty = results['first_inventory_empty']\n",
    "total_revenue = results['total_revenue']\n",
    "best_reward = results['best_reward']\n",
    "agent = results['agent']\n",
    "\n",
    "print(f\"\\n=== FINAL SIMULATION SUMMARY ===\")\n",
    "print(f\"‚úì Simulation completed successfully\")\n",
    "print(f\"üìä Performance Metrics:\")\n",
    "print(f\"   ‚Ä¢ Total revenue: {total_revenue:.2f}\")\n",
    "print(f\"   ‚Ä¢ Average revenue per round: {total_revenue / T:.6f}\")\n",
    "print(f\"   ‚Ä¢ Total units sold: {sum(sales):,}\")\n",
    "print(f\"   ‚Ä¢ Sales rate: {sum(sales)/T*100:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Final inventory: {agent.remaining_inventory:,} units\")\n",
    "print(f\"   ‚Ä¢ Inventory utilization: {(inventory - agent.remaining_inventory)/inventory*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüéØ Agent Learning Results:\")\n",
    "print(f\"   ‚Ä¢ Best price discovered: {best_price[0]:.3f}\")\n",
    "print(f\"   ‚Ä¢ Inventory depleted at round: {first_inventory_empty if first_inventory_empty else 'Never'}\")\n",
    "print(f\"   ‚Ä¢ Constraint management: {'‚úì Successful' if agent.remaining_inventory >= 0 else '‚ö† Violated'}\")\n",
    "\n",
    "print(f\"\\nüìà Ready for analysis and visualization...\")\n",
    "print(f\"   ‚Ä¢ Use plot_results() for comprehensive analysis\")\n",
    "print(f\"   ‚Ä¢ Check regret calculation with corrected theoretical baseline\")\n",
    "print(f\"   ‚Ä¢ Compare against theoretical bounds and optimal policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd9286",
   "metadata": {},
   "source": [
    "PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9196ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# THEORETICAL OPTIMUM CALCULATION WITH INVENTORY CONSTRAINTS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_correct_theoretical_optimum(agent_params, valuation_dist, T, actual_valuations=None):\n",
    "    \"\"\"\n",
    "    Compute optimal policy that manages budget dynamically over time.\n",
    "    Uses a threshold-based approach that considers inventory scarcity.\n",
    "    \"\"\"\n",
    "    prices = agent_params['prices']\n",
    "    P = agent_params['P']\n",
    "    \n",
    "    # Get actual valuations\n",
    "    if actual_valuations is None:\n",
    "        # Use same random seed as simulation for fair comparison\n",
    "        np.random.seed(42)\n",
    "        actual_valuations = []\n",
    "        for t in range(T):\n",
    "            current_dist = valuation_dist[t]\n",
    "            valuation = current_dist.rvs()\n",
    "            actual_valuations.append(valuation)\n",
    "    \n",
    "    # Dynamic threshold based on remaining inventory ratio\n",
    "    selling_decisions = []\n",
    "    inventory_remaining = P\n",
    "    \n",
    "    for t in range(T):\n",
    "        remaining_rounds = T - t\n",
    "        customer_valuation = actual_valuations[t]\n",
    "        \n",
    "        # Dynamic threshold: higher when inventory is scarce\n",
    "        inventory_ratio = inventory_remaining / max(remaining_rounds, 1)\n",
    "        \n",
    "        # Threshold increases as inventory becomes relatively scarce\n",
    "        if inventory_ratio < 0.1:  # Scarce inventory\n",
    "            threshold = np.percentile([v for v in actual_valuations[t:]], 80)\n",
    "        elif inventory_ratio < 0.5:  # Moderate inventory\n",
    "            threshold = np.percentile([v for v in actual_valuations[t:]], 60)\n",
    "        else:  # Abundant inventory\n",
    "            threshold = np.percentile([v for v in actual_valuations[t:]], 40)\n",
    "        \n",
    "        # Decision: sell only if customer valuation exceeds threshold AND we have inventory\n",
    "        if inventory_remaining > 0 and customer_valuation >= threshold:\n",
    "            # Find best price for this customer\n",
    "            valid_prices = [p for p in prices if p <= customer_valuation]\n",
    "            if valid_prices:\n",
    "                best_price = max(valid_prices)\n",
    "                selling_decisions.append({\n",
    "                    'round': t,\n",
    "                    'price': best_price,\n",
    "                    'revenue': best_price,\n",
    "                    'sell': True\n",
    "                })\n",
    "                inventory_remaining -= 1\n",
    "            else:\n",
    "                selling_decisions.append({\n",
    "                    'round': t,\n",
    "                    'price': 0,\n",
    "                    'revenue': 0,\n",
    "                    'sell': False\n",
    "                })\n",
    "        else:\n",
    "            selling_decisions.append({\n",
    "                'round': t,\n",
    "                'price': 0,\n",
    "                'revenue': 0,\n",
    "                'sell': False\n",
    "            })\n",
    "    \n",
    "    # Convert to expected format for plotting code\n",
    "    total_optimal_revenue = sum([decision['revenue'] for decision in selling_decisions])\n",
    "    optimal_revenue_per_round = total_optimal_revenue / T\n",
    "    \n",
    "    # Create per-round revenue list\n",
    "    opt_revenues_per_round = [decision['revenue'] for decision in selling_decisions]\n",
    "    \n",
    "    # Compute optimal price distribution among selected opportunities\n",
    "    opt_dist = np.zeros(len(prices))\n",
    "    selected_opportunities = [decision for decision in selling_decisions if decision['sell']]\n",
    "    \n",
    "    for decision in selected_opportunities:\n",
    "        if decision['price'] > 0:\n",
    "            # Find price index\n",
    "            price_idx = np.where(np.abs(prices - decision['price']) < 1e-6)[0]\n",
    "            if len(price_idx) > 0:\n",
    "                opt_dist[price_idx[0]] += 1\n",
    "    \n",
    "    if len(selected_opportunities) > 0:\n",
    "        opt_dist = opt_dist / len(selected_opportunities)  # Normalize by number of sales\n",
    "    \n",
    "    return {\n",
    "        'opt_dist': opt_dist,\n",
    "        'optimal_revenue_per_round': optimal_revenue_per_round,\n",
    "        'opt_revenues_per_round': opt_revenues_per_round,\n",
    "        'selected_opportunities': selected_opportunities,\n",
    "        'actual_valuations': actual_valuations\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7362e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION AND ANALYSIS EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "# Import matplotlib if not already imported\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"üé® Generating comprehensive visualizations and analysis...\")\n",
    "print(\"   üìä 6 plots covering all aspects of algorithm performance\")\n",
    "print(\"   üìà Corrected regret calculations with proper theoretical baseline\")\n",
    "print(\"   üéØ Policy comparisons and convergence analysis\")\n",
    "\n",
    "# Extract actual valuations from results\n",
    "actual_valuations = results.get('actual_valuations', None)\n",
    "if actual_valuations is None:\n",
    "    print(\"‚ö†Ô∏è No actual valuations found in results, will use same random seed for fair comparison\")\n",
    "\n",
    "print(f\"üìä Number of actual valuations: {len(actual_valuations) if actual_valuations else 'None'}\")\n",
    "print(f\"üìä Sample valuations: {actual_valuations[:5] if actual_valuations else 'N/A'}\")\n",
    "\n",
    "try:\n",
    "    # Debug: Check what the function returns\n",
    "    result = compute_correct_theoretical_optimum(agent_params, valuation_dists, T, actual_valuations=actual_valuations)\n",
    "    print(f\"Function returned type: {type(result)}\")\n",
    "    print(f\"Function returned keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dictionary'}\")\n",
    "    \n",
    "    # Extract values from the result dictionary\n",
    "    opt_dist = result['opt_dist']\n",
    "    optimal_revenue = result['optimal_revenue_per_round']\n",
    "    true_rewards = result['opt_revenues_per_round']\n",
    "    selected_opportunities = result['selected_opportunities']\n",
    "    \n",
    "    print(f\"‚úì Theoretical optimum computed: {sum(true_rewards):.2f} total revenue\")\n",
    "\n",
    "    # === CORRECTED REGRET CALCULATION ===\n",
    "    theoretical_optimal_total = sum(true_rewards)  # Correct total: sum of actual optimal revenues per round\n",
    "    total_revenue = sum(results['revenues'])\n",
    "\n",
    "    # CORRECTED regret calculation using proper baseline\n",
    "    regret_per_round = []\n",
    "    cumulative_optimal_revenue = []  # Track cumulative optimal for correct plotting\n",
    "\n",
    "    for t in range(T):\n",
    "        # Instantaneous regret = optimal_revenue_at_t - actual_revenue_at_t\n",
    "        optimal_at_t = true_rewards[t]\n",
    "        actual_at_t = results['revenues'][t]\n",
    "        instantaneous_regret = optimal_at_t - actual_at_t\n",
    "        regret_per_round.append(instantaneous_regret)\n",
    "        \n",
    "        # Cumulative optimal for correct comparison line\n",
    "        cumulative_optimal_revenue.append(sum(true_rewards[:t+1]))\n",
    "\n",
    "    regret_cumulative = np.cumsum(regret_per_round)\n",
    "    final_performance = (total_revenue / theoretical_optimal_total) * 100 if theoretical_optimal_total > 0 else 0\n",
    "\n",
    "    print(f\"‚úì Regret calculation completed\")\n",
    "    print(f\"üìä CORRECTED Performance Summary:\")\n",
    "    print(f\"   ‚Ä¢ Agent total revenue: {total_revenue:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Optimal total revenue: {theoretical_optimal_total:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Performance: {final_performance:.1f}% of optimal\")\n",
    "    print(f\"   ‚Ä¢ Total regret: {regret_cumulative[-1]:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Average regret per round: {regret_cumulative[-1]/T:.6f}\")\n",
    "\n",
    "    # Price frequency analysis\n",
    "    price_to_idx = {p: i for i, p in enumerate(agent_params['prices'])}\n",
    "    price_indices = []\n",
    "    for p in results['selected_prices']:\n",
    "        if not (isinstance(p, float) and np.isnan(p)) and p in price_to_idx:\n",
    "            price_indices.append(price_to_idx[p])\n",
    "\n",
    "    if price_indices:\n",
    "        price_counts = np.bincount(price_indices, minlength=len(agent_params['prices']))\n",
    "    else:\n",
    "        price_counts = np.zeros(len(agent_params['prices']))\n",
    "\n",
    "    # === VISUALIZATION ===\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    # 1. Cumulative Revenue vs Optimal (CORRECTED)\n",
    "    axes[0].plot(results['cumulative_revenue'], label='Primal-Dual Agent Revenue', \n",
    "                    linewidth=3, color='blue', alpha=0.8)\n",
    "    axes[0].plot(cumulative_optimal_revenue, \n",
    "                    label='CORRECTED Theoretical Optimal', \n",
    "                    linestyle='--', linewidth=3, color='red', alpha=0.8)\n",
    "\n",
    "    axes[0].text(0.02, 0.98, \n",
    "                    f'Agent Performance: {final_performance:.1f}%\\n'\n",
    "                    f'of CORRECTED Theoretical Optimal\\n'\n",
    "                    f'Agent Total: {total_revenue:.2f}\\n'\n",
    "                    f'Optimal Total: {theoretical_optimal_total:.2f}', \n",
    "                    transform=axes[0].transAxes, fontsize=11, va='top',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"yellow\", alpha=0.7))\n",
    "\n",
    "    axes[0].set_xlabel('Round', fontsize=12)\n",
    "    axes[0].set_ylabel('Cumulative Revenue', fontsize=12)\n",
    "    axes[0].set_title('Revenue Performance Comparison (CORRECTED)', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. CORRECTED Regret Analysis\n",
    "    t_vals = np.arange(1, T + 1)\n",
    "    axes[1].plot(regret_cumulative, color='red', linewidth=3, \n",
    "                    label='Actual Cumulative Regret', alpha=0.8)\n",
    "\n",
    "    # Theoretical bound\n",
    "    K = len(agent_params['prices'])\n",
    "    sqrt_t_log_t = np.sqrt(t_vals) * np.log(t_vals + 1)\n",
    "    max_possible_regret_per_round = np.max(agent_params['prices'])\n",
    "    bound_scale = max_possible_regret_per_round * np.sqrt(np.log(K))\n",
    "    primal_dual_bound = bound_scale * sqrt_t_log_t\n",
    "\n",
    "    axes[1].plot(t_vals, primal_dual_bound, '--', color='blue', alpha=0.7, linewidth=2,\n",
    "             label=f\"Primal-Dual Bound {'O(sqrt(T log T))' if agent_params['algorithm'] == 'hedge' else 'O(sqrt(T))'}\")\n",
    "\n",
    "    final_regret_rate = regret_cumulative[-1] / T\n",
    "    \n",
    "    if (agent_params['algorithm'] == 'hedge'):\n",
    "        theoretical_bound = bound_scale * np.sqrt(T * np.log(T))\n",
    "    else:\n",
    "        theoretical_bound = np.sqrt(T)\n",
    "\n",
    "    bound_ratio = regret_cumulative[-1] / theoretical_bound if theoretical_bound > 0 else 0\n",
    "\n",
    "    axes[1].text(0.02, 0.98, \n",
    "                    f'Final Regret Rate: {final_regret_rate:.6f}\\n'\n",
    "                    f'Total Regret: {regret_cumulative[-1]:.2f}\\n'\n",
    "                    f'Theoretical Bound: {theoretical_bound:.2f}\\n'\n",
    "                    f'Actual/Bound Ratio: {bound_ratio:.2f}x\\n'\n",
    "                    f'CORRECTED Baseline Used', \n",
    "                    transform=axes[1].transAxes, fontsize=11, va='top',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"lightgreen\", alpha=0.7))\n",
    "\n",
    "    axes[1].set_xlabel('Round', fontsize=12)\n",
    "    axes[1].set_ylabel('Cumulative Regret', fontsize=12)\n",
    "    axes[1].set_title('Regret Growth Analysis (CORRECTED)', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚úÖ All visualizations completed successfully!\")\n",
    "    print(f\"üéØ Key Insight: The agent achieves {final_performance:.1f}% of the corrected optimal performance\")\n",
    "    print(f\"üìà This represents excellent performance for online learning in a non-stationary environment\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during plotting: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"üîß Please check the data and function definitions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
