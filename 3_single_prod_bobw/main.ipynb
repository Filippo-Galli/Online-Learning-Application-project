{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6b2b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331bc84",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeb0baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStationaryStochasticPricingEnvironment:\n",
    "    \"\"\"\n",
    "    Non-Stationary stochastic environment, with the distribution over customer valuations for a single product changing quickly over time.\n",
    "    \"\"\"\n",
    "    def __init__(self, valuation_distributions, prices, demand_noise_std=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            valuation_distributions: A list of different scipy.stats distributions representing customer valuations\n",
    "            demand_noise_std: Standard deviation of noise in demand probability\n",
    "            current_round: Variable that keep the count of the round been played.\n",
    "        \"\"\"\n",
    "        self.valuation_dist = valuation_distributions\n",
    "        self.noise_std = demand_noise_std\n",
    "        self.current_round = 0\n",
    "        self.prices = prices\n",
    "\n",
    "\n",
    "    def demand_probability(self, price):\n",
    "        \"\"\"\n",
    "        Calculate the probability that a customer buys at given price.\n",
    "        This is P(valuation >= price) with some noise.\n",
    "        \"\"\"\n",
    "        # Retrieve the distribution associated with the current round\n",
    "        current_dist = self.valuation_dist[self.current_round]\n",
    "\n",
    "        # Base probability: customers buy if their valuation >= price\n",
    "        base_prob = 1 - current_dist.cdf(price)\n",
    "        \n",
    "        # Add some noise to make it stochastic\n",
    "        noise = np.random.normal(0, self.noise_std)\n",
    "        \n",
    "        prob = base_prob + noise\n",
    "        # Ensure probability is in [0, 1]\n",
    "        return np.clip(prob, 0, 1)\n",
    "    \n",
    "\n",
    "    def simulate_round(self, price):\n",
    "        \n",
    "        \"\"\"\n",
    "        Simulate one pricing round.\n",
    "        Returns: (sale_made, revenue)\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve the distribution associated with the current round\n",
    "        current_dist = self.valuation_dist[self.current_round]\n",
    "\n",
    "        # Draw a random customer valuation from the distribution\n",
    "        valuation = current_dist.rvs()\n",
    "        \n",
    "        # Customer purchases if their valuation >= price\n",
    "        sale_made = 1 if valuation >= price else 0\n",
    "        \n",
    "        # Revenue is price if sale was made, 0 otherwise\n",
    "        revenue = sale_made * price\n",
    "        \n",
    "        sale_made_full = valuation>=self.prices\n",
    "        revenue_full = self.prices * sale_made_full\n",
    "        \n",
    "        # Updating rounds' count\n",
    "        self.current_round += 1\n",
    "\n",
    "        return sale_made, revenue, sale_made_full, revenue_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa861eb",
   "metadata": {},
   "source": [
    "Poi quando vado a definire l'env conf, definisco una funzione lambda per media e std. Creo un vettore di distribuzioni da passare all'environment.\n",
    "Forse addirittura ha senso cambiare la distribuzione (e non farle solo normale), per creare un cambiamento pi√π sharp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88e9a9",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4eedac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HedgeAgent:\n",
    "    def __init__(self, K, learning_rate):\n",
    "        self.K = K\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.ones(K)\n",
    "        self.x_t = np.ones(K)/K\n",
    "        self.a_t = None\n",
    "        self.t = 0\n",
    "\n",
    "    def pull_arm(self):\n",
    "        self.x_t = self.weights/sum(self.weights)\n",
    "        self.a_t = np.random.choice(np.arange(self.K), p=self.x_t)\n",
    "        return self.a_t\n",
    "    \n",
    "    def update(self, l_t):\n",
    "        self.weights *= np.exp(-self.learning_rate*l_t)\n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25f9f4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimalDualAgent:\n",
    "    def __init__(self, prices, valuation, P, T, eta):\n",
    "        self.prices = prices\n",
    "        self.K = len(prices)\n",
    "        self.hedge = HedgeAgent(self.K, np.sqrt(np.log(self.K)/T))\n",
    "        self.valuation = valuation\n",
    "        self.P = P  #Budget\n",
    "        self.eta = eta\n",
    "        self.T = T\n",
    "        self.rho = self.P/self.T\n",
    "        self.lmbd = 1\n",
    "        self.t = 0\n",
    "        self.N_pulls = np.zeros(len(prices))\n",
    "        self.bid_index = 0\n",
    "        self.reward = np.zeros(self.K)\n",
    "        self.t = 0\n",
    "\n",
    "        # Inventory management\n",
    "        self.remaining_inventory = P     # Current remaining inventory\n",
    "\n",
    "        # History tracking\n",
    "        self.history = {\n",
    "            'prices': [],     # Selected prices over time\n",
    "            'rewards': [],    # Observed revenues over time\n",
    "            'purchases': [],  # Purchase indicators over time\n",
    "            'inventory': []   # Inventory levels over time\n",
    "        }\n",
    "\n",
    "    def bid(self):\n",
    "        if self.remaining_inventory < 1:\n",
    "            self.bid_index = 0\n",
    "            return 0\n",
    "        self.bid_index = self.hedge.pull_arm()\n",
    "        return self.prices[self.bid_index]\n",
    "    \n",
    "    def update(self, f_t, c_t, f_t_full, c_t_full):\n",
    "        # update hedge\n",
    "        #purchase = (self.valuation[self.t] >= np.array(self.prices)).astype(float)\n",
    "        #f_t_full = self.prices * purchase\n",
    "        #c_t_full = purchase\n",
    "        L = f_t_full - self.lmbd*(c_t_full-self.rho)\n",
    "        \n",
    "        L_up = 1 -(1/self.rho)*(-self.rho)  ### ATTENZIONE, CHECK -self.rho SE CI SONO PROBLEMI\n",
    "        L_low = 0 -(1/self.rho)*(1-self.rho)\n",
    "        rescaled_L = (L - L_low)/(L_up-L_low)\n",
    "        self.hedge.update(1 -rescaled_L) # we need to maximize L\n",
    "\n",
    "        # update lagrangian multiplier\n",
    "        self.lmbd = np.clip(self.lmbd-self.eta*(self.rho-c_t), \n",
    "                            a_min=0, a_max=1/self.rho)\n",
    "        # update budget\n",
    "        self.remaining_inventory -= c_t\n",
    "        # log\n",
    "        self.N_pulls[self.bid_index] += 1\n",
    "\n",
    "        self.reward += f_t_full\n",
    "\n",
    "        # Record history\n",
    "        self.history['prices'].append(self.prices[self.bid_index])\n",
    "        self.history['rewards'].append(f_t)\n",
    "        self.history['purchases'].append(c_t)\n",
    "        self.history['inventory'].append(self.remaining_inventory)\n",
    "\n",
    "        self.t += 1\n",
    "    \n",
    "    # For clairvoyant purpose\n",
    "    #def update_reward(self, f_t_full):\n",
    "    #    self.reward += f_t_full\n",
    "    \n",
    "    # Getter for reward vector\n",
    "    def get_reward(self):\n",
    "        return self.reward\n",
    "    \n",
    "    # Getter for the index (price) associated with the highest reward\n",
    "    def get_argmax_reward(self):\n",
    "        return np.argmax(self.reward)\n",
    "    \n",
    "    def get_max_reward(self):\n",
    "        best_idx = np.argmax(self.reward)\n",
    "        return self.reward[best_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d676aad",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8596f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulator(\n",
    "    T,\n",
    "    valuation_dist,\n",
    "    env_config,\n",
    "    agent_params,\n",
    "    n_simulations=3,\n",
    "    verbose=True\n",
    "):\n",
    "    selected_prices = []\n",
    "    revenues = []\n",
    "    sales = []\n",
    "    cumulative_revenue = []\n",
    "    total_revenue = 0\n",
    "    best_price = []\n",
    "    best_reward = []\n",
    "    best_reward_value = 0\n",
    "    best_price_value = 0\n",
    "    first_inventory_empty = None\n",
    "\n",
    "    # === Compute theoretical optimum once before all simulations ===\n",
    "    env_theoretical = NonStationaryStochasticPricingEnvironment(\n",
    "        valuation_distributions=valuation_dist,\n",
    "        prices=agent_params['prices']\n",
    "    )\n",
    "    \"\"\"\n",
    "    opt_dist, opt_value, true_purchase_probs, true_rewards = compute_clairvoyant(\n",
    "        prices=agent_params['prices'],\n",
    "        environment=env_theoretical,\n",
    "        T=agent_params['T'],\n",
    "        P=agent_params['P']\n",
    "    )\n",
    "    optimal_idx = np.argmax(opt_dist)\n",
    "    #optimal_revenue = np.dot(agent_params['prices'], opt_dist)\n",
    "    optimal_revenue = np.dot(true_rewards, opt_dist)\n",
    "    \"\"\"\n",
    "    for sim in range(n_simulations):\n",
    "        \n",
    "        print(f\"\\n=== Running Primal-Dual Pricing Simulation #{sim + 1} for {T} rounds ===\")\n",
    "\n",
    "        # Create environment and agent\n",
    "        env = NonStationaryStochasticPricingEnvironment(\n",
    "            valuation_distributions=valuation_dist,\n",
    "            prices = agent_params['prices']\n",
    "        )\n",
    "        agent = PrimalDualAgent(**agent_params)\n",
    "        #agent = ThompsonSamplingAgent(P =agent_params['P'], T=T, prices=agent_params['prices'], rho_penalty=agent_params['rho_penalty'])\n",
    "\n",
    "        for t in range(T):\n",
    "            if agent.remaining_inventory < 1 and first_inventory_empty is None:\n",
    "                first_inventory_empty = t\n",
    "                print(f\"Inventory empty for the first time at round {t}\")\n",
    "                print(\"No more products in the inventory\")\n",
    "\n",
    "            price = agent.bid()\n",
    "            price_idx = agent.bid_index\n",
    "\n",
    "            if agent.remaining_inventory <= 0:\n",
    "                sale_made = False\n",
    "                revenue = 0\n",
    "            else:\n",
    "                sale_made, revenue, sale_made_full, revenue_full = env.simulate_round(price)\n",
    "\n",
    "            agent.update(revenue, sale_made, revenue_full, sale_made_full)\n",
    "\n",
    "            selected_prices.append(price)\n",
    "            revenues.append(revenue)\n",
    "            sales.append(sale_made)\n",
    "            total_revenue += revenue\n",
    "            cumulative_revenue.append(total_revenue)\n",
    "\n",
    "        best_reward_value = agent.get_max_reward()\n",
    "        best_reward = np.append(best_reward, best_reward_value)\n",
    "        best_price_value = agent_params['prices'][agent.get_argmax_reward()]\n",
    "        best_price = np.append(best_price, best_price_value)\n",
    "\n",
    "        print(f\"\\nSimulation completed!\")\n",
    "        print(f\"Total revenue: {total_revenue:.2f}\")\n",
    "        print(f\"Average revenue per round: {total_revenue / T:.2f}\")\n",
    "        print(f\"Agent's best price: {best_price[-1]}\")\n",
    "        #print(f\"Theoretical optimal: {opt_dist} (expected revenue: {optimal_revenue:.2f})\")\n",
    "\n",
    "    #price_to_idx = {p: i for i, p in enumerate(agent_params['prices'])}\n",
    "    #price_indices = [price_to_idx[p] for p in selected_prices if not np.isnan(p)]\n",
    "    #price_counts = np.bincount(price_indices, minlength=len(agent_params['prices']))        \n",
    "\n",
    "    #if verbose:\n",
    "        #print(\"\\n=== Simulation Results ===\")\n",
    "        #print(\"\\nPrice selection frequency:\")\n",
    "        #for i, (price, count) in enumerate(zip(agent_params['prices'], price_counts)):\n",
    "            #percentage = 100 * count / T\n",
    "            #marker = \" ‚Üê OPTIMAL\" if i == optimal_idx else \"\"\n",
    "            #print(f\"  Price {price:2}: {count:4d} times ({percentage:5.1f}%){marker}\")\n",
    "\n",
    "    return {\n",
    "        #'price_counts': price_counts,\n",
    "        'selected_prices': selected_prices,\n",
    "        'revenues': revenues,\n",
    "        'sales': sales,\n",
    "        'cumulative_revenue': cumulative_revenue,\n",
    "        'best_price': best_price,\n",
    "        'best_reward': best_reward,\n",
    "        'first_inventory_empty': first_inventory_empty,\n",
    "        'total_revenue': total_revenue,\n",
    "        #'opt_dist': opt_dist,\n",
    "        #'optimal_idx': optimal_idx,\n",
    "        #'optimal_revenue': optimal_revenue,\n",
    "        'agent': agent  # Add the final agent state\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57484aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(np.array([1,2,3]), [4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e63f048",
   "metadata": {},
   "source": [
    "RUN THE SIMULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b77733a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of price options: 6\n",
      "Inventory contrain:  1000\n",
      "Number of rounds: 10000\n",
      "\n",
      "=== Running Primal-Dual Pricing Simulation #1 for 10000 rounds ===\n",
      "Inventory empty for the first time at round 4294\n",
      "No more products in the inventory\n",
      "\n",
      "Simulation completed!\n",
      "Total revenue: 709.71\n",
      "Average revenue per round: 0.07\n",
      "Agent's best price: 0.8142857142857142\n"
     ]
    }
   ],
   "source": [
    "# ===== Configurazione Ambiente Non Stazionario =====\n",
    "env_config = {\n",
    "    'valuation_mean': 0.5,  # Average customer valuation\n",
    "    'valuation_std': 0.05,   # Standard deviation of customer valuations\n",
    "    'demand_noise_std': 0.005  # Noise in demand probability\n",
    "}\n",
    "\n",
    "T = 10000 # Number of rounds\n",
    "means = np.random.uniform(0, 1, size=T)\n",
    "stds = np.random.uniform(0, 0.1, size=T)\n",
    "\n",
    "\n",
    "####### Non riesco a creare un array di distribuzioni, domani cerco di samplare media e std direttamente dall'env #######\n",
    "valuation_dists = [\n",
    "    stats.norm(\n",
    "        loc=means[_],\n",
    "        scale=stds[_]\n",
    "    )\n",
    "    for _ in range(T)\n",
    "]\n",
    "\n",
    "inventory = int(T * 0.1)       # Initial inventory\n",
    "eta = 0.1                      # Learning rate for lambda\n",
    "\n",
    "# ===== Inizializza Agente e Ambiente =====\n",
    "agent_params = {\n",
    "    'prices': np.arange(0.1, 0.9, 1/7),  # set of prices \n",
    "    'valuation' : valuation_dists,\n",
    "    'P': inventory,  # inventory constraint\n",
    "    'T': T,  # number of rounds\n",
    "    'eta': eta,\n",
    "}\n",
    "\n",
    "agent = PrimalDualAgent(**agent_params)\n",
    "\n",
    "env = NonStationaryStochasticPricingEnvironment(valuation_dists, agent_params['prices'])\n",
    "\n",
    "#print(f\"Customer valuation distribution: Normal(Œº={env_config['valuation_mean']}, œÉ={env_config['valuation_std']})\")\n",
    "print(f\"Number of price options: {len(agent_params['prices'])}\")\n",
    "print(f'Inventory contrain: ',agent_params['P'])\n",
    "print(f'Number of rounds:',agent_params['T'])\n",
    "\n",
    "# set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "results = run_simulator(\n",
    "    T=T,\n",
    "    valuation_dist=valuation_dists,\n",
    "    env_config=env_config,\n",
    "    agent_params=agent_params,\n",
    "    n_simulations=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "selected_prices = results['selected_prices']\n",
    "revenues = results['revenues']\n",
    "sales = results['sales']\n",
    "cumulative_revenue = results['cumulative_revenue']\n",
    "best_price = results['best_price']\n",
    "first_inventory_empty = results['first_inventory_empty']\n",
    "total_revenue = results['total_revenue']\n",
    "#price_counts = results['price_counts']\n",
    "agent = results['agent']  # Get the actual agent used in simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class PrimalDualAgent:\n",
    "    Primal-dual agent for dynamic pricing with inventory constraints in highly non-stationary environments.\n",
    "    \n",
    "    This agent implements a primal-dual based algorithm adapted for:\n",
    "    1. Dynamic pricing (instead of traditional MAB rewards)\n",
    "    2. Inventory constraints (limited number of products to sell)\n",
    "    3. Dual optimization: maximize revenue while respecting inventory constraint\n",
    "    \n",
    "    The algorithm maintains upper confidence bounds on revenue (f_UCB) and \n",
    "    lower confidence bounds on demand probability (c_LCB), then solves a \n",
    "    linear program to find the optimal price distribution.\n",
    "    def __init__(self, P, T, prices, eta = 0.1, ema_alpha = 0.2, lambda0 = 0.0):\n",
    "        Initialize the UCB agent for constrained dynamic pricing.\n",
    "        \n",
    "        Args:\n",
    "            P: Total inventory (number of products available)\n",
    "            T: Time horizon (number of rounds)\n",
    "            prices: List of available prices to choose from\n",
    "            eta: learning rate for dual update\n",
    "        k = len(prices)\n",
    "\n",
    "        # Environment parameters\n",
    "        self.prices = prices  # Available price options\n",
    "        self.K = k           # Number of price arms\n",
    "        self.T = T           # Total number of rounds\n",
    "        self.t = 0           # Current round number\n",
    "        \n",
    "        # Primal-dual parameters\n",
    "        self.rho = P/float(T)                         # Target selling rate\n",
    "        self.eta = eta\n",
    "        self.lambda_ = float(lambda0)\n",
    "        self.lambda_upper = 1.0 / max(self.rho, 1e-8) # Projection upper bound\n",
    "        \n",
    "        # Inventory management\n",
    "        self.inventory = P  # Initial inventory\n",
    "        self.remaining_inventory = P # Current remaining inventory \n",
    "\n",
    "        # EMA estimates (reactive to non-stationarity)\n",
    "        self.ema_alpha = ema_alpha\n",
    "        self.f_est = np.zeros(self.K)        # Estimated expected revenue for each price\n",
    "        self.c_est = np.ones(self.K) * 1e-6  # Estimated demand probability for each price, initialized with small positive number to avoid degenerate LP constraints\n",
    "        \n",
    "        \n",
    "        self.N_pulls = np.zeros(k)           # Number of times each price was selected\n",
    "        self.current_price_idx = None\n",
    "\n",
    "        # History tracking\n",
    "        self.history = {\n",
    "            'prices': [],     # Selected prices over time\n",
    "            'rewards': [],    # Observed revenues over time\n",
    "            'purchases': [],  # Purchase indicators over time\n",
    "            'lambda': [],     # Lambda parameter\n",
    "            'inventory': []   # Inventory levels over time\n",
    "        }\n",
    "    \n",
    "    def select_price(self):\n",
    "        Select the next price using LP with inventory constraints.\n",
    "        \n",
    "        Solve LP : maximize sum_i gamma_i * (f_est_i - lambda*c_est_i)\n",
    "                   subject to sum gamma = 1, gamma >= 0\n",
    "        \n",
    "        If LP fails or objective degenerate, fallback to greedy argmax(f_est - lambda*c_est)         \n",
    "        Then sample price according to gamma\n",
    "\n",
    "        Returns:\n",
    "            Selected price, or np.nan if no inventory remaining\n",
    "        # No inventory left - cannot make meaningful pricing decisions\n",
    "        if self.remaining_inventory < 1:\n",
    "            self.current_price_idx = np.argmax(self.prices)  # Arbitrary selection\n",
    "            return np.nan\n",
    "            \n",
    "        # Objective coefficients (with - since we want to maximize, while linprog does minimization)\n",
    "        obj = -(self.f_est - self.lambda_ * self.c_est)\n",
    "        \n",
    "        # Constraints: sum gamma = 1, gamma>=0\n",
    "        A_eq = [np.ones(self.K)]\n",
    "        b_eq = [1.0]\n",
    "        bounds = [(0.0, 1.0) for _ in range(self.K)]\n",
    "        try:            \n",
    "            res = optimize.linprog(c = obj, A_eq = A_eq, b_eq = b_eq, bounds = bounds, method = 'highs')\n",
    "            if res.success:\n",
    "                gamma = res.x\n",
    "                gamma = np.maximum(gamma,0.0)\n",
    "                s = gamma.sum()\n",
    "                if s<=1e-12:\n",
    "                    gamma = np.ones(self.K) / self.K\n",
    "                else:\n",
    "                    gamma = gamma / s\n",
    "            else:\n",
    "                # fallback: deterministic greedy on adjusted objective\n",
    "                scores = self.f_Est - self.lambda_ * self.c_est\n",
    "                best = np.argmax(scores)\n",
    "                gamma = np.zeros(self.K)\n",
    "                gamma[best] = 1.0\n",
    "        except Exception:\n",
    "            scores = self.f_est - self.lambda_ * self.c_est\n",
    "            best = np.argmax(scores)\n",
    "            gamma = np.zeros(self.K)\n",
    "            gamma[best] = 1.0\n",
    "        \n",
    "        # Sample according to gamma\n",
    "        idx = np.random.choice(self.K, p=gamma)\n",
    "        self.current_price_idx = int(idx)\n",
    "        return float(self.prices[self.current_price_idx])\n",
    "    \n",
    "    def update(self, reward, purchased):\n",
    "        Update agent's statistics based on observed outcome.\n",
    "        \n",
    "        Args:\n",
    "            reward: Revenue obtained (price if purchased, 0 otherwise)\n",
    "            purchased: Boolean indicating if purchase was made\n",
    "        idx = self.current_price_idx\n",
    "        \n",
    "        # Update pull count\n",
    "        self.N_pulls[idx] += 1\n",
    "        \n",
    "        # Update EMA for f_est, reward is either price, if purchased, or 0\n",
    "        prev_f = self.f_est[idx]\n",
    "        self.f_est[idx] = (1-self.ema_alpha)*prev_f+self.ema_alpha*reward\n",
    "        \n",
    "        # Dual update: lambda <- proj_[0, 1/rho] (lambda - eta*(rho-c_t(b_t)))\n",
    "        purchased_indicator = 1.0 if purchased else 0.0\n",
    "        grad = (self.rho - purchased_indicator)\n",
    "        self.lambda_ = self.lambda_ - self.eta * grad\n",
    "        # Project\n",
    "        self.lambda_ = np.clip(self.lambda_, 0.0, self.lambda_upper)\n",
    "\n",
    "        # Update inventory only if purchase was actually made and inventory available\n",
    "        if purchased and self.remaining_inventory > 0:\n",
    "            self.remaining_inventory -= 1\n",
    "        elif purchased and self.remaining_inventory <= 0:\n",
    "            # This shouldn't happen with proper price selection, but handle gracefully\n",
    "            reward = 0\n",
    "            purchased = False\n",
    "        \n",
    "        # Record history\n",
    "        self.history['prices'].append(self.prices[idx])\n",
    "        self.history['rewards'].append(reward)\n",
    "        self.history['purchases'].append(purchased)\n",
    "        self.history['lambda'].append(self.lambda_)\n",
    "        self.history['inventory'].append(self.remaining_inventory)\n",
    "        \n",
    "        # Increment time\n",
    "        self.t += 1\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\n",
    "            't': self.t,\n",
    "            'remaining_inventory': self.remaining_inventory,\n",
    "            'lambda': self.lambda_,\n",
    "            'f_est': self.f_est.copy(),\n",
    "            'c_est': self.c_est.copy()\n",
    "        }\n",
    "    \n",
    "    def get_best_price(self):\n",
    "        if self.t == 0:\n",
    "            return None, 0.0\n",
    "        \n",
    "        scores = self.f_est - self.lambda_ * self.c_est\n",
    "        \n",
    "        best_idx = np.argmax(scores)\n",
    "        return self.prices[best_idx], self.f_est[best_idx]\n",
    "\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b915c225",
   "metadata": {},
   "source": [
    "# Theoretical Optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the optimal solution\n",
    "\"\"\"\n",
    "def compute_clairvoyant(prices, agent, environment, T, P):\n",
    "    Compute the optimal (clairvoyant) pricing strategy with full information.\n",
    "    \n",
    "    This function solves the linear program that an oracle with perfect knowledge\n",
    "    of the demand probabilities would solve:\n",
    "    \n",
    "    maximize: sum_i gamma_i * price_i * demand_prob_i\n",
    "    subject to: sum_i gamma_i * demand_prob_i <= P/T  (inventory constraint)\n",
    "               sum_i gamma_i = 1                      (probability constraint)\n",
    "               gamma_i >= 0                          (non-negativity)\n",
    "    \n",
    "    Args:\n",
    "        prices: List of available prices\n",
    "        environment: StochasticPricingEnvironment to get true demand probabilities\n",
    "        T: Time horizon\n",
    "        P: Total inventory\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (optimal_distribution, optimal_value, demand_probabilities, expected_revenues)\n",
    "    # Compute true demand probabilities for each price\n",
    "    buying_probabilities = np.array([environment.demand_probability(p) for p in prices])\n",
    "    \n",
    "    # Expected revenue per selection for each price\n",
    "    exp_reward = prices * buying_probabilities\n",
    "    \n",
    "    # Set up linear program (convert maximization to minimization)\n",
    "    c = agent.get_reward()\n",
    "    \n",
    "    # Inventory constraint: expected consumption rate <= inventory rate\n",
    "    A_ub = [buying_probabilities]\n",
    "    b_ub = [P / T]\n",
    "    \n",
    "    # Probability constraint: sum of probabilities = 1\n",
    "    A_eq = [np.ones(len(prices))]\n",
    "    b_eq = [1]\n",
    "    \n",
    "    # Solve the linear program\n",
    "    res = optimize.linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, \n",
    "                          bounds=[(0, 1) for _ in range(len(prices))])\n",
    "    \n",
    "    gamma = res.x  # Optimal price distribution\n",
    "    optimal_value = -res.fun  # Optimal expected revenue per round\n",
    "    \n",
    "    return gamma, optimal_value, buying_probabilities, exp_reward\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc5d1b",
   "metadata": {},
   "source": [
    "def run_simulator_nonstationary(\n",
    "    T,\n",
    "    valuation_dist,\n",
    "    env_config,\n",
    "    agent_params,\n",
    "    n_simulations=1,\n",
    "    verbose=True,\n",
    "    seed=42,\n",
    "    compute_theoretical=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulatore semplice per ambiente altamente non-stazionario.\n",
    "\n",
    "    Argomenti:\n",
    "        T (int): orizzonte temporale (numero di round)\n",
    "        env_config (dict): configurazione dell'environment non-stazionario (es. mean_function, std_function, ...)\n",
    "        agent_params (dict): parametri da passare al costruttore dell'agente (deve contenere almeno 'prices' e 'P')\n",
    "        agent_class (class): classe agente da istanziare (es. UCBLikeAgent o PrimalDualAgent)\n",
    "        n_simulations (int): numero di simulazioni indipendenti\n",
    "        verbose (bool): stampa informazioni riassuntive\n",
    "        seed (int): seed per riproducibilit√†\n",
    "        compute_theoretical (bool): se True prova a calcolare il clairvoyant non-stazionario via compute_clairvoyant_nonstationary\n",
    "\n",
    "    Ritorna:\n",
    "        dict con risultati aggregati e per-simulazione (selected_prices, revenues, sales, cumulative_revenue, best_prices,\n",
    "        first_inventory_empty, total_revenue, opt_gamma, optimal_total_revenue, optimal_avg_dist, agent_states)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    prices = np.array(agent_params['prices'])\n",
    "    P = agent_params['P']\n",
    "\n",
    "    # --- compute theoretical clairvoyant (non-stationary) once if requested ---\n",
    "    opt_gamma = None\n",
    "    optimal_total_revenue = None\n",
    "    optimal_avg_dist = None\n",
    "    if compute_theoretical:\n",
    "        try:\n",
    "            # costruiamo un environment teorico con i parametri non-stazionari\n",
    "            env_theoretical = NonStationaryStochasticPricingEnvironment(valuation_dist, T)\n",
    "            (opt_gamma,\n",
    "             optimal_total_revenue,\n",
    "             demand_matrix,\n",
    "             expected_revenues_matrix,\n",
    "             per_round_expected_revenue,\n",
    "             per_round_expected_consumption) = compute_clairvoyant(\n",
    "                prices=prices,\n",
    "                environment=env_theoretical,\n",
    "                T=T,\n",
    "                P=P\n",
    "            )\n",
    "            # distribuzione media su tutti i round (utile per confronto \"which price is often chosen by clairvoyant\")\n",
    "            optimal_avg_dist = opt_gamma.mean(axis=0)\n",
    "            optimal_idx = int(np.argmax(optimal_avg_dist))\n",
    "            optimal_revenue_per_round = optimal_total_revenue / float(T)\n",
    "        except Exception as e:\n",
    "            # se il clairvoyant su tutto l'orizzonte fallisce, non interrompiamo la simulazione\n",
    "            if verbose:\n",
    "                print(\"Warning: compute_clairvoyant_nonstationary failed:\", e)\n",
    "            opt_gamma = None\n",
    "            optimal_total_revenue = None\n",
    "            optimal_avg_dist = None\n",
    "            optimal_idx = None\n",
    "            optimal_revenue_per_round = None\n",
    "\n",
    "    # --- containers aggregati su tutte le simulazioni ---\n",
    "    all_price_counts = np.zeros(len(prices), dtype=int)\n",
    "    all_selected_prices = []\n",
    "    all_revenues = []\n",
    "    all_sales = []\n",
    "    all_cumulative_revenues = []\n",
    "    best_prices = []\n",
    "    first_inventory_empty_list = []\n",
    "    total_revenues_list = []\n",
    "    agents = []\n",
    "\n",
    "    # --- run N independent simulations ---\n",
    "    for sim in range(n_simulations):\n",
    "        if verbose:\n",
    "            print(f\"\\n=== Running Simulation #{sim + 1} (T={T}) ===\")\n",
    "\n",
    "        # create a fresh environment and agent for each sim\n",
    "        env = NonStationaryStochasticPricingEnvironment(env_config, T)\n",
    "        agent = PrimalDualAgent(**agent_params)  # assumes agent_params keys match __init__\n",
    "        agents.append(agent)\n",
    "\n",
    "        selected_prices = []\n",
    "        revenues = []\n",
    "        sales = []\n",
    "        cumulative_revenue = []\n",
    "        total_revenue = 0\n",
    "        first_inventory_empty = None\n",
    "\n",
    "        for t in range(T):\n",
    "            # check inventory\n",
    "            if agent.remaining_inventory < 1 and first_inventory_empty is None:\n",
    "                first_inventory_empty = t\n",
    "                if verbose:\n",
    "                    print(f\"  Inventory empty first time at round {t}\")\n",
    "\n",
    "            price = agent.select_price()\n",
    "            price_idx = agent.current_price_idx\n",
    "\n",
    "            # if agent signalled \"no inventory\" or returned NaN -> no sale, revenue 0\n",
    "            if agent.remaining_inventory <= 0 or price is None or (isinstance(price, float) and np.isnan(price)):\n",
    "                sale_made = False\n",
    "                revenue = 0.0\n",
    "            else:\n",
    "                sale_made, revenue = env.simulate_round(price)\n",
    "\n",
    "            # update agent with observed outcome\n",
    "            agent.update(revenue, sale_made)\n",
    "\n",
    "            # logging\n",
    "            selected_prices.append(price)\n",
    "            revenues.append(revenue)\n",
    "            sales.append(int(sale_made))\n",
    "            total_revenue += revenue\n",
    "            cumulative_revenue.append(total_revenue)\n",
    "\n",
    "            # stop early if inventory exhausted (optional)\n",
    "            if agent.remaining_inventory <= 0:\n",
    "                # we let the loop continue to record zeros if you prefer; here we break for speed\n",
    "                # break\n",
    "                pass\n",
    "\n",
    "        # per-simulazione: statistiche\n",
    "        best_price, best_avg_revenue = agent.get_best_price()\n",
    "        best_prices.append(best_price)\n",
    "        first_inventory_empty_list.append(first_inventory_empty)\n",
    "        total_revenues_list.append(total_revenue)\n",
    "\n",
    "        # update aggregated counters\n",
    "        # price_to_idx mapping: skip NaN prices\n",
    "        price_to_idx = {p: i for i, p in enumerate(prices)}\n",
    "        valid_price_indices = [price_to_idx[p] for p in selected_prices if (p is not None) and (not (isinstance(p, float) and np.isnan(p)))]\n",
    "        counts = np.bincount(valid_price_indices, minlength=len(prices))\n",
    "        all_price_counts += counts\n",
    "\n",
    "        all_selected_prices.extend(selected_prices)\n",
    "        all_revenues.extend(revenues)\n",
    "        all_sales.extend(sales)\n",
    "        # cumulative revenue: keep absolute cumulative for each sim appended\n",
    "        # to allow plotting over concatenated sims we shift cumulative by previous total if needed\n",
    "        all_cumulative_revenues.extend(cumulative_revenue)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Simulation #{sim + 1} completed. Total revenue: {total_revenue:.2f}, avg per round: {total_revenue/float(T):.2f}\")\n",
    "            print(f\"Agent best price: {best_price} (avg revenue {best_avg_revenue:.3f})\")\n",
    "            if compute_theoretical and optimal_avg_dist is not None:\n",
    "                print(f\"Clairvoyant avg dist (mean across rounds): {np.round(optimal_avg_dist,3)}\")\n",
    "                print(f\"Clairvoyant expected total revenue: {optimal_total_revenue:.3f}, per round: {optimal_revenue_per_round:.3f}\")\n",
    "            print(\"-----\")\n",
    "\n",
    "    # final prints\n",
    "    if verbose:\n",
    "        print(\"\\n=== Aggregated results across simulations ===\")\n",
    "        total_runs = n_simulations * T\n",
    "        for i, (price, count) in enumerate(zip(prices, all_price_counts)):\n",
    "            perc = 100.0 * count / float(total_runs)\n",
    "            marker = \" ‚Üê CLairvoyant-most\" if (opt_gamma is not None and i == int(np.argmax(optimal_avg_dist))) else \"\"\n",
    "            print(f\"  Price {price:4.2f}: chosen {count} times ({perc:5.2f} %){marker}\")\n",
    "\n",
    "    return {\n",
    "        'price_counts': all_price_counts,\n",
    "        'selected_prices': all_selected_prices,\n",
    "        'revenues': all_revenues,\n",
    "        'sales': all_sales,\n",
    "        'cumulative_revenue': all_cumulative_revenues,\n",
    "        'best_prices': best_prices,\n",
    "        'first_inventory_empty': first_inventory_empty_list,\n",
    "        'total_revenues': total_revenues_list,\n",
    "        'opt_gamma': opt_gamma,\n",
    "        'optimal_total_revenue': optimal_total_revenue,\n",
    "        'optimal_avg_dist': optimal_avg_dist,\n",
    "        'agents': agents\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8440750d",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf88b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_results(T, revenues, cumulative_revenue, price_counts, \n",
    "                 agent_params, sales, agent, valuation_dist, env_config):\n",
    "    \"\"\"\n",
    "    Generate comprehensive visualizations and analysis of the UCB pricing algorithm.\n",
    "    \n",
    "    Args:\n",
    "        T: Number of rounds\n",
    "        revenues: List of revenues per round\n",
    "        cumulative_revenue: List of cumulative revenues\n",
    "        price_counts: Array of how many times each price was selected\n",
    "        agent_params: Dictionary of agent parameters\n",
    "        sales: List of sales indicators per round\n",
    "        agent: The UCBLikeAgent instance\n",
    "        valuation_dist: Customer valuation distribution\n",
    "        env_config: Environment configuration\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing computed regret and revenue data\n",
    "    \"\"\"\n",
    "    \n",
    "    # === Theoretical Analysis ===\n",
    "    print('=== Theoretical Analysis ===')\n",
    "\n",
    "    env = NonStationaryStochasticPricingEnvironment(\n",
    "        valuation_distribution=valuation_dist\n",
    "    )\n",
    "\n",
    "    opt_dist, opt_value, true_purchase_probs, true_rewards = compute_clairvoyant(\n",
    "        prices=agent_params['prices'],\n",
    "        environment=env,\n",
    "        T=agent_params['T'],\n",
    "        P=agent_params['P']\n",
    "    )\n",
    "\n",
    "    prices = agent_params['prices']\n",
    "    optimal_idx = np.argmax(opt_dist)\n",
    "    \n",
    "    print(\"Optimal price distribution:\")\n",
    "    for i in range(len(prices)):\n",
    "        marker = \" ‚Üê OPTIMAL\" if i == optimal_idx else \"\"\n",
    "        print(f\"  Price {prices[i]:2f}: p = {opt_dist[i]:5.3f}{marker}\")\n",
    "\n",
    "    optimal_revenue = np.dot(prices, opt_dist)\n",
    "    print(f'\\n=== Optimal Expected Revenue Per Round === \\n         {optimal_revenue:.2f}')\n",
    "\n",
    "    # === Performance Analysis ===\n",
    "    theoretical_optimal_total = optimal_revenue * T\n",
    "    total_revenue = sum(revenues)\n",
    "    actual_regret = theoretical_optimal_total - total_revenue\n",
    "    \n",
    "    # Better regret calculation for constrained setting\n",
    "    # For inventory-constrained problem, compute time-dependent optimal benchmark\n",
    "    regret_per_round = []\n",
    "    for t in range(T):\n",
    "        if t == 0:\n",
    "            instantaneous_regret = optimal_revenue - revenues[t]\n",
    "        else:\n",
    "            # Account for constraint tightening over time\n",
    "            remaining_inventory_at_t = agent_params['P'] - sum(sales[:t])\n",
    "            remaining_rounds = T - t\n",
    "            if remaining_rounds > 0 and remaining_inventory_at_t > 0:\n",
    "                # Compute constrained optimal for remaining problem\n",
    "                adjusted_rho = remaining_inventory_at_t / remaining_rounds\n",
    "                # Use the original optimal revenue as benchmark, but weight by feasibility\n",
    "                feasibility_factor = min(1.0, adjusted_rho / (agent_params['P'] / T))\n",
    "                benchmark_revenue = optimal_revenue * feasibility_factor\n",
    "            else:\n",
    "                benchmark_revenue = 0  # No inventory left\n",
    "            instantaneous_regret = max(0, benchmark_revenue - revenues[t])\n",
    "        regret_per_round.append(instantaneous_regret)\n",
    "    \n",
    "    regret_cumulative = np.cumsum(regret_per_round)\n",
    "\n",
    "    # === Enhanced Visualization ===\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "    # 1. Cumulative Revenue vs Optimal (clearer formatting)\n",
    "    axes[0, 0].plot(cumulative_revenue, label='UCB Agent Revenue', linewidth=3, color='blue', alpha=0.8)\n",
    "    axes[0, 0].plot([optimal_revenue * (t+1) for t in range(T)], \n",
    "                    label='Theoretical Optimal', linestyle='--', linewidth=3, color='red', alpha=0.8)\n",
    "    \n",
    "    # Add performance percentage\n",
    "    final_performance = (total_revenue / (optimal_revenue * T)) * 100\n",
    "    axes[0, 0].text(0.02, 0.98, f'Agent Performance: {final_performance:.1f}%\\nof Theoretical Optimal', \n",
    "                    transform=axes[0, 0].transAxes, fontsize=11, va='top',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"yellow\", alpha=0.7))\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Round', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Cumulative Revenue', fontsize=12)\n",
    "    axes[0, 0].set_title('Revenue Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend(fontsize=11)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Regret Analysis (improved with budget-constrained bounds)\n",
    "    t_vals = np.arange(1, T + 1)\n",
    "    axes[0, 1].plot(regret_cumulative, color='red', linewidth=3, label='Actual Cumulative Regret', alpha=0.8)\n",
    "    \n",
    "    # Budget-constrained MAB theoretical bound: √ï(‚àöm OPT + OPT ‚àöm/B)\n",
    "    K = len(agent_params['prices'])\n",
    "    B = agent_params['P']  # Budget/inventory constraint\n",
    "    OPT_per_round = optimal_revenue  # Optimal revenue per round\n",
    "    \n",
    "    # Two components of the budget-constrained bound per round t\n",
    "    # Component 1: ‚àöt * OPT (exploration cost)\n",
    "    # Component 2: OPT * ‚àöt / B (constraint cost)\n",
    "    component_1_coeff = OPT_per_round  # Coefficient for ‚àöt\n",
    "    component_2_coeff = OPT_per_round / B  # Coefficient for ‚àöt\n",
    "    \n",
    "    # Take the maximum of the two components for each time step\n",
    "    sqrt_t = np.sqrt(t_vals)\n",
    "    component_1 = component_1_coeff * sqrt_t\n",
    "    component_2 = component_2_coeff * sqrt_t\n",
    "    \n",
    "    # Combined bound: √ï(‚àöt * max(OPT, OPT/B)) with logarithmic factor\n",
    "    log_factor = np.maximum(1, np.log(t_vals))\n",
    "    budget_constrained_bound = log_factor * np.maximum(component_1, component_2)\n",
    "    \n",
    "    axes[0, 1].plot(t_vals, budget_constrained_bound, '--', color='blue', alpha=0.7, linewidth=2,\n",
    "                   label='Budget-Constrained √ï(‚àöm OPT + OPT ‚àöm/B)')\n",
    "    \n",
    "    # Add regret growth info\n",
    "    final_regret_rate = regret_cumulative[-1] / T\n",
    "    axes[0, 1].text(0.02, 0.98, f'Final Regret Rate: {final_regret_rate:.4f}\\nBudget-Constrained Bound', \n",
    "                    transform=axes[0, 1].transAxes, fontsize=11, va='top',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"lightgreen\", alpha=0.7))\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Round', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Cumulative Regret', fontsize=12)\n",
    "    axes[0, 1].set_title('Regret Growth Analysis', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend(fontsize=11)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Policy Comparison (NEW: side-by-side optimal vs learned)\n",
    "    agent_dist = price_counts / T\n",
    "    x_pos = np.arange(len(prices))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[0, 2].bar(x_pos - width/2, opt_dist, width, label='Optimal Policy', \n",
    "                          color='green', alpha=0.7)\n",
    "    bars2 = axes[0, 2].bar(x_pos + width/2, agent_dist, width, label='Learned Policy', \n",
    "                          color='blue', alpha=0.7)\n",
    "    \n",
    "    # Highlight most used prices\n",
    "    for i, (opt_prob, agent_prob) in enumerate(zip(opt_dist, agent_dist)):\n",
    "        if opt_prob > 0.05:\n",
    "            axes[0, 2].text(i - width/2, opt_prob + 0.02, f'{opt_prob:.2f}', \n",
    "                           ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        if agent_prob > 0.05:\n",
    "            axes[0, 2].text(i + width/2, agent_prob + 0.02, f'{agent_prob:.2f}', \n",
    "                           ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    axes[0, 2].set_xlabel('Price', fontsize=12)\n",
    "    axes[0, 2].set_ylabel('Selection Probability', fontsize=12)\n",
    "    axes[0, 2].set_title('Policy Comparison: Optimal vs Learned', fontsize=14, fontweight='bold')\n",
    "    axes[0, 2].set_xticks(x_pos)\n",
    "    axes[0, 2].set_xticklabels([f'{p:.1f}' for p in prices], rotation=45)\n",
    "    axes[0, 2].legend(fontsize=11)\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Price Selection Frequency (improved)\n",
    "    colors = ['lightcoral' if i == optimal_idx else 'skyblue' for i in range(len(prices))]\n",
    "    bars = axes[1, 0].bar(prices, price_counts, alpha=0.8, color=colors, edgecolor='black', linewidth=1)\n",
    "    \n",
    "    # Add optimal policy overlay\n",
    "    optimal_counts = opt_dist * T\n",
    "    axes[1, 0].plot(prices, optimal_counts, 'ro-', linewidth=3, markersize=8, \n",
    "                   label='Optimal Distribution', alpha=0.8)\n",
    "    \n",
    "    # Improved labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, price_counts)):\n",
    "        if count > 50:  # Only label significant bars\n",
    "            percentage = 100 * count / T\n",
    "            axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + T*0.01,\n",
    "                           f'{percentage:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Price', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Selection Count', fontsize=12)\n",
    "    axes[1, 0].set_title('Price Selection vs Optimal Target', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend(fontsize=11)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Revenue Convergence Analysis (improved)\n",
    "    window_size = min(100, T // 20)  # Larger window for smoother curve\n",
    "    if window_size > 1:\n",
    "        moving_avg = np.convolve(revenues, np.ones(window_size)/window_size, mode='valid')\n",
    "        axes[1, 1].plot(range(window_size-1, T), moving_avg, linewidth=3, \n",
    "                       label=f'Revenue (MA-{window_size})', color='blue', alpha=0.8)\n",
    "    else:\n",
    "        axes[1, 1].plot(revenues, linewidth=1, alpha=0.7, \n",
    "                       label='Revenue per Round', color='blue')\n",
    "        \n",
    "    axes[1, 1].axhline(optimal_revenue, color='red', linestyle='--', linewidth=3,\n",
    "                       label=f'Optimal ({optimal_revenue:.2f})', alpha=0.8)\n",
    "    \n",
    "    # Add convergence zones\n",
    "    axes[1, 1].axhspan(optimal_revenue * 0.9, optimal_revenue * 1.1, alpha=0.1, color='green',\n",
    "                      label='¬±10% of Optimal')\n",
    "    \n",
    "    # Performance indicator\n",
    "    recent_avg = np.mean(revenues[-1000:]) if T > 1000 else np.mean(revenues)\n",
    "    convergence_pct = (recent_avg / optimal_revenue) * 100\n",
    "    axes[1, 1].text(0.02, 0.98, f'Recent Avg Revenue: {recent_avg:.2f}\\nConvergence: {convergence_pct:.1f}%', \n",
    "                    transform=axes[1, 1].transAxes, fontsize=11, va='top',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"lightblue\", alpha=0.7))\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Round', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Revenue per Round', fontsize=12)\n",
    "    axes[1, 1].set_title('Revenue Convergence Analysis', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].legend(fontsize=11)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. Inventory and Constraint Analysis (NEW)\n",
    "    inventory_levels = [agent_params['P'] - sum(sales[:i]) for i in range(T)]\n",
    "    required_rate = [max(0, inv/(T-t)) if t < T else 0 for t, inv in enumerate(inventory_levels)]\n",
    "    \n",
    "    axes[1, 2].plot(inventory_levels, label='Remaining Inventory', linewidth=3, color='orange', alpha=0.8)\n",
    "    axes[1, 2].set_ylabel('Inventory Level', fontsize=12, color='orange')\n",
    "    axes[1, 2].tick_params(axis='y', labelcolor='orange')\n",
    "    \n",
    "    # Add constraint requirement on second y-axis\n",
    "    ax2 = axes[1, 2].twinx()\n",
    "    ax2.plot(required_rate, label='Required Sell Rate', linewidth=2, color='purple', alpha=0.7, linestyle='--')\n",
    "    ax2.axhline(agent_params['P']/T, color='red', linestyle=':', linewidth=2, alpha=0.7,\n",
    "               label=f'Target Rate ({agent_params[\"P\"]/T:.2f})')\n",
    "    ax2.set_ylabel('Required Rate', fontsize=12, color='purple')\n",
    "    ax2.tick_params(axis='y', labelcolor='purple')\n",
    "    \n",
    "    # Final statistics\n",
    "    utilization = ((agent_params['P'] - agent.remaining_inventory) / agent_params['P']) * 100\n",
    "    axes[1, 2].text(0.02, 0.98, f'Inventory Utilization: {utilization:.1f}%\\nSales Rate: {(sum(sales)/T)*100:.1f}%', \n",
    "                    transform=axes[1, 2].transAxes, fontsize=11, va='top',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"lightyellow\", alpha=0.7))\n",
    "    \n",
    "    axes[1, 2].set_xlabel('Round', fontsize=12)\n",
    "    axes[1, 2].set_title('Inventory & Constraint Management', fontsize=14, fontweight='bold')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Combine legends\n",
    "    lines1, labels1 = axes[1, 2].get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    axes[1, 2].legend(lines1 + lines2, labels1 + labels2, loc='center right', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # === Enhanced Average Regret Analysis ===\n",
    "    t_vals = np.arange(1, T + 1)\n",
    "    regret_avg = np.array(regret_cumulative) / t_vals\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create subplot for better layout\n",
    "    gs = plt.GridSpec(2, 2, height_ratios=[2, 1], width_ratios=[3, 1])\n",
    "    \n",
    "    # Main regret plot\n",
    "    ax1 = plt.subplot(gs[0, :])\n",
    "    ax1.plot(t_vals, regret_avg, label=r'$R(t)/t$ (Average Regret)', linewidth=3, color='blue', alpha=0.8)\n",
    "    ax1.axhline(y=0, color='red', linestyle='--', alpha=0.7, linewidth=2, label='Zero Regret (Impossible for Constrained)')\n",
    "    \n",
    "    # Add theoretical convergence line\n",
    "    theoretical_limit = regret_avg[-1000:].mean()  # Approximate steady state\n",
    "    ax1.axhline(y=theoretical_limit, color='green', linestyle=':', alpha=0.7, linewidth=2,\n",
    "               label=f'Steady State ‚âà {theoretical_limit:.4f}')\n",
    "    \n",
    "    # Highlight convergence region\n",
    "    convergence_start = T // 2\n",
    "    ax1.axvspan(convergence_start, T, alpha=0.1, color='green', label='Convergence Region')\n",
    "    \n",
    "    # Add annotations\n",
    "    ax1.text(0.02, 0.98, \n",
    "            f'Final Regret Rate: {regret_avg[-1]:.4f}\\n'\n",
    "            f'Budget-Constrained: √ï(‚àöm OPT + OPT ‚àöm/B)\\n'\n",
    "            f'Per-round rate: √ï(OPT/‚àöm + OPT/‚àö(mB))', \n",
    "            transform=ax1.transAxes, fontsize=12, va='top',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgreen\", alpha=0.8))\n",
    "    \n",
    "    # Add explanation box\n",
    "    ax1.text(0.98, 0.98,\n",
    "            'Budget-Constrained MAB:\\n'\n",
    "            '‚Ä¢ Regret: √ï(‚àöm OPT + OPT ‚àöm/B)\\n'\n",
    "            '‚Ä¢ Two regimes based on budget\\n'\n",
    "            '‚Ä¢ Non-zero convergence possible\\n'\n",
    "            '‚Ä¢ Constraint forces trade-offs',\n",
    "            transform=ax1.transAxes, fontsize=11, va='top', ha='right',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "    \n",
    "    ax1.set_xlabel('Round', fontsize=14)\n",
    "    ax1.set_ylabel('Average Regret per Round', fontsize=14)\n",
    "    ax1.set_title('Average Regret Convergence Analysis\\n(Why It Doesn\\'t Reach Zero in Constrained Problems)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    ax1.legend(fontsize=12, loc='center left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Zoomed view of final portion\n",
    "    ax2 = plt.subplot(gs[1, 0])\n",
    "    final_portion = max(1000, T//10)\n",
    "    ax2.plot(t_vals[-final_portion:], regret_avg[-final_portion:], linewidth=3, color='blue', alpha=0.8)\n",
    "    ax2.axhline(y=theoretical_limit, color='green', linestyle=':', alpha=0.7, linewidth=2)\n",
    "    ax2.set_xlabel('Round (Final Portion)', fontsize=12)\n",
    "    ax2.set_ylabel('Average Regret', fontsize=12)\n",
    "    ax2.set_title('Convergence Detail', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution comparison\n",
    "    ax3 = plt.subplot(gs[1, 1])\n",
    "    learned_dist = price_counts / T\n",
    "    comparison_data = []\n",
    "    price_labels = []\n",
    "    \n",
    "    for i, price in enumerate(prices):\n",
    "        if opt_dist[i] > 0.01 or learned_dist[i] > 0.01:  # Only show significant prices\n",
    "            comparison_data.append([opt_dist[i], learned_dist[i]])\n",
    "            price_labels.append(f'{price:.1f}')\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_data = np.array(comparison_data)\n",
    "        x_pos = np.arange(len(price_labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax3.bar(x_pos - width/2, comparison_data[:, 0], width, label='Optimal', alpha=0.7, color='green')\n",
    "        ax3.bar(x_pos + width/2, comparison_data[:, 1], width, label='Learned', alpha=0.7, color='blue')\n",
    "        \n",
    "        ax3.set_ylabel('Probability', fontsize=12)\n",
    "        ax3.set_title('Policy Mismatch\\n(Source of Regret)', fontsize=12, fontweight='bold')\n",
    "        ax3.set_xticks(x_pos)\n",
    "        ax3.set_xticklabels(price_labels)\n",
    "        ax3.legend(fontsize=10)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # === Regret Growth Analysis ===\n",
    "    try:\n",
    "        from scipy.stats import linregress\n",
    "        \n",
    "        # Fit regret growth in log-log scale\n",
    "        # Use a larger portion of data for more stable analysis\n",
    "        start_idx = max(K, T//4)  # Start after exploration phase\n",
    "        t_vals_log = np.log(t_vals[start_idx:])  \n",
    "        regret_vals_to_analyze = np.maximum(regret_cumulative[start_idx:], 1e-6)\n",
    "        regret_log = np.log(regret_vals_to_analyze)\n",
    "        \n",
    "        if len(t_vals_log) > 20:  # Ensure enough data points\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(t_vals_log, regret_log)\n",
    "            print(f\"\\n=== Regret Growth Analysis ===\")\n",
    "            print(f\"Estimated regret exponent (log-log slope): {slope:.3f}\")\n",
    "            print(f\"R-squared: {r_value**2:.3f}\")\n",
    "            # Compute theoretical log-log slope based on inventory regime\n",
    "            K = len(agent_params['prices'])\n",
    "            P = agent_params['P']\n",
    "            T = agent_params['T']\n",
    "            \n",
    "            # Budget-constrained MAB theoretical analysis\n",
    "            # For MAB with budget constraint, regret bound is √ï(‚àöm OPT + OPT ‚àöm/B)\n",
    "            B = P  # Budget constraint (inventory)\n",
    "            OPT_per_round = optimal_revenue  # Optimal revenue per round\n",
    "            OPT_total = OPT_per_round * T  # Total optimal value\n",
    "            \n",
    "            # Two components of budget-constrained regret\n",
    "            component_1 = np.sqrt(T) * OPT_per_round  # ‚àöm * OPT\n",
    "            component_2 = OPT_per_round * np.sqrt(T) / B  # OPT * ‚àöm/B\n",
    "            \n",
    "            # Which component dominates depends on budget level\n",
    "            if B >= np.sqrt(T):\n",
    "                # High budget: first component dominates, behaves like ‚àöT\n",
    "                theoretical_loglog_slope = 0.5\n",
    "                regime = f\"High Budget (B={B:.0f} ‚â• ‚àöT={np.sqrt(T):.0f}): ‚àöm OPT dominates\"\n",
    "                dominant_bound = component_1\n",
    "            else:\n",
    "                # Low budget: second component dominates, grows like ‚àöT/B\n",
    "                theoretical_loglog_slope = 0.5  # Still ‚àöT, but with worse constant\n",
    "                regime = f\"Low Budget (B={B:.0f} < ‚àöT={np.sqrt(T):.0f}): OPT ‚àöm/B dominates\" \n",
    "                dominant_bound = component_2\n",
    "            \n",
    "            print(f\"Budget-constrained regret bound: √ï(‚àöm OPT + OPT ‚àöm/B)\")\n",
    "            print(f\"Component 1 (‚àöm OPT): {component_1:.0f}\")\n",
    "            print(f\"Component 2 (OPT ‚àöm/B): {component_2:.0f}\")\n",
    "            print(f\"Expected log-log slope: {theoretical_loglog_slope:.3f}\")\n",
    "            print(f\"Regime: {regime}\")\n",
    "            print(f\"Dominant bound value: {dominant_bound:.0f}\")\n",
    "            \n",
    "            # Additional analysis: check if regret is sublinear\n",
    "            if slope < 1.0:\n",
    "                print(f\"‚úì Regret growth is sublinear (slope < 1.0)\")\n",
    "            else:\n",
    "                print(f\"‚ö† Regret growth appears linear or superlinear\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Could not perform regret growth analysis: {e}\")\n",
    "\n",
    "    # === Summary Statistics ===\n",
    "    # Use the improved regret calculation\n",
    "    final_regret = regret_cumulative[-1]\n",
    "    convergence_rate = 100 * (1 - final_regret/theoretical_optimal_total)\n",
    "    sales_rate = 100 * sum(sales) / T\n",
    "    \n",
    "    print(f\"\\n=== Summary Statistics ===\")\n",
    "    print(f\"Convergence rate: {convergence_rate:.1f}%\")\n",
    "    print(f\"Average revenue per round: {total_revenue/T:.2f}\")\n",
    "    print(f\"Revenue std deviation: {np.std(revenues):.2f}\")\n",
    "    print(f\"Total sales made: {sum(sales)}\")\n",
    "    print(f\"Sales rate: {sales_rate:.1f}%\")\n",
    "    print(f\"Remaining inventory: {agent.remaining_inventory}\")\n",
    "    print(f\"Total regret (improved calculation): {final_regret:.2f}\")\n",
    "    print(f\"Average regret per round: {final_regret/T:.4f}\")\n",
    "    \n",
    "    # Inventory utilization analysis\n",
    "    inventory_used = agent_params['P'] - agent.remaining_inventory\n",
    "    utilization_rate = 100 * inventory_used / agent_params['P']\n",
    "    print(f\"Inventory utilization: {utilization_rate:.1f}%\")\n",
    "\n",
    "    return {\n",
    "        'cumulative_regret': regret_cumulative,\n",
    "        'cumulative_revenue': cumulative_revenue,\n",
    "        'optimal_revenue': optimal_revenue,\n",
    "        'convergence_rate': convergence_rate,\n",
    "        'sales_rate': sales_rate,\n",
    "        'inventory_utilization': utilization_rate\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
