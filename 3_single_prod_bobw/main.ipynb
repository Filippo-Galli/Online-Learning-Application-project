{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331bc84",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStationaryStochasticPricingEnvironment:\n",
    "    \"\"\"\n",
    "    Non-Stationary stochastic environment, with the distribution over customer valuations for a single product changing quickly over time.\n",
    "    \"\"\"\n",
    "    def __init__(self, valuation_distributions, demand_noise_std=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            valuation_distributions: A list of different scipy.stats distributions representing customer valuations\n",
    "            demand_noise_std: Standard deviation of noise in demand probability\n",
    "            current_round: Variable that keep the count of the round been played.\n",
    "        \"\"\"\n",
    "        self.valuation_dist = valuation_distributions\n",
    "        self.noise_std = demand_noise_std\n",
    "        self.current_round = 0\n",
    "\n",
    "\n",
    "    def demand_probability(self, price):\n",
    "        \"\"\"\n",
    "        Calculate the probability that a customer buys at given price.\n",
    "        This is P(valuation >= price) with some noise.\n",
    "        \"\"\"\n",
    "        # Retrieve the distribution associated with the current round\n",
    "        current_dist = self.valuation_dist[self.current_round]\n",
    "\n",
    "        # Base probability: customers buy if their valuation >= price\n",
    "        base_prob = 1 - current_dist.cdf(price)\n",
    "        \n",
    "        # Add some noise to make it stochastic\n",
    "        noise = np.random.normal(0, self.noise_std)\n",
    "        \n",
    "        prob = base_prob + noise\n",
    "        # Ensure probability is in [0, 1]\n",
    "        return np.clip(prob, 0, 1)\n",
    "    \n",
    "\n",
    "    def simulate_round(self, price):\n",
    "        \n",
    "        \"\"\"\n",
    "        Simulate one pricing round.\n",
    "        Returns: (sale_made, revenue)\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve the distribution associated with the current round\n",
    "        current_dist = self.valuation_dist[self.current_round]\n",
    "\n",
    "        # Draw a random customer valuation from the distribution\n",
    "        valuation = current_dist.rvs()\n",
    "        \n",
    "        # Customer purchases if their valuation >= price\n",
    "        sale_made = 1 if valuation >= price else 0\n",
    "        \n",
    "        # Revenue is price if sale was made, 0 otherwise\n",
    "        revenue = sale_made * price\n",
    "        \n",
    "        # Updating rounds' count\n",
    "        self.current_round += 1\n",
    "\n",
    "        return sale_made, revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa861eb",
   "metadata": {},
   "source": [
    "Poi quando vado a definire l'env conf, definisco una funzione lambda per media e std. Creo un vettore di distribuzioni da passare all'environment.\n",
    "Forse addirittura ha senso cambiare la distribuzione (e non farle solo normale), per creare un cambiamento piÃ¹ sharp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88e9a9",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimalDualAgent:\n",
    "    \"\"\"\n",
    "    Primal-dual agent for dynamic pricing with inventory constraints in highly non-stationary environments.\n",
    "    \n",
    "    This agent implements a primal-dual based algorithm adapted for:\n",
    "    1. Dynamic pricing (instead of traditional MAB rewards)\n",
    "    2. Inventory constraints (limited number of products to sell)\n",
    "    3. Dual optimization: maximize revenue while respecting inventory constraint\n",
    "    \n",
    "    The algorithm maintains upper confidence bounds on revenue (f_UCB) and \n",
    "    lower confidence bounds on demand probability (c_LCB), then solves a \n",
    "    linear program to find the optimal price distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, P, T, prices, eta = 0.1, ema_alpha = 0.2, lambda0 = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize the UCB agent for constrained dynamic pricing.\n",
    "        \n",
    "        Args:\n",
    "            P: Total inventory (number of products available)\n",
    "            T: Time horizon (number of rounds)\n",
    "            prices: List of available prices to choose from\n",
    "            eta: learning rate for dual update\n",
    "        \"\"\"\n",
    "        k = len(prices)\n",
    "\n",
    "        # Environment parameters\n",
    "        self.prices = prices  # Available price options\n",
    "        self.K = k           # Number of price arms\n",
    "        self.T = T           # Total number of rounds\n",
    "        self.t = 0           # Current round number\n",
    "        \n",
    "        # Primal-dual parameters\n",
    "        self.rho = P/float(T)                         # Target selling rate\n",
    "        self.eta = eta\n",
    "        self.lambda_ = float(lambda0)\n",
    "        self.lambda_upper = 1.0 / max(self.rho, 1e-8) # Projection upper bound\n",
    "        \n",
    "        # Inventory management\n",
    "        self.inventory = P  # Initial inventory\n",
    "        self.remaining_inventory = P # Current remaining inventory \n",
    "\n",
    "        # EMA estimates (reactive to non-stationarity)\n",
    "        self.ema_alpha = ema_alpha\n",
    "        self.f_est = np.zeros(self.K)        # Estimated expected revenue for each price\n",
    "        self.c_est = np.ones(self.K) * 1e-6  # Estimated demand probability for each price, initialized with small positive number to avoid degenerate LP constraints\n",
    "        \n",
    "        \n",
    "        self.N_pulls = np.zeros(k)           # Number of times each price was selected\n",
    "        self.current_price_idx = None\n",
    "\n",
    "        # History tracking\n",
    "        self.history = {\n",
    "            'prices': [],     # Selected prices over time\n",
    "            'rewards': [],    # Observed revenues over time\n",
    "            'purchases': [],  # Purchase indicators over time\n",
    "            'lambda': [],     # Lambda parameter\n",
    "            'inventory': []   # Inventory levels over time\n",
    "        }\n",
    "    \n",
    "    def select_price(self):\n",
    "        \"\"\"\n",
    "        Select the next price using LP with inventory constraints.\n",
    "        \n",
    "        Solve LP : maximize sum_i gamma_i * (f_est_i - lambda*c_est_i)\n",
    "                   subject to sum gamma = 1, gamma >= 0\n",
    "        \n",
    "        If LP fails or objective degenerate, fallback to greedy argmax(f_est - lambda*c_est)         \n",
    "        Then sample price according to gamma\n",
    "\n",
    "        Returns:\n",
    "            Selected price, or np.nan if no inventory remaining\n",
    "        \"\"\"\n",
    "        # No inventory left - cannot make meaningful pricing decisions\n",
    "        if self.remaining_inventory < 1:\n",
    "            self.current_price_idx = np.argmax(self.prices)  # Arbitrary selection\n",
    "            return np.nan\n",
    "            \n",
    "        # Objective coefficients (with - since we want to maximize, while linprog does minimization)\n",
    "        obj = -(self.f_est - self.lambda_ * self.c_est)\n",
    "        \n",
    "        # Constraints: sum gamma = 1, gamma>=0\n",
    "        A_eq = [np.ones(self.K)]\n",
    "        b_eq = [1.0]\n",
    "        bounds = [(0.0, 1.0) for _ in range(self.K)]\n",
    "        try:            \n",
    "            res = optimize.linprog(c = obj, A_eq = A_eq, b_eq = b_eq, bounds = bounds, method = 'highs')\n",
    "            if res.success:\n",
    "                gamma = res.x\n",
    "                gamma = np.maximum(gamma,0.0)\n",
    "                s = gamma.sum()\n",
    "                if s<=1e-12:\n",
    "                    gamma = np.ones(self.K) / self.K\n",
    "                else:\n",
    "                    gamma = gamma / s\n",
    "            else:\n",
    "                # fallback: deterministic greedy on adjusted objective\n",
    "                scores = self.f_Est - self.lambda_ * self.c_est\n",
    "                best = np.argmax(scores)\n",
    "                gamma = np.zeros(self.K)\n",
    "                gamma[best] = 1.0\n",
    "        except Exception:\n",
    "            scores = self.f_est - self.lambda_ * self.c_est\n",
    "            best = np.argmax(scores)\n",
    "            gamma = np.zeros(self.K)\n",
    "            gamma[best] = 1.0\n",
    "        \n",
    "        # Sample according to gamma\n",
    "        idx = np.random.choice(self.K, p=gamma)\n",
    "        self.current_price_idx = int(idx)\n",
    "        return float(self.prices[self.current_price_idx])\n",
    "    \n",
    "    def update(self, reward, purchased):\n",
    "        \"\"\"\n",
    "        Update agent's statistics based on observed outcome.\n",
    "        \n",
    "        Args:\n",
    "            reward: Revenue obtained (price if purchased, 0 otherwise)\n",
    "            purchased: Boolean indicating if purchase was made\n",
    "        \"\"\"\n",
    "        idx = self.current_price_idx\n",
    "        \n",
    "        # Update pull count\n",
    "        self.N_pulls[idx] += 1\n",
    "        \n",
    "        # Update EMA for f_est, reward is either price, if purchased, or 0\n",
    "        prev_f = self.f_est[idx]\n",
    "        self.f_est[idx] = (1-self.ema_alpha)*prev_f+self.ema_alpha*reward\n",
    "        \n",
    "        # Dual update: lambda <- proj_[0, 1/rho] (lambda - eta*(rho-c_t(b_t)))\n",
    "        purchased_indicator = 1.0 if purchased else 0.0\n",
    "        grad = (self.rho - purchased_indicator)\n",
    "        self.lambda_ = self.lambda_ - self.eta * grad\n",
    "        # Project\n",
    "        self.lambda_ = np.clip(self.lambda_, 0.0, self.lambda_upper)\n",
    "\n",
    "        # Update inventory only if purchase was actually made and inventory available\n",
    "        if purchased and self.remaining_inventory > 0:\n",
    "            self.remaining_inventory -= 1\n",
    "        elif purchased and self.remaining_inventory <= 0:\n",
    "            # This shouldn't happen with proper price selection, but handle gracefully\n",
    "            reward = 0\n",
    "            purchased = False\n",
    "        \n",
    "        # Record history\n",
    "        self.history['prices'].append(self.prices[idx])\n",
    "        self.history['rewards'].append(reward)\n",
    "        self.history['purchases'].append(purchased)\n",
    "        self.history['lambda'].append(self.lambda_)\n",
    "        self.history['inventory'].append(self.remaining_inventory)\n",
    "        \n",
    "        # Increment time\n",
    "        self.t += 1\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\n",
    "            't': self.t,\n",
    "            'remaining_inventory': self.remaining_inventory,\n",
    "            'lambda': self.lambda_,\n",
    "            'f_est': self.f_est.copy(),\n",
    "            'c_est': self.c_est.copy()\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
