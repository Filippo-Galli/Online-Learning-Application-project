{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78563f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "from scipy.optimize import linear_sum_assignment, linprog\n",
    "from scipy import optimize\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "# Reload the module to get the latest changes\n",
    "import importlib\n",
    "if 'UCB_SW' in sys.modules:\n",
    "    importlib.reload(sys.modules['UCB_SW'])\n",
    "\n",
    "from UCB_SW import UCBSWAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b41171",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # For reproducibility\n",
    "T = 10000          # total time horizon\n",
    "Inventory_frac = 0.30 \n",
    "W = int(np.sqrt(T))  # sliding window size\n",
    "n_products = 3\n",
    "B = T * Inventory_frac * n_products  # total inventory across all products\n",
    "n_intervals = 10\n",
    "prices_per_product = 7\n",
    "prices = np.linspace(0.1, 0.9, prices_per_product)\n",
    "\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc1430",
   "metadata": {},
   "source": [
    "# Slightly Non Stationary Environment\n",
    "\n",
    "### We will simulate a slightly non-stationary environment by partitioning the rounds into intervals. The distribution in each intervall will be costant and at each intervall the mean and standard deviation will be incremented by a quantity sampled from a distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a79194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStationaryEnvironment:\n",
    "    def __init__(self, possible_prices, num_products, change_probability=0.01, mu_lim=(0.3,0.7), std_lim=(0.05,0.15), rng=None):\n",
    "        \"\"\"\n",
    "        Initialize a non-stationary environment for pricing optimization.\n",
    "        \n",
    "        Args:\n",
    "            possible_prices: Array of possible price points\n",
    "            num_products: Number of products in the environment\n",
    "            change_probability: Probability of parameter change at each timestep\n",
    "            mu_lim: Tuple defining limits for mean valuations (min, max)\n",
    "            std_lim: Tuple defining limits for standard deviations (min, max)\n",
    "            rng: Random number generator instance\n",
    "        \"\"\"\n",
    "        self.rng = rng if rng else np.random.default_rng()\n",
    "        self.possible_prices = possible_prices\n",
    "        self.num_products = num_products\n",
    "        self.change_probability = change_probability\n",
    "        self.mu_lim = mu_lim\n",
    "        self.std_lim = std_lim\n",
    "\n",
    "    def sample_parameters(self, T):\n",
    "        \"\"\"\n",
    "        Sample mean and standard deviation parameters for each timestep.\n",
    "        Creates piecewise constant distributions that change at random intervals.\n",
    "        \n",
    "        Args:\n",
    "            T: Total number of timesteps\n",
    "        \"\"\"\n",
    "        means_per_timestep = np.empty((T, self.num_products))\n",
    "        stds_per_timestep = np.empty((T, self.num_products))\n",
    "\n",
    "        # Determine when changes occur for each product at each timestep\n",
    "        change_triggers = np.random.random(size=(T, self.num_products))\n",
    "        change_mask = change_triggers < self.change_probability\n",
    "        n_changes = np.sum(change_mask)\n",
    "        \n",
    "        # Sample initial parameters for each product\n",
    "        starting_means = self.rng.uniform(low=self.mu_lim[0], high=self.mu_lim[1], size=self.num_products)\n",
    "        starting_stds = self.rng.uniform(low=self.std_lim[0], high=self.std_lim[1], size=self.num_products)\n",
    "\n",
    "        # Sample new parameters for all change events\n",
    "        means = self.rng.uniform(low=self.mu_lim[0], high=self.mu_lim[1], size=n_changes)\n",
    "        stds = self.rng.uniform(low=self.std_lim[0], high=self.std_lim[1], size=n_changes)\n",
    "\n",
    "        # Get indices where changes occur\n",
    "        change_timesteps, change_products = np.nonzero(change_mask)\n",
    "        \n",
    "        # Fill parameters for each product across all timesteps\n",
    "        for product in range(self.num_products):\n",
    "            product_mask = change_products == product\n",
    "            if np.sum(product_mask) == 0:\n",
    "                # No changes for this product - use starting values throughout\n",
    "                means_per_timestep[:, product] = starting_means[product]\n",
    "                stds_per_timestep[:, product] = starting_stds[product]\n",
    "                continue\n",
    "            \n",
    "            # Get change points and new values for this product\n",
    "            product_change_steps = change_timesteps[product_mask]\n",
    "            product_means = means[product_mask]\n",
    "            product_stds = stds[product_mask]\n",
    "            \n",
    "            # Fill initial period (before first change)\n",
    "            means_per_timestep[:product_change_steps[0], product] = starting_means[product]\n",
    "            stds_per_timestep[:product_change_steps[0], product] = starting_stds[product]\n",
    "            \n",
    "            # Fill periods between changes\n",
    "            for i in range(len(product_change_steps) - 1):\n",
    "                start_step = product_change_steps[i]\n",
    "                end_step = product_change_steps[i+1]\n",
    "                means_per_timestep[start_step:end_step, product] = product_means[i]\n",
    "                stds_per_timestep[start_step:end_step, product] = product_stds[i]\n",
    "\n",
    "            # Fill final period (after last change)\n",
    "            means_per_timestep[product_change_steps[-1]:, product] = product_means[-1]\n",
    "            stds_per_timestep[product_change_steps[-1]:, product] = product_stds[-1]\n",
    "            \n",
    "        # Store generated parameters\n",
    "        self.means = means_per_timestep\n",
    "        self.stds = stds_per_timestep\n",
    "\n",
    "    def sample_valuation(self, t):\n",
    "        \"\"\"\n",
    "        Sample customer valuations from normal distributions for timestep t.\n",
    "        \n",
    "        Args:\n",
    "            t: Current timestep\n",
    "            \n",
    "        Returns:\n",
    "            Clipped valuations in [0,1] range for current timestep only\n",
    "        \"\"\"\n",
    "        # Sample valuations for current timestep only\n",
    "        current_means = self.means[t]  # Shape: (num_products,)\n",
    "        current_stds = self.stds[t]    # Shape: (num_products,)\n",
    "        valuations = self.rng.normal(loc=current_means, scale=current_stds)\n",
    "        return np.clip(valuations, 0, 1)\n",
    "\n",
    "    def buyer_response(self, prices, valuations):\n",
    "        \"\"\"\n",
    "        Determine buyer purchase decisions based on prices vs valuations.\n",
    "        \n",
    "        Args:\n",
    "            prices: Array of prices for each product\n",
    "            valuations: Array of customer valuations for each product\n",
    "            \n",
    "        Returns:\n",
    "            Boolean array indicating purchases (True if valuation > price)\n",
    "        \"\"\"\n",
    "        return (prices < valuations).astype(bool)\n",
    "\n",
    "    def get_rewards(self, t, prices):\n",
    "        \"\"\"\n",
    "        Get rewards for given prices at timestep t.\n",
    "        \n",
    "        Args:\n",
    "            t: Current timestep\n",
    "            prices: Array of prices for each product\n",
    "            \n",
    "        Returns:\n",
    "            Array of rewards (price if purchased, 0 otherwise)\n",
    "        \"\"\"\n",
    "        # Sample customer valuations for this timestep only\n",
    "        valuations = self.sample_valuation(t)\n",
    "        \n",
    "        # Determine purchases and calculate rewards\n",
    "        purchases = self.buyer_response(prices, valuations)\n",
    "        rewards = prices * purchases\n",
    "        \n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4793c5",
   "metadata": {},
   "source": [
    "### Evolution of the mean and standard deviation during time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d43bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12345)\n",
    "env = NonStationaryEnvironment(prices, n_products, change_probability=0.001, rng=rng)\n",
    "env.sample_parameters(T)\n",
    "\n",
    "# Plot delle medie\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "for p in range(n_products):\n",
    "    plt.plot(env.means[:, p], label=f\"Prodotto {p} (mean)\")\n",
    "\n",
    "plt.title(\"Evoluzione delle medie nel tempo\")\n",
    "plt.xlabel(\"Round t\")\n",
    "plt.ylabel(\"Valore medio (μ)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot delle deviazioni standard\n",
    "#plt.figure(figsize=(12,5))\n",
    "\n",
    "#for p in range(n_products):\n",
    "#    plt.plot(env.stds[p], label=f\"Prodotto {p} (std)\")\n",
    "\n",
    "#plt.title(\"Evoluzione delle deviazioni standard nel tempo\")\n",
    "#plt.xlabel(\"Round t\")\n",
    "#plt.ylabel(\"Deviazione standard (σ)\")\n",
    "#plt.legend()\n",
    "#plt.grid(True)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e566b3",
   "metadata": {},
   "source": [
    "Now we compare the performances of our learner with the baseline. The baseline knows the true expected reward of each arm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725e1fd",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53aae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the revenue of the baseline strategy that knows the distribution of valuations\n",
    "def baseline_revenue(env, B, T):\n",
    "     \n",
    "    K = len(env.possible_prices)\n",
    "\n",
    "    #Reward for every product at each timestep\n",
    "    reward_history = np.full((T, env.num_products), 0.0)\n",
    "\n",
    "    optimal_gamma = optimal_policy(env, B, T)\n",
    "    inventory = B\n",
    "\n",
    "    for t in range(T):\n",
    "        #print(\"Step \",t , \" in the optimal policy\")\n",
    "        if inventory < env.num_products:\n",
    "            print(f\"Inventory deployed at step {t}\")\n",
    "            break\n",
    "        arms = np.empty(env.num_products, dtype=int)\n",
    "        for product in range(env.num_products):\n",
    "            #For every product sample a price (the index)\n",
    "            arms[product] = np.random.choice(K, p = optimal_gamma[product])\n",
    "       \n",
    "        #Price selected for each product\n",
    "        prices = env.possible_prices[arms]\n",
    "\n",
    "        #Reward for each product at time t: if valuation > price -> reward = price, else 0\n",
    "        rewards = env.get_rewards(t, prices)\n",
    "\n",
    "        #Inventory consumed\n",
    "        inventory -= np.count_nonzero(rewards)\n",
    "\n",
    "        reward_history[t] = rewards\n",
    "    #Return the cumulative reward of each product overtime    \n",
    "    return reward_history\n",
    "\n",
    "\n",
    "#Compute the optimal policy knowing the true expected rewards and purchase probabilities\n",
    "def optimal_policy(env, B, T):\n",
    "    \n",
    "    n = env.num_products\n",
    "    K = len(env.possible_prices)\n",
    "\n",
    "    #Compute true expected reward and demand probabilities for each product and price\n",
    "    purchase_prob = np.zeros((T, n, K))\n",
    "\n",
    "    for t in range(T):\n",
    "         for i in range(n):\n",
    "            for j in range(K):\n",
    "                 #P(valuation > price) = 1 - CDF(price) for each product and for each timestep\n",
    "                 purchase_prob[t,i,j] = 1 - norm.cdf(env.possible_prices[j], loc=env.means[t,i], scale=env.stds[t,i])\n",
    "\n",
    "    #Compute the average purchase probability over time for each product and price\n",
    "    avg_purchase_prob = purchase_prob.mean(axis=0)\n",
    "\n",
    "    #Compute the expected reward for each product and price\n",
    "    expected_rewards = np.zeros((n, K))\n",
    "    for i in range(n):\n",
    "         expected_rewards[i,:] = env.possible_prices * avg_purchase_prob[i,:]\n",
    "\n",
    "    #Initialize the coefficient matrix for equality constraints\n",
    "    A_eq = np.zeros((n, n*K))\n",
    "    b_eq = np.ones(n)\n",
    "\n",
    "    for dim in range(n):\n",
    "        A_eq[dim,dim*K:(dim+1)*K] = 1\n",
    "\n",
    "    c = -expected_rewards.flatten()\n",
    "    A_ub = [avg_purchase_prob.flatten()]\n",
    "    b_ub = [B/T]\n",
    "    \n",
    "\n",
    "    # Solve linear program\n",
    "    try:\n",
    "        res = optimize.linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq = b_eq,\n",
    "                                bounds=(0,1))\n",
    "        if res.success:\n",
    "            gamma_flat = res.x\n",
    "            # Reshape gamma back to (n_products, n_price_options)\n",
    "            gamma = gamma_flat.reshape((n, K))\n",
    "\n",
    "            # Ensure valid probability distribution for each product\n",
    "            for product_idx in range(n):\n",
    "                gamma[product_idx, :] = np.maximum(gamma[product_idx, :], 0)\n",
    "                sum_gamma = np.sum(gamma[product_idx, :])\n",
    "                if sum_gamma > 0:\n",
    "                        gamma[product_idx, :] /= sum_gamma\n",
    "                else:\n",
    "                        # Fallback: uniform distribution for this product\n",
    "                        gamma[product_idx, :] = np.ones(K) / K\n",
    "\n",
    "            return gamma\n",
    "        else:\n",
    "            # Fallback: uniform distribution for all products\n",
    "            return np.ones((n, K)) / K\n",
    "    except Exception as e:\n",
    "        print(f\"LP Error: {e}\")\n",
    "        # Fallback: uniform distribution for all products\n",
    "        return np.ones((n, K)) / K\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4285026",
   "metadata": {},
   "source": [
    "## Initialize the enviroment and Run a simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892293b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "rng = np.random.default_rng(42)\n",
    "env = NonStationaryEnvironment(prices, n_products, change_probability=0.001, rng=rng)\n",
    "env.sample_parameters(T)\n",
    "\n",
    "# Initialize the agent with improved inventory management\n",
    "agent = UCBSWAgent(\n",
    "    n_products=n_products, \n",
    "    price_options=prices, \n",
    "    inventory=B, \n",
    "    T=T, \n",
    "    window_size=W,\n",
    ")\n",
    "\n",
    "# total_reward = 0\n",
    "reward_history = np.zeros((T, env.num_products))\n",
    "inventory_consumed = 0\n",
    "\n",
    "for t in range(T):\n",
    "\n",
    "    if t % (T//10) == 0:\n",
    "        print(f\"Time step: {t}, Remaining inventory: {agent.remaining_inventory}\")\n",
    "\n",
    "    # Agent selects a price for each product - returns (product_subset, prices)\n",
    "    product_subset, selected_prices = agent.select_action()\n",
    "    if len(product_subset) == 0:\n",
    "        print(f\"Not enough inventory left at time step {t}\")\n",
    "        print(f\"Total inventory consumed: {inventory_consumed}\")\n",
    "        break\n",
    "\n",
    "    # Create full price vector for all products\n",
    "    price = np.zeros(env.num_products)\n",
    "    price[product_subset] = selected_prices\n",
    "\n",
    "    # Vector of the rewards of the arm\n",
    "    f_t = env.get_rewards(t, price)\n",
    "    \n",
    "    # Vector of the cost of the arm\n",
    "    c_t = (f_t != 0).astype(int)\n",
    "    inventory_consumed += np.sum(c_t)\n",
    "    reward_history[t] = f_t\n",
    "    agent.update(f_t, c_t)\n",
    "\n",
    "print(f\"Simulation completed. Total inventory consumed: {inventory_consumed}\")\n",
    "\n",
    "# Cumulative reward of each product over time\n",
    "agent_cumulative_reward = reward_history.cumsum(axis=0)\n",
    "\n",
    "# Reward history of the baseline\n",
    "baseline_reward_history = baseline_revenue(env, B, T)\n",
    "baseline_cumulative_reward = baseline_reward_history.cumsum(axis=0)\n",
    "\n",
    "print(f\"Agent total reward: {agent_cumulative_reward[-1].sum():.2f}\")\n",
    "print(f\"Baseline total reward: {baseline_cumulative_reward[-1].sum():.2f}\")\n",
    "print(f\"Performance ratio: {agent_cumulative_reward[-1].sum() / baseline_cumulative_reward[-1].sum():.3f}\")\n",
    "print(f\"Inventory utilization: {inventory_consumed / B:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d044e4f",
   "metadata": {},
   "source": [
    "# Plot of the cumulative rewards and regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637c3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regret at each time step\n",
    "regret = (baseline_cumulative_reward - agent_cumulative_reward).sum(axis=1)\n",
    "\n",
    "# More appropriate theoretical bound for sliding window UCB with non-stationarity\n",
    "theoretical_regret = [(t + 1)**(2/3) * np.log(t + 1) for t in range(len(regret))]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(agent_cumulative_reward)\n",
    "plt.plot(agent_cumulative_reward.sum(axis=1))\n",
    "plt.plot(baseline_cumulative_reward, linestyle=\"--\")\n",
    "plt.plot(baseline_cumulative_reward.sum(axis=1), linestyle=\"--\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Reward of combinatorial-UCB agent\")\n",
    "plt.legend([\"UCB-SW product 1\", \"UCB-SW product 2\", \"UCB-SW product 3\", \"UCB-SW agent total\", \"baseline product 1\", \"baseline product 2\", \"baseline product 3\", \"baseline total\"])\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(regret, label=\"Actual Regret\", color='red')\n",
    "plt.plot(theoretical_regret, label=\"Theoretical Regret Bound O(T^(2/3)·log T)\", linestyle=\"--\")\n",
    "plt.axhline(0, color='black', linestyle='--',linewidth=0.7)\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.title(\"Regret Over Time (UCB-SW Agent)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "t_vals = np.arange(len(regret)) + 1  # Avoid division by zero\n",
    "plt.plot(regret/t_vals, label=\"Regret/T\", color='blue')\n",
    "plt.axhline(0, color='black', linestyle='--',linewidth=0.7)\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Regret/T\")\n",
    "plt.title(\"R_t/t (UCB-SW Agent)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final regret: {regret[-1]:.2f}\")\n",
    "print(f\"Theoretical bound at T: {theoretical_regret[-1]:.2f}\")\n",
    "print(f\"Regret/Theoretical ratio: {regret[-1]/theoretical_regret[-1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a1d464",
   "metadata": {},
   "source": [
    "# Comparison Combinatorial UCB vs UCB-SW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50603193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "# Import UCB_multi_constr\n",
    "from UCB1_multi_constr import UCBMatchingAgent\n",
    "\n",
    "def run_simulation(agent_class, agent_params, env, T, B, description=\"Agent\"):\n",
    "    \"\"\"\n",
    "    Run a simulation with a specific agent and return the results.\n",
    "    \n",
    "    Args:\n",
    "        agent_class: The agent class to instantiate\n",
    "        agent_params: Dictionary of parameters for the agent\n",
    "        env: The environment instance\n",
    "        T: Time horizon\n",
    "        B: Total inventory\n",
    "        description: Description for logging\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with simulation results\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running simulation with {description} ---\")\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent = agent_class(**agent_params)\n",
    "    \n",
    "    # Track results\n",
    "    reward_history = np.zeros((T, env.num_products))\n",
    "    inventory_consumed = 0\n",
    "    \n",
    "    for t in range(T):\n",
    "        if t % (T//10) == 0:\n",
    "            print(f\"Time step: {t}, Remaining inventory: {agent.remaining_inventory}\")\n",
    "        \n",
    "        # Agent selects action based on its interface\n",
    "        if hasattr(agent, 'select_action') and 'UCBSWAgent' in str(type(agent)):  # UCB_SW interface\n",
    "            product_subset, selected_prices = agent.select_action()\n",
    "            if len(product_subset) == 0:\n",
    "                print(f\"Not enough inventory left at time step {t}\")\n",
    "                break\n",
    "            \n",
    "            # Create full price vector for all products\n",
    "            prices = np.zeros(env.num_products)\n",
    "            prices[product_subset] = selected_prices\n",
    "            \n",
    "            rewards = env.get_rewards(t, prices)\n",
    "            costs = (rewards != 0).astype(int)\n",
    "            \n",
    "            inventory_consumed += np.sum(costs)\n",
    "            reward_history[t] = rewards\n",
    "            agent.update(rewards, costs)\n",
    "            \n",
    "        elif hasattr(agent, 'select_action'):  # UCB_multi_constr interface\n",
    "            product_subset, prices = agent.select_action()\n",
    "            if len(product_subset) == 0:\n",
    "                print(f\"Not enough inventory left at time step {t}\")\n",
    "                break\n",
    "            \n",
    "            # Get rewards for selected products\n",
    "            full_prices = np.zeros(env.num_products)\n",
    "            full_prices[product_subset] = prices\n",
    "            \n",
    "            full_rewards = env.get_rewards(t, full_prices)\n",
    "            \n",
    "            # Only consider rewards for selected products\n",
    "            rewards = full_rewards[product_subset]\n",
    "            products_purchased = [i for i, product_idx in enumerate(product_subset) if full_rewards[product_idx] > 0]\n",
    "            \n",
    "            # Convert to product indices that were actually purchased\n",
    "            actual_purchased_products = [product_subset[i] for i in products_purchased]\n",
    "            \n",
    "            inventory_consumed += len(actual_purchased_products)\n",
    "            reward_history[t] = full_rewards  # Store full reward vector\n",
    "            \n",
    "            agent.update(actual_purchased_products, np.sum(rewards))\n",
    "    \n",
    "    print(f\"Simulation completed. Total inventory consumed: {inventory_consumed}\")\n",
    "    \n",
    "    return {\n",
    "        'reward_history': reward_history,\n",
    "        'cumulative_reward': reward_history.cumsum(axis=0),\n",
    "        'inventory_consumed': inventory_consumed,\n",
    "        'agent': agent,\n",
    "        'description': description\n",
    "    }\n",
    "\n",
    "# Simulation parameters\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Create fresh environment for comparison\n",
    "rng = np.random.default_rng(12345)\n",
    "env_comparison = NonStationaryEnvironment(prices, n_products, change_probability=0.001, rng=rng)\n",
    "env_comparison.sample_parameters(T)\n",
    "\n",
    "# Parameters for UCB-SW\n",
    "ucb_sw_params = {\n",
    "    'n_products': n_products,\n",
    "    'price_options': prices,\n",
    "    'inventory': B,\n",
    "    'T': T,\n",
    "    'window_size': W\n",
    "}\n",
    "\n",
    "# Parameters for UCB_multi_constr\n",
    "ucb_multi_params = {\n",
    "    'n_products': n_products,\n",
    "    'price_options': prices,\n",
    "    'inventory': B,\n",
    "    'T': T,\n",
    "    'selection_method': 'lsa'  # Use Linear Sum Assignment\n",
    "}\n",
    "\n",
    "# Run simulations\n",
    "results_ucb_sw = run_simulation(UCBSWAgent, ucb_sw_params, env_comparison, T, B, \"UCB-SW\")\n",
    "results_ucb_multi = run_simulation(UCBMatchingAgent, ucb_multi_params, env_comparison, T, B, \"UCB Multi-Constraint\")\n",
    "\n",
    "# Get baseline results (reuse from previous simulation)\n",
    "baseline_reward_history_comp = baseline_revenue(env_comparison, B, T)\n",
    "baseline_cumulative_reward_comp = baseline_reward_history_comp.cumsum(axis=0)\n",
    "\n",
    "print(f\"\\n--- Performance Comparison ---\")\n",
    "print(f\"UCB-SW total reward: {results_ucb_sw['cumulative_reward'][-1].sum():.2f}\")\n",
    "print(f\"UCB Multi-Constraint total reward: {results_ucb_multi['cumulative_reward'][-1].sum():.2f}\")\n",
    "print(f\"Baseline total reward: {baseline_cumulative_reward_comp[-1].sum():.2f}\")\n",
    "print(f\"UCB-SW performance ratio: {results_ucb_sw['cumulative_reward'][-1].sum() / baseline_cumulative_reward_comp[-1].sum():.3f}\")\n",
    "print(f\"UCB Multi-Constraint performance ratio: {results_ucb_multi['cumulative_reward'][-1].sum() / baseline_cumulative_reward_comp[-1].sum():.3f}\")\n",
    "print(f\"UCB-SW inventory utilization: {results_ucb_sw['inventory_consumed'] / B:.3f}\")\n",
    "print(f\"UCB Multi-Constraint inventory utilization: {results_ucb_multi['inventory_consumed'] / B:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd778e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute regrets for both algorithms\n",
    "regret_ucb_sw = (baseline_cumulative_reward_comp - results_ucb_sw['cumulative_reward']).sum(axis=1)\n",
    "regret_ucb_multi = (baseline_cumulative_reward_comp - results_ucb_multi['cumulative_reward']).sum(axis=1)\n",
    "\n",
    "# Theoretical regret bounds\n",
    "theoretical_regret_ucb_sw = [(t + 1)**(2/3) * np.log(t + 1) for t in range(len(regret_ucb_sw))]\n",
    "theoretical_regret_ucb_multi = [np.sqrt((t + 1) * np.log(t + 1)) for t in range(len(regret_ucb_multi))]\n",
    "\n",
    "# Create comprehensive comparison plots\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Plot 1: Cumulative Rewards Comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(results_ucb_sw['cumulative_reward'].sum(axis=1), label='UCB-SW', linewidth=2, color='blue')\n",
    "plt.plot(results_ucb_multi['cumulative_reward'].sum(axis=1), label='UCB Multi-Constraint', linewidth=2, color='red')\n",
    "plt.plot(baseline_cumulative_reward_comp.sum(axis=1), label='Baseline (Optimal)', linestyle='--', linewidth=2, color='green')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Cumulative Reward Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Regret Comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(regret_ucb_sw, label='UCB-SW Regret', linewidth=2, color='blue')\n",
    "plt.plot(regret_ucb_multi, label='UCB Multi-Constraint Regret', linewidth=2, color='red')\n",
    "plt.plot(theoretical_regret_ucb_sw, label='UCB-SW Theoretical O(T^(2/3)·log T)', linestyle='--', alpha=0.7, color='lightblue')\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.7)\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Regret')\n",
    "plt.title('Regret Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Regret/T Comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "t_vals = np.arange(len(regret_ucb_sw)) + 1\n",
    "plt.plot(regret_ucb_sw/t_vals, label='UCB-SW Regret/T', linewidth=2, color='blue')\n",
    "plt.plot(regret_ucb_multi/t_vals, label='UCB Multi-Constraint Regret/T', linewidth=2, color='red')\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.7)\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Regret/T')\n",
    "plt.title('Average Regret Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Per-Product Cumulative Rewards (UCB-SW)\n",
    "plt.subplot(2, 3, 4)\n",
    "for i in range(n_products):\n",
    "    plt.plot(results_ucb_sw['cumulative_reward'][:, i], label=f'UCB-SW Product {i}', alpha=0.7)\n",
    "    plt.plot(baseline_cumulative_reward_comp[:, i], label=f'Baseline Product {i}', linestyle='--', alpha=0.7)\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Per-Product Rewards: UCB-SW vs Baseline')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Per-Product Cumulative Rewards (UCB Multi-Constraint)\n",
    "plt.subplot(2, 3, 5)\n",
    "for i in range(n_products):\n",
    "    plt.plot(results_ucb_multi['cumulative_reward'][:, i], label=f'UCB Multi Product {i}', alpha=0.7)\n",
    "    plt.plot(baseline_cumulative_reward_comp[:, i], label=f'Baseline Product {i}', linestyle='--', alpha=0.7)\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Per-Product Rewards: UCB Multi-Constraint vs Baseline')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Direct Algorithm Comparison\n",
    "plt.subplot(2, 3, 6)\n",
    "# Difference in cumulative rewards\n",
    "reward_difference = results_ucb_multi['cumulative_reward'].sum(axis=1) - results_ucb_sw['cumulative_reward'].sum(axis=1)\n",
    "plt.plot(reward_difference, linewidth=2, color='purple')\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.7)\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('UCB Multi-Constraint - UCB-SW Reward')\n",
    "plt.title('Direct Algorithm Comparison\\n(Positive = UCB Multi-Constraint Better)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison statistics\n",
    "print(f\"\\n--- Detailed Performance Analysis ---\")\n",
    "print(f\"Final Regret:\")\n",
    "print(f\"  UCB-SW: {regret_ucb_sw[-1]:.2f}\")\n",
    "print(f\"  UCB Multi-Constraint: {regret_ucb_multi[-1]:.2f}\")\n",
    "print(f\"  Difference: {regret_ucb_multi[-1] - regret_ucb_sw[-1]:.2f}\")\n",
    "\n",
    "print(f\"\\nFinal Regret/T:\")\n",
    "print(f\"  UCB-SW: {regret_ucb_sw[-1]/T:.4f}\")\n",
    "print(f\"  UCB Multi-Constraint: {regret_ucb_multi[-1]/T:.4f}\")\n",
    "\n",
    "print(f\"\\nTheoretical vs Actual Regret Ratios:\")\n",
    "print(f\"  UCB-SW: {regret_ucb_sw[-1]/theoretical_regret_ucb_sw[-1]:.3f}\")\n",
    "print(f\"  UCB Multi-Constraint: {regret_ucb_multi[-1]/theoretical_regret_ucb_multi[-1]:.3f}\")\n",
    "\n",
    "print(f\"\\nTotal Revenue Difference: {results_ucb_multi['cumulative_reward'][-1].sum() - results_ucb_sw['cumulative_reward'][-1].sum():.2f}\")\n",
    "print(f\"Percentage Improvement: {((results_ucb_multi['cumulative_reward'][-1].sum() - results_ucb_sw['cumulative_reward'][-1].sum()) / results_ucb_sw['cumulative_reward'][-1].sum() * 100):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
