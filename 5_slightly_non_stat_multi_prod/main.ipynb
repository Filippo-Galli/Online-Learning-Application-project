{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78563f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "from scipy.optimize import linear_sum_assignment, linprog\n",
    "from scipy import optimize\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "# Reload the module to get the latest changes\n",
    "import importlib\n",
    "if 'UCB_SW' in sys.modules:\n",
    "    importlib.reload(sys.modules['UCB_SW'])\n",
    "\n",
    "from UCB_SW import UCBSWAgent\n",
    "from Multi_Primal_Dual import MultiProductPrimalDualAgent\n",
    "from UCB1_multi_constr import UCBMatchingAgent\n",
    "from Multi_Thompson_constr_SW import MultiThompsonSamplingPricingAgentSW\n",
    "from Multi_Thompson_constr import MultiThompsonSamplingPricingAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b41171",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # For reproducibility\n",
    "T = 10000          # total time horizon\n",
    "Inventory_frac = 0.8 \n",
    "W = int(np.sqrt(T))   # sliding window size\n",
    "W = int(np.sqrt(T)*5) # adjust according to the plots\n",
    "print(f\"Sliding window size: {W}\")\n",
    "n_products = 3\n",
    "B = T * Inventory_frac * n_products  # total inventory across all products\n",
    "n_intervals = 10\n",
    "prices_per_product = 7\n",
    "prices = np.linspace(0.1, 0.9, prices_per_product)\n",
    "\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc1430",
   "metadata": {},
   "source": [
    "# Slightly Non Stationary Environment\n",
    "\n",
    "### We will simulate a slightly non-stationary environment by partitioning the rounds into intervals. The distribution in each intervall will be costant and at each intervall the mean and standard deviation will be incremented by a quantity sampled from a distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a79194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStationaryEnvironment:\n",
    "    def __init__(self, possible_prices, num_products, change_probability=0.01, mu_lim=(0.3,0.7), std_lim=(0.05,0.15), rng=None):\n",
    "        \"\"\"\n",
    "        Initialize a non-stationary environment for pricing optimization.\n",
    "        \n",
    "        Args:\n",
    "            possible_prices: Array of possible price points\n",
    "            num_products: Number of products in the environment\n",
    "            change_probability: Probability of parameter change at each timestep\n",
    "            mu_lim: Tuple defining limits for mean valuations (min, max)\n",
    "            std_lim: Tuple defining limits for standard deviations (min, max)\n",
    "            rng: Random number generator instance\n",
    "        \"\"\"\n",
    "        self.rng = rng if rng else np.random.default_rng()\n",
    "        self.possible_prices = possible_prices\n",
    "        self.num_products = num_products\n",
    "        self.change_probability = change_probability\n",
    "        self.mu_lim = mu_lim\n",
    "        self.std_lim = std_lim\n",
    "\n",
    "    def sample_parameters(self, T):\n",
    "        \"\"\"\n",
    "        Sample mean and standard deviation parameters for each timestep.\n",
    "        Creates piecewise constant distributions that change at random intervals.\n",
    "        \n",
    "        Args:\n",
    "            T: Total number of timesteps\n",
    "        \"\"\"\n",
    "        means_per_timestep = np.empty((T, self.num_products))\n",
    "        stds_per_timestep = np.empty((T, self.num_products))\n",
    "\n",
    "        # Determine when changes occur for each product at each timestep\n",
    "        change_triggers = np.random.random(size=(T, self.num_products))\n",
    "        change_mask = change_triggers < self.change_probability\n",
    "        n_changes = np.sum(change_mask)\n",
    "        \n",
    "        # Sample initial parameters for each product\n",
    "        starting_means = self.rng.uniform(low=self.mu_lim[0], high=self.mu_lim[1], size=self.num_products)\n",
    "        starting_stds = self.rng.uniform(low=self.std_lim[0], high=self.std_lim[1], size=self.num_products)\n",
    "\n",
    "        # Sample new parameters for all change events\n",
    "        means = self.rng.uniform(low=self.mu_lim[0], high=self.mu_lim[1], size=n_changes)\n",
    "        stds = self.rng.uniform(low=self.std_lim[0], high=self.std_lim[1], size=n_changes)\n",
    "\n",
    "        # Get indices where changes occur\n",
    "        change_timesteps, change_products = np.nonzero(change_mask)\n",
    "        \n",
    "        # Fill parameters for each product across all timesteps\n",
    "        for product in range(self.num_products):\n",
    "            product_mask = change_products == product\n",
    "            if np.sum(product_mask) == 0:\n",
    "                # No changes for this product - use starting values throughout\n",
    "                means_per_timestep[:, product] = starting_means[product]\n",
    "                stds_per_timestep[:, product] = starting_stds[product]\n",
    "                continue\n",
    "            \n",
    "            # Get change points and new values for this product\n",
    "            product_change_steps = change_timesteps[product_mask]\n",
    "            product_means = means[product_mask]\n",
    "            product_stds = stds[product_mask]\n",
    "            \n",
    "            # Fill initial period (before first change)\n",
    "            means_per_timestep[:product_change_steps[0], product] = starting_means[product]\n",
    "            stds_per_timestep[:product_change_steps[0], product] = starting_stds[product]\n",
    "            \n",
    "            # Fill periods between changes\n",
    "            for i in range(len(product_change_steps) - 1):\n",
    "                start_step = product_change_steps[i]\n",
    "                end_step = product_change_steps[i+1]\n",
    "                means_per_timestep[start_step:end_step, product] = product_means[i]\n",
    "                stds_per_timestep[start_step:end_step, product] = product_stds[i]\n",
    "\n",
    "            # Fill final period (after last change)\n",
    "            means_per_timestep[product_change_steps[-1]:, product] = product_means[-1]\n",
    "            stds_per_timestep[product_change_steps[-1]:, product] = product_stds[-1]\n",
    "            \n",
    "        # Store generated parameters\n",
    "        self.means = means_per_timestep\n",
    "        self.stds = stds_per_timestep\n",
    "\n",
    "    def sample_valuation(self, t):\n",
    "        \"\"\"\n",
    "        Sample customer valuations from normal distributions for timestep t.\n",
    "        \n",
    "        Args:\n",
    "            t: Current timestep\n",
    "            \n",
    "        Returns:\n",
    "            Clipped valuations in [0,1] range for current timestep only\n",
    "        \"\"\"\n",
    "        # Sample valuations for current timestep only\n",
    "        current_means = self.means[t]  # Shape: (num_products,)\n",
    "        current_stds = self.stds[t]    # Shape: (num_products,)\n",
    "        valuations = self.rng.normal(loc=current_means, scale=current_stds)\n",
    "        return np.clip(valuations, 0, 1)\n",
    "\n",
    "    def buyer_response(self, prices, valuations):\n",
    "        \"\"\"\n",
    "        Determine buyer purchase decisions based on prices vs valuations.\n",
    "        \n",
    "        Args:\n",
    "            prices: Array of prices for each product\n",
    "            valuations: Array of customer valuations for each product\n",
    "            \n",
    "        Returns:\n",
    "            Boolean array indicating purchases (True if valuation > price)\n",
    "        \"\"\"\n",
    "        return (prices < valuations).astype(bool)\n",
    "\n",
    "    def get_rewards(self, t, prices):\n",
    "        \"\"\"\n",
    "        Get rewards for given prices at timestep t.\n",
    "        \n",
    "        Args:\n",
    "            t: Current timestep\n",
    "            prices: Array of prices for each product\n",
    "            \n",
    "        Returns:\n",
    "            Array of rewards (price if purchased, 0 otherwise)\n",
    "        \"\"\"\n",
    "        # Sample customer valuations for this timestep only\n",
    "        valuations = self.sample_valuation(t)\n",
    "        \n",
    "        # Determine purchases and calculate rewards\n",
    "        purchases = self.buyer_response(prices, valuations)\n",
    "        rewards = prices * purchases\n",
    "        \n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4793c5",
   "metadata": {},
   "source": [
    "### Evolution of the mean and standard deviation during time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d43bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12345)\n",
    "env = NonStationaryEnvironment(prices, n_products, change_probability=0.0003, rng=rng)\n",
    "env.sample_parameters(T)\n",
    "\n",
    "# Plot delle medie\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "for p in range(n_products):\n",
    "    plt.plot(env.means[:, p], label=f\"Prodotto {p} (mean)\")\n",
    "\n",
    "plt.title(\"Evoluzione delle medie nel tempo\")\n",
    "plt.xlabel(\"Round t\")\n",
    "plt.ylabel(\"Valore medio (Î¼)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e566b3",
   "metadata": {},
   "source": [
    "Now we compare the performances of our learner with the baseline. The baseline knows the true expected reward of each arm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725e1fd",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53aae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the revenue of the baseline strategy that knows the distribution of valuations\n",
    "def baseline_revenue(env, B, T, precomputed_valuations=None):\n",
    "     \n",
    "    K = len(env.possible_prices)\n",
    "\n",
    "    #Reward for every product at each timestep\n",
    "    reward_history = np.full((T, env.num_products), 0.0)\n",
    "\n",
    "    optimal_gamma = optimal_policy(env, B, T)\n",
    "    inventory = B\n",
    "\n",
    "    for t in range(T):\n",
    "        #print(\"Step \",t , \" in the optimal policy\")\n",
    "        if inventory < env.num_products:\n",
    "            print(f\"Inventory deployed at step {t}\")\n",
    "            break\n",
    "        arms = np.empty(env.num_products, dtype=int)\n",
    "        for product in range(env.num_products):\n",
    "            #For every product sample a price (the index)\n",
    "            arms[product] = np.random.choice(K, p = optimal_gamma[product])\n",
    "       \n",
    "        #Price selected for each product\n",
    "        prices = env.possible_prices[arms]\n",
    "\n",
    "        #Reward for each product at time t: if valuation > price -> reward = price, else 0\n",
    "        if precomputed_valuations is not None:\n",
    "            valuations = precomputed_valuations[t]\n",
    "            purchases = env.buyer_response(prices, valuations)\n",
    "            rewards = prices * purchases\n",
    "        else:\n",
    "            rewards = env.get_rewards(t, prices)\n",
    "\n",
    "        #Inventory consumed\n",
    "        inventory -= np.count_nonzero(rewards)\n",
    "\n",
    "        reward_history[t] = rewards\n",
    "    #Return the cumulative reward of each product overtime    \n",
    "    return reward_history\n",
    "\n",
    "\n",
    "#Compute the optimal policy knowing the true expected rewards and purchase probabilities\n",
    "def optimal_policy(env, B, T):\n",
    "    \n",
    "    n = env.num_products\n",
    "    K = len(env.possible_prices)\n",
    "\n",
    "    #Compute true expected reward and demand probabilities for each product and price\n",
    "    purchase_prob = np.zeros((T, n, K))\n",
    "\n",
    "    for t in range(T):\n",
    "         for i in range(n):\n",
    "            for j in range(K):\n",
    "                 #P(valuation > price) = 1 - CDF(price) for each product and for each timestep\n",
    "                 purchase_prob[t,i,j] = 1 - norm.cdf(env.possible_prices[j], loc=env.means[t,i], scale=env.stds[t,i])\n",
    "\n",
    "    #Compute the average purchase probability over time for each product and price\n",
    "    avg_purchase_prob = purchase_prob.mean(axis=0)\n",
    "\n",
    "    #Compute the expected reward for each product and price\n",
    "    expected_rewards = np.zeros((n, K))\n",
    "    for i in range(n):\n",
    "         expected_rewards[i,:] = env.possible_prices * avg_purchase_prob[i,:]\n",
    "\n",
    "    #Initialize the coefficient matrix for equality constraints\n",
    "    A_eq = np.zeros((n, n*K))\n",
    "    b_eq = np.ones(n)\n",
    "\n",
    "    for dim in range(n):\n",
    "        A_eq[dim,dim*K:(dim+1)*K] = 1\n",
    "\n",
    "    c = -expected_rewards.flatten()\n",
    "    A_ub = [avg_purchase_prob.flatten()]\n",
    "    b_ub = [B/T]\n",
    "    \n",
    "\n",
    "    # Solve linear program\n",
    "    try:\n",
    "        res = optimize.linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq = b_eq,\n",
    "                                bounds=(0,1))\n",
    "        if res.success:\n",
    "            gamma_flat = res.x\n",
    "            # Reshape gamma back to (n_products, n_price_options)\n",
    "            gamma = gamma_flat.reshape((n, K))\n",
    "\n",
    "            # Ensure valid probability distribution for each product\n",
    "            for product_idx in range(n):\n",
    "                gamma[product_idx, :] = np.maximum(gamma[product_idx, :], 0)\n",
    "                sum_gamma = np.sum(gamma[product_idx, :])\n",
    "                if sum_gamma > 0:\n",
    "                        gamma[product_idx, :] /= sum_gamma\n",
    "                else:\n",
    "                        # Fallback: uniform distribution for this product\n",
    "                        gamma[product_idx, :] = np.ones(K) / K\n",
    "\n",
    "            return gamma\n",
    "        else:\n",
    "            # Fallback: uniform distribution for all products\n",
    "            return np.ones((n, K)) / K\n",
    "    except Exception as e:\n",
    "        print(f\"LP Error: {e}\")\n",
    "        # Fallback: uniform distribution for all products\n",
    "        return np.ones((n, K)) / K\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4285026",
   "metadata": {},
   "source": [
    "## Initialize the enviroment and Run a simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892293b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "rng = np.random.default_rng(42)\n",
    "env = NonStationaryEnvironment(prices, n_products, change_probability=0.001, rng=rng)\n",
    "env.sample_parameters(T)\n",
    "\n",
    "# Pre-generate valuations for consistent comparison later\n",
    "np.random.seed(42)  # Reset seed for consistent valuation generation\n",
    "precomputed_valuations_individual = np.zeros((T, env.num_products))\n",
    "for t in range(T):\n",
    "    precomputed_valuations_individual[t] = env.sample_valuation(t)\n",
    "\n",
    "# Initialize the agent with improved inventory management\n",
    "ag_UCB = UCBSWAgent(\n",
    "    n_products=n_products, \n",
    "    price_options=prices, \n",
    "    inventory=B, \n",
    "    T=T, \n",
    "    window_size=W,\n",
    ")\n",
    "\n",
    "# Initialize the agent with improved inventory management\n",
    "ag_SW = MultiThompsonSamplingPricingAgentSW(\n",
    "    price_options=prices,\n",
    "    window_size=W,\n",
    "    alpha_prior=1.0,\n",
    "    beta_prior=1.0,\n",
    "    n_products=n_products,\n",
    "    T=T,\n",
    "    inventory=B\n",
    ")\n",
    "\n",
    "agents = {\n",
    "    \"UCB\": ag_UCB,\n",
    "    \"Thompson\": ag_SW\n",
    "}\n",
    "\n",
    "# total_reward = 0\n",
    "reward_history = {}\n",
    "inventory_consumed = {}\n",
    "for agent_name, agent in agents.items():\n",
    "    reward_history[agent_name] = np.zeros((T, env.num_products))\n",
    "    inventory_consumed[agent_name] = 0\n",
    "\n",
    "for t in range(T):\n",
    "    if t % (T//10) == 0:\n",
    "        for agent_name, agent in agents.items():\n",
    "            print(f\"Time step: {t}, Remaining inventory for {agent_name}: {agent.remaining_inventory}\")\n",
    "\n",
    "    # Agent selects a price for each product - returns (product_subset, prices)\n",
    "    product_subset={}\n",
    "    selected_prices={}\n",
    "\n",
    "    for agent_name, agent in agents.items():\n",
    "        subset, prices = agent.select_action()\n",
    "        product_subset[agent_name] = subset\n",
    "        selected_prices[agent_name] = prices\n",
    "        if len(product_subset[agent_name]) == 0:\n",
    "            print(f\"Not enough inventory left for {agent_name} at time step {t}\")\n",
    "            print(f\"Total inventory consumed: {inventory_consumed}\")\n",
    "            # Don't break here, continue with other agents\n",
    "\n",
    "    # Create full price vector for all products\n",
    "    price={\n",
    "        \"UCB\": np.zeros(env.num_products),\n",
    "        \"Thompson\": np.zeros(env.num_products)\n",
    "    }\n",
    "    for agent_name, agent in agents.items():\n",
    "        if len(product_subset[agent_name]) > 0:  # Only if agent has inventory\n",
    "            price[agent_name][product_subset[agent_name]] = selected_prices[agent_name]\n",
    "\n",
    "    # Use precomputed valuations for consistency\n",
    "    valuations = precomputed_valuations_individual[t]\n",
    "    \n",
    "    purchases={}\n",
    "    f_t={}\n",
    "    \n",
    "    # Calculate purchases and rewards for each agent\n",
    "    for agent_name, agent in agents.items():\n",
    "        if len(product_subset[agent_name]) > 0:  # Only if agent has inventory\n",
    "            purchases[agent_name] = env.buyer_response(price[agent_name], valuations)\n",
    "            f_t[agent_name] = price[agent_name] * purchases[agent_name]\n",
    "        else:\n",
    "            purchases[agent_name] = np.zeros(env.num_products, dtype=bool)\n",
    "            f_t[agent_name] = np.zeros(env.num_products)\n",
    "\n",
    "    # Vector of the cost of the arm (which products were purchased)\n",
    "    c_t={}\n",
    "    for agent_name in agents.keys():\n",
    "        if len(product_subset[agent_name]) > 0:  # Only if agent has inventory\n",
    "            c_t[agent_name] = purchases[agent_name].astype(int)  # Convert boolean to int\n",
    "            agent = agents[agent_name]\n",
    "            \n",
    "            # Count units sold\n",
    "            units_sold = np.sum(c_t[agent_name])\n",
    "            \n",
    "            # Update inventory tracking\n",
    "            inventory_consumed[agent_name] += units_sold\n",
    "            \n",
    "            # Store reward history\n",
    "            reward_history[agent_name][t] = f_t[agent_name]\n",
    "            \n",
    "            # Update agent based on its interface\n",
    "            if hasattr(agent, 'update') and 'Thompson' in agent_name:\n",
    "                # For Thompson Sampling: update(products_purchased, total_revenue)\n",
    "                products_purchased = [i for i, purchased in enumerate(c_t[agent_name]) if purchased]\n",
    "                total_revenue = np.sum(f_t[agent_name])\n",
    "                agent.update(products_purchased, total_revenue)\n",
    "            else:\n",
    "                # For UCB: update(rewards, costs)\n",
    "                agent.update(f_t[agent_name], c_t[agent_name])\n",
    "        else:\n",
    "            # Agent has no inventory left\n",
    "            c_t[agent_name] = np.zeros(env.num_products, dtype=int)\n",
    "            reward_history[agent_name][t] = np.zeros(env.num_products)\n",
    "\n",
    "print(f\"Simulation completed. Total inventory consumed: {inventory_consumed}\")\n",
    "\n",
    "# Cumulative reward of each product over time and each agent\n",
    "agent_cumulative_reward = {agent_name: history.cumsum(axis=0) for agent_name, history in reward_history.items()}\n",
    "\n",
    "# Reward history of the baseline using the same precomputed valuations\n",
    "baseline_reward_history = baseline_revenue(env, B, T, precomputed_valuations_individual)\n",
    "baseline_cumulative_reward = baseline_reward_history.cumsum(axis=0)\n",
    "\n",
    "for agent_name in agents.keys():\n",
    "    print(f\"Results for agent: {agent_name}\")\n",
    "    print(f\"Agent total reward: {agent_cumulative_reward[agent_name][-1].sum():.2f}\")\n",
    "    print(f\"Baseline total reward: {baseline_cumulative_reward[-1].sum():.2f}\")\n",
    "    print(f\"Performance ratio: {agent_cumulative_reward[agent_name][-1].sum() / baseline_cumulative_reward[-1].sum():.3f}\")\n",
    "    print(f\"Inventory utilization: {inventory_consumed[agent_name] / B:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d044e4f",
   "metadata": {},
   "source": [
    "# Plot of the cumulative rewards and regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637c3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for both agents\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Plot for UCB agent\n",
    "ucb_cumulative = agent_cumulative_reward[\"UCB\"]\n",
    "ucb_regret = (baseline_cumulative_reward - ucb_cumulative).sum(axis=1)\n",
    "theoretical_regret_ucb = [(t + 1)**(2/3) * np.log(t + 1) for t in range(len(ucb_regret))]\n",
    "\n",
    "# Plot for Thompson agent  \n",
    "thompson_cumulative = agent_cumulative_reward[\"Thompson\"]\n",
    "thompson_regret = (baseline_cumulative_reward - thompson_cumulative).sum(axis=1)\n",
    "theoretical_regret_thompson = [(t + 1)**(2/3) * np.log(t + 1) for t in range(len(thompson_regret))]\n",
    "\n",
    "# Subplot 1: Cumulative Rewards Comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(ucb_cumulative.sum(axis=1), label='UCB-SW Total', linewidth=2, color='blue')\n",
    "plt.plot(thompson_cumulative.sum(axis=1), label='Thompson-SW Total', linewidth=2, color='red')\n",
    "plt.plot(baseline_cumulative_reward.sum(axis=1), label='Baseline Total', linestyle='--', linewidth=2, color='green')\n",
    "for i in range(n_products):\n",
    "    plt.plot(ucb_cumulative[:, i], alpha=0.5, color='blue', linestyle=':')\n",
    "    plt.plot(thompson_cumulative[:, i], alpha=0.5, color='red', linestyle=':')\n",
    "    plt.plot(baseline_cumulative_reward[:, i], alpha=0.5, color='green', linestyle='--')\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Reward Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Regret Comparison  \n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(ucb_regret, label=\"UCB-SW Regret\", color='blue', linewidth=2)\n",
    "plt.plot(thompson_regret, label=\"Thompson-SW Regret\", color='red', linewidth=2)\n",
    "plt.plot(theoretical_regret_ucb, label=\"Theoretical Bound O(T^(2/3)Â·log T)\", linestyle=\"--\", alpha=0.7)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.7)\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.title(\"Regret Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Regret/T Comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "t_vals = np.arange(len(ucb_regret)) + 1\n",
    "plt.plot(ucb_regret/t_vals, label=\"UCB-SW Regret/T\", color='blue', linewidth=2)\n",
    "plt.plot(thompson_regret/t_vals, label=\"Thompson-SW Regret/T\", color='red', linewidth=2)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.7)\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Regret/T\")\n",
    "plt.title(\"Average Regret Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Per-Product UCB-SW\n",
    "plt.subplot(2, 3, 4)\n",
    "for i in range(n_products):\n",
    "    plt.plot(ucb_cumulative[:, i], label=f\"UCB-SW Product {i}\", alpha=0.8)\n",
    "    plt.plot(baseline_cumulative_reward[:, i], label=f\"Baseline Product {i}\", linestyle=\"--\", alpha=0.8)\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Per-Product Rewards: UCB-SW\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 5: Per-Product Thompson-SW\n",
    "plt.subplot(2, 3, 5)\n",
    "for i in range(n_products):\n",
    "    plt.plot(thompson_cumulative[:, i], label=f\"Thompson-SW Product {i}\", alpha=0.8)\n",
    "    plt.plot(baseline_cumulative_reward[:, i], label=f\"Baseline Product {i}\", linestyle=\"--\", alpha=0.8)\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Per-Product Rewards: Thompson-SW\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 6: Direct Performance Comparison\n",
    "plt.subplot(2, 3, 6)\n",
    "performance_data = {\n",
    "    'UCB-SW': ucb_cumulative[-1].sum(),\n",
    "    'Thompson-SW': thompson_cumulative[-1].sum(),\n",
    "    'Baseline': baseline_cumulative_reward[-1].sum()\n",
    "}\n",
    "bars = plt.bar(performance_data.keys(), performance_data.values(), \n",
    "               color=['blue', 'red', 'green'], alpha=0.7)\n",
    "plt.ylabel(\"Total Cumulative Reward\")\n",
    "plt.title(\"Final Performance Comparison\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison\n",
    "print(f\"\\n--- Performance Analysis ---\")\n",
    "print(f\"UCB-SW:\")\n",
    "print(f\"  Final regret: {ucb_regret[-1]:.2f}\")\n",
    "print(f\"  Final regret/T: {ucb_regret[-1]/T:.4f}\")\n",
    "print(f\"  Theoretical bound ratio: {ucb_regret[-1]/theoretical_regret_ucb[-1]:.3f}\")\n",
    "\n",
    "print(f\"\\nThompson-SW:\")\n",
    "print(f\"  Final regret: {thompson_regret[-1]:.2f}\")\n",
    "print(f\"  Final regret/T: {thompson_regret[-1]/T:.4f}\")\n",
    "print(f\"  Theoretical bound ratio: {thompson_regret[-1]/theoretical_regret_thompson[-1]:.3f}\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "regret_diff = thompson_regret[-1] - ucb_regret[-1]\n",
    "print(f\"  Thompson vs UCB regret difference: {regret_diff:.2f}\")\n",
    "print(f\"  Thompson better by: {-regret_diff:.2f}\" if regret_diff < 0 else f\"  UCB better by: {regret_diff:.2f}\")\n",
    "\n",
    "reward_diff = thompson_cumulative[-1].sum() - ucb_cumulative[-1].sum()\n",
    "print(f\"  Thompson vs UCB reward difference: {reward_diff:.2f}\")\n",
    "print(f\"  Percentage difference: {(reward_diff/ucb_cumulative[-1].sum())*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a1d464",
   "metadata": {},
   "source": [
    "# Comprehensive Comparison: UCB-SW vs UCB Multi-Constraint vs Multi-Product Primal-Dual\n",
    "\n",
    "This section compares three different approaches for the multi-product pricing problem with inventory constraints in a slightly non-stationary environment:\n",
    "\n",
    "1. **UCB-SW (UCB with Sliding Window)**: Handles non-stationarity by maintaining a sliding window of recent observations. Uses UCB confidence bounds within the window.\n",
    "\n",
    "2. **UCB Multi-Constraint**: Uses combinatorial optimization (Linear Sum Assignment) to select the best combination of products and prices while respecting inventory constraints.\n",
    "\n",
    "3. **Multi-Product Primal-Dual**: Uses a primal-dual approach with a dual variable Î» that controls inventory consumption. Each product has its own learning algorithm (EXP3) and the rewards are adjusted by the Lagrangian to incorporate inventory costs.\n",
    "\n",
    "Each algorithm has different theoretical properties and practical advantages for handling the trade-off between exploration, exploitation, and inventory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50603193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(agent_class, agent_params, env, T, B, precomputed_valuations=None, description=\"Agent\"):\n",
    "    \"\"\"\n",
    "    Run a simulation with a specific agent and return the results.\n",
    "    \n",
    "    Args:\n",
    "        agent_class: The agent class to instantiate\n",
    "        agent_params: Dictionary of parameters for the agent\n",
    "        env: The environment instance\n",
    "        T: Time horizon\n",
    "        B: Total inventory\n",
    "        precomputed_valuations: Optional pre-computed valuations for consistent comparison\n",
    "        description: Description for logging\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with simulation results\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running simulation with {description} ---\")\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent = agent_class(**agent_params)\n",
    "    \n",
    "    # Track results\n",
    "    reward_history = np.zeros((T, env.num_products))\n",
    "    inventory_consumed = 0\n",
    "    \n",
    "    for t in range(T):\n",
    "        if t % (T//10) == 0:\n",
    "            print(f\"Time step: {t}, Remaining inventory: {agent.remaining_inventory}\")\n",
    "        \n",
    "        # Agent selects action based on its interface\n",
    "        if hasattr(agent, 'select_action') and 'UCBSWAgent' in str(type(agent)):  # UCB_SW interface\n",
    "            product_subset, selected_prices = agent.select_action()\n",
    "            if len(product_subset) == 0:\n",
    "                print(f\"Not enough inventory left at time step {t}\")\n",
    "                break\n",
    "            \n",
    "            # Create full price vector for all products\n",
    "            prices = np.zeros(env.num_products)\n",
    "            prices[product_subset] = selected_prices\n",
    "            \n",
    "            # Use precomputed valuations if available, otherwise sample from environment\n",
    "            if precomputed_valuations is not None:\n",
    "                valuations = precomputed_valuations[t]\n",
    "                purchases = env.buyer_response(prices, valuations)\n",
    "                rewards = prices * purchases\n",
    "            else:\n",
    "                rewards = env.get_rewards(t, prices)\n",
    "            \n",
    "            costs = (rewards != 0).astype(int)\n",
    "            \n",
    "            inventory_consumed += np.sum(costs)\n",
    "            reward_history[t] = rewards\n",
    "            agent.update(rewards, costs)\n",
    "            \n",
    "        elif hasattr(agent, 'select_action'):  # UCB_multi_constr interface\n",
    "            product_subset, prices = agent.select_action()\n",
    "            if len(product_subset) == 0:\n",
    "                print(f\"Not enough inventory left at time step {t}\")\n",
    "                break\n",
    "            \n",
    "            # Get rewards for selected products\n",
    "            full_prices = np.zeros(env.num_products)\n",
    "            full_prices[product_subset] = prices\n",
    "            \n",
    "            # Use precomputed valuations if available, otherwise sample from environment\n",
    "            if precomputed_valuations is not None:\n",
    "                valuations = precomputed_valuations[t]\n",
    "                purchases = env.buyer_response(full_prices, valuations)\n",
    "                full_rewards = full_prices * purchases\n",
    "            else:\n",
    "                full_rewards = env.get_rewards(t, full_prices)\n",
    "            \n",
    "            # Only consider rewards for selected products\n",
    "            rewards = full_rewards[product_subset]\n",
    "            products_purchased = [i for i, product_idx in enumerate(product_subset) if full_rewards[product_idx] > 0]\n",
    "            \n",
    "            # Convert to product indices that were actually purchased\n",
    "            actual_purchased_products = [product_subset[i] for i in products_purchased]\n",
    "            \n",
    "            inventory_consumed += len(actual_purchased_products)\n",
    "            reward_history[t] = full_rewards  # Store full reward vector\n",
    "            \n",
    "            agent.update(actual_purchased_products, np.sum(rewards))\n",
    "            \n",
    "        elif hasattr(agent, 'bid'):  # Primal-Dual interface\n",
    "            prices = agent.bid()\n",
    "            \n",
    "            # Use precomputed valuations if available, otherwise sample from environment\n",
    "            if precomputed_valuations is not None:\n",
    "                valuations = precomputed_valuations[t]\n",
    "                purchases = env.buyer_response(prices, valuations)\n",
    "                rewards = prices * purchases\n",
    "            else:\n",
    "                rewards = env.get_rewards(t, prices)\n",
    "                \n",
    "            costs = (rewards != 0).astype(int)\n",
    "            \n",
    "            inventory_consumed += np.sum(costs)\n",
    "            reward_history[t] = rewards\n",
    "            \n",
    "            # For primal-dual, we need to pass per-product revenue and purchases\n",
    "            agent.update(costs, rewards)\n",
    "    \n",
    "    print(f\"Simulation completed. Total inventory consumed: {inventory_consumed}\")\n",
    "    \n",
    "    return {\n",
    "        'reward_history': reward_history,\n",
    "        'cumulative_reward': reward_history.cumsum(axis=0),\n",
    "        'inventory_consumed': inventory_consumed,\n",
    "        'agent': agent,\n",
    "        'description': description\n",
    "    }\n",
    "\n",
    "# Simulation parameters\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Use the same environment as the individual UCB-SW simulation for fair comparison\n",
    "env_comparison = env\n",
    "\n",
    "# Use the same precomputed valuations as the individual simulation for consistency\n",
    "precomputed_valuations = precomputed_valuations_individual\n",
    "\n",
    "# Create price grid for primal-dual (it expects a 2D array)\n",
    "prices_grid = np.tile(prices, (n_products, 1))\n",
    "\n",
    "# Parameters for UCB-SW\n",
    "ucb_sw_params = {\n",
    "    'n_products': n_products,\n",
    "    'price_options': prices,\n",
    "    'inventory': B,\n",
    "    'T': T,\n",
    "    'window_size': W\n",
    "}\n",
    "\n",
    "# Parameters for UCB_multi_constr\n",
    "ucb_multi_params = {\n",
    "    'n_products': n_products,\n",
    "    'price_options': prices,\n",
    "    'inventory': B,\n",
    "    'T': T,\n",
    "    'selection_method': 'lsa'  # Use Linear Sum Assignment\n",
    "}\n",
    "\n",
    "# Parameters for Primal-Dual\n",
    "eta = np.sqrt(np.log(prices_per_product) / T)  # Learning rate for primal-dual\n",
    "primal_dual_params = {\n",
    "    'prices_grid': prices_grid,\n",
    "    'T': T,\n",
    "    'P': int(B),\n",
    "    'eta': eta,\n",
    "    'lambda_init': 1.0,\n",
    "    'algorithm': 'Exp3'  # Use EXP3 for bandit feedback\n",
    "}\n",
    "\n",
    "# Parameters for Thompson-SW\n",
    "thompson_sw_params = {\n",
    "    'price_options': prices,\n",
    "    'window_size': W,\n",
    "    'alpha_prior': 1.0,\n",
    "    'beta_prior': 1.0,\n",
    "    'n_products': n_products,\n",
    "    'T': T,\n",
    "    'inventory': B\n",
    "}\n",
    "\n",
    "thompson_multi_params = {\n",
    "    'price_options': prices,\n",
    "    'alpha_prior': 1.0,\n",
    "    'beta_prior': 1.0,\n",
    "    'n_products': n_products,\n",
    "    'T': T,\n",
    "    'inventory': B\n",
    "}\n",
    "\n",
    "\n",
    "# Run simulations (using the same environment and precomputed valuations for fair comparison)\n",
    "results_ucb_sw = run_simulation(UCBSWAgent, ucb_sw_params, env_comparison, T, B, precomputed_valuations, \"UCB-SW\")\n",
    "results_ucb_multi = run_simulation(UCBMatchingAgent, ucb_multi_params, env_comparison, T, B, precomputed_valuations, \"UCB Multi-Constraint\")\n",
    "results_primal_dual = run_simulation(MultiProductPrimalDualAgent, primal_dual_params, env_comparison, T, B, precomputed_valuations, \"Multi-Product Primal-Dual\")\n",
    "results_thompson_sw = run_simulation(MultiThompsonSamplingPricingAgentSW, thompson_sw_params, env_comparison, T, B, precomputed_valuations, \"Thompson-SW\")\n",
    "results_thompson_multi = run_simulation(MultiThompsonSamplingPricingAgent, thompson_multi_params, env_comparison, T, B, precomputed_valuations, \"Thompson Multi-Constraint\")\n",
    "\n",
    "# Get baseline results using the same precomputed valuations for consistency\n",
    "baseline_reward_history_comp = baseline_revenue(env_comparison, B, T, precomputed_valuations)\n",
    "baseline_cumulative_reward_comp = baseline_reward_history_comp.cumsum(axis=0)\n",
    "\n",
    "print(f\"\\n--- Performance Comparison ---\")\n",
    "print(f\"UCB-SW total reward: {results_ucb_sw['cumulative_reward'][-1].sum():.2f}\")\n",
    "print(f\"UCB Multi-Constraint total reward: {results_ucb_multi['cumulative_reward'][-1].sum():.2f}\")\n",
    "print(f\"Primal-Dual total reward: {results_primal_dual['cumulative_reward'][-1].sum():.2f}\")\n",
    "print(f\"Thompson-SW total reward: {results_thompson_sw['cumulative_reward'][-1].sum():.2f}\")\n",
    "print(f\"Thompson Multi-Constraint total reward: {results_thompson_multi['cumulative_reward'][-1].sum():.2f}\")\n",
    "print(f\"Baseline total reward: {baseline_cumulative_reward_comp[-1].sum():.2f}\")\n",
    "\n",
    "print(f\"UCB-SW performance ratio: {results_ucb_sw['cumulative_reward'][-1].sum() / baseline_cumulative_reward_comp[-1].sum():.3f}\")\n",
    "print(f\"UCB Multi-Constraint performance ratio: {results_ucb_multi['cumulative_reward'][-1].sum() / baseline_cumulative_reward_comp[-1].sum():.3f}\")\n",
    "print(f\"Thompson-SW performance ratio: {results_thompson_sw['cumulative_reward'][-1].sum() / baseline_cumulative_reward_comp[-1].sum():.3f}\")\n",
    "print(f\"Thompson Multi-Constraint performance ratio: {results_thompson_multi['cumulative_reward'][-1].sum() / baseline_cumulative_reward_comp[-1].sum():.3f}\")  \n",
    "print(f\"Primal-Dual performance ratio: {results_primal_dual['cumulative_reward'][-1].sum() / baseline_cumulative_reward_comp[-1].sum():.3f}\")\n",
    "\n",
    "print(f\"UCB-SW inventory utilization: {results_ucb_sw['inventory_consumed'] / B:.3f}\")\n",
    "print(f\"UCB Multi-Constraint inventory utilization: {results_ucb_multi['inventory_consumed'] / B:.3f}\")\n",
    "print(f\"Thompson-SW inventory utilization: {results_thompson_sw['inventory_consumed'] / B:.3f}\")\n",
    "print(f\"Thompson Multi-Constraint inventory utilization: {results_thompson_multi['inventory_consumed'] / B:.3f}\")\n",
    "print(f\"Primal-Dual inventory utilization: {results_primal_dual['inventory_consumed'] / B:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd778e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute regrets for all algorithms\n",
    "regret_ucb_sw = (baseline_cumulative_reward_comp - results_ucb_sw['cumulative_reward']).sum(axis=1)\n",
    "regret_ucb_multi = (baseline_cumulative_reward_comp - results_ucb_multi['cumulative_reward']).sum(axis=1)\n",
    "regret_primal_dual = (baseline_cumulative_reward_comp - results_primal_dual['cumulative_reward']).sum(axis=1)\n",
    "regret_thompson_sw = (baseline_cumulative_reward_comp - results_thompson_sw['cumulative_reward']).sum(axis=1)\n",
    "regret_thompson_multi = (baseline_cumulative_reward_comp - results_thompson_multi['cumulative_reward']).sum(axis=1)\n",
    "\n",
    "# Theoretical regret bounds\n",
    "theoretical_regret_ucb_sw = [(t + 1)**(2/3) * np.log(t + 1) for t in range(len(regret_ucb_sw))]\n",
    "theoretical_regret_ucb_multi = [np.sqrt((t + 1) * np.log(t + 1)) for t in range(len(regret_ucb_multi))]\n",
    "theoretical_regret_primal_dual = [np.sqrt((t + 1) * np.log(t + 1)) for t in range(len(regret_primal_dual))]\n",
    "theoretical_regret_thompson_sw = [(t + 1)**(2/3) * np.log(t + 1) for t in range(len(regret_thompson_sw))]\n",
    "theoretical_regret_thompson_multi = [np.sqrt((t + 1) * np.log(t + 1)) for t in range(len(regret_thompson_multi))]\n",
    "\n",
    "# Create comprehensive comparison plots\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Cumulative Rewards Comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(results_ucb_sw['cumulative_reward'].sum(axis=1), label='UCB-SW', linewidth=2, color='blue')\n",
    "plt.plot(results_ucb_multi['cumulative_reward'].sum(axis=1), label='UCB Multi-Constraint', linewidth=2, color='red')\n",
    "plt.plot(results_primal_dual['cumulative_reward'].sum(axis=1), label='Primal-Dual', linewidth=2, color='orange')\n",
    "plt.plot(results_thompson_sw['cumulative_reward'].sum(axis=1), label='Thompson-SW', linewidth=2, color='purple')\n",
    "plt.plot(results_thompson_multi['cumulative_reward'].sum(axis=1), label='Thompson Multi-Constraint', linewidth=2, color='brown')\n",
    "plt.plot(baseline_cumulative_reward_comp.sum(axis=1), label='Baseline (Optimal)', linestyle='--', linewidth=2, color='green')\n",
    "\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Cumulative Reward Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Regret Comparison with Theoretical Bounds\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(regret_ucb_sw, label='UCB-SW Regret', linewidth=2, color='blue')\n",
    "plt.plot(regret_ucb_multi, label='UCB Multi-Constraint Regret', linewidth=2, color='red')\n",
    "plt.plot(regret_primal_dual, label='Primal-Dual Regret', linewidth=2, color='orange')\n",
    "plt.plot(regret_thompson_sw, label='Thompson-SW Regret', linewidth=2, color='purple')\n",
    "plt.plot(regret_thompson_multi, label='Thompson Multi-Constraint Regret', linewidth=2, color='brown')\n",
    "plt.plot(theoretical_regret_ucb_sw, label='Theoretical Bound UCB-SW O(T^(2/3)Â·log T)', linestyle='--', alpha=0.7, color='lightblue')\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.7)\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Regret')\n",
    "plt.title('Regret Comparison with Theoretical Bounds')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Regret/T Comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "t_vals = np.arange(len(regret_ucb_sw)) + 1\n",
    "plt.plot(regret_ucb_sw/t_vals, label='UCB-SW Regret/T', linewidth=2, color='blue')\n",
    "plt.plot(regret_ucb_multi/t_vals, label='UCB Multi-Constraint Regret/T', linewidth=2, color='red')\n",
    "plt.plot(regret_primal_dual/t_vals, label='Primal-Dual Regret/T', linewidth=2, color='orange')\n",
    "plt.plot(regret_thompson_sw/t_vals, label='Thompson-SW Regret/T', linewidth=2, color='purple')\n",
    "plt.plot(regret_thompson_multi/t_vals, label='Thompson Multi-Constraint Regret/T', linewidth=2, color='brown')\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.7)\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('Regret/T')\n",
    "plt.title('Average Regret Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison statistics\n",
    "print(f\"\\n--- Detailed Performance Analysis ---\")\n",
    "print(f\"Final Regret:\")\n",
    "print(f\"  UCB-SW: {regret_ucb_sw[-1]:.2f}\")\n",
    "print(f\"  UCB Multi-Constraint: {regret_ucb_multi[-1]:.2f}\")\n",
    "print(f\"  Primal-Dual: {regret_primal_dual[-1]:.2f}\")\n",
    "print(f\"  Thompson-SW: {regret_thompson_sw[-1]:.2f}\")\n",
    "print(f\"  Thompson Multi-Constraint: {regret_thompson_multi[-1]:.2f}\")\n",
    "\n",
    "print(f\"\\nFinal Regret/T:\")\n",
    "print(f\"  UCB-SW: {regret_ucb_sw[-1]/T:.4f}\")\n",
    "print(f\"  UCB Multi-Constraint: {regret_ucb_multi[-1]/T:.4f}\")\n",
    "print(f\"  Primal-Dual: {regret_primal_dual[-1]/T:.4f}\")\n",
    "print(f\"  Thompson-SW: {regret_thompson_sw[-1]/T:.4f}\")\n",
    "print(f\"  Thompson Multi-Constraint: {regret_thompson_multi[-1]/T:.4f}\")\n",
    "\n",
    "print(f\"\\nTheoretical vs Actual Regret Ratios:\")\n",
    "print(f\"  UCB-SW: {regret_ucb_sw[-1]/theoretical_regret_ucb_sw[-1]:.3f}\")\n",
    "print(f\"  UCB Multi-Constraint: {regret_ucb_multi[-1]/theoretical_regret_ucb_multi[-1]:.3f}\")\n",
    "print(f\"  Primal-Dual: {regret_primal_dual[-1]/theoretical_regret_primal_dual[-1]:.3f}\")\n",
    "print(f\"  Thompson-SW: {regret_thompson_sw[-1]/theoretical_regret_thompson_sw[-1]:.3f}\")\n",
    "print(f\"  Thompson Multi-Constraint: {regret_thompson_multi[-1]/theoretical_regret_thompson_multi[-1]:.3f}\")\n",
    "\n",
    "print(f\"\\nPairwise Revenue Comparisons:\")\n",
    "print(f\"  UCB Multi vs UCB-SW: {results_ucb_multi['cumulative_reward'][-1].sum() - results_ucb_sw['cumulative_reward'][-1].sum():.2f}\")\n",
    "print(f\"  Primal-Dual vs UCB-SW: {results_primal_dual['cumulative_reward'][-1].sum() - results_ucb_sw['cumulative_reward'][-1].sum():.2f}\")\n",
    "print(f\"  Primal-Dual vs UCB Multi: {results_primal_dual['cumulative_reward'][-1].sum() - results_ucb_multi['cumulative_reward'][-1].sum():.2f}\")\n",
    "print(f\"  Thompson-SW vs UCB-SW: {results_thompson_sw['cumulative_reward'][-1].sum() - results_ucb_sw['cumulative_reward'][-1].sum():.2f}\")\n",
    "print(f\"  Thompson Multi vs Thompson-SW: {results_thompson_multi['cumulative_reward'][-1].sum() - results_thompson_sw['cumulative_reward'][-1].sum():.2f}\")\n",
    "\n",
    "print(f\"\\nPercentage Improvements over UCB-SW:\")\n",
    "ucb_sw_total = results_ucb_sw['cumulative_reward'][-1].sum()\n",
    "print(f\"  UCB Multi-Constraint: {((results_ucb_multi['cumulative_reward'][-1].sum() - ucb_sw_total) / ucb_sw_total * 100):.2f}%\")\n",
    "print(f\"  Primal-Dual: {((results_primal_dual['cumulative_reward'][-1].sum() - ucb_sw_total) / ucb_sw_total * 100):.2f}%\")\n",
    "print(f\"  Thompson-SW: {((results_thompson_sw['cumulative_reward'][-1].sum() - ucb_sw_total) / ucb_sw_total * 100):.2f}%\")\n",
    "print(f\"  Thompson Multi-Constraint: {((results_thompson_multi['cumulative_reward'][-1].sum() - ucb_sw_total) / ucb_sw_total * 100):.2f}%\")\n",
    "\n",
    "print(f\"\\nInventory Utilization Efficiency:\")\n",
    "print(f\"  UCB-SW: {results_ucb_sw['inventory_consumed'] / B:.3f}\")\n",
    "print(f\"  UCB Multi-Constraint: {results_ucb_multi['inventory_consumed'] / B:.3f}\")\n",
    "print(f\"  Primal-Dual: {results_primal_dual['inventory_consumed'] / B:.3f}\")\n",
    "print(f\"  Thompson-SW: {results_thompson_sw['inventory_consumed'] / B:.3f}\")\n",
    "print(f\"  Thompson Multi-Constraint: {results_thompson_multi['inventory_consumed'] / B:.3f}\")\n",
    "\n",
    "# Performance ranking\n",
    "performance_ranking = sorted([\n",
    "    ('UCB-SW', results_ucb_sw['cumulative_reward'][-1].sum()),\n",
    "    ('UCB Multi-Constraint', results_ucb_multi['cumulative_reward'][-1].sum()),\n",
    "    ('Primal-Dual', results_primal_dual['cumulative_reward'][-1].sum()),\n",
    "    ('Thompson-SW', results_thompson_sw['cumulative_reward'][-1].sum()),\n",
    "    ('Thompson Multi-Constraint', results_thompson_multi['cumulative_reward'][-1].sum())\n",
    "], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nPerformance Ranking (by total reward):\")\n",
    "for i, (name, reward) in enumerate(performance_ranking, 1):\n",
    "    print(f\"  {i}. {name}: {reward:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
