{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd4ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy import optimize \n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "\n",
    "from UCB1_constrained import UCBLikeAgent \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64db230b",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticPricingEnvironment:\n",
    "    \"\"\"\n",
    "    Stochastic environment for dynamic pricing with customer valuation uncertainty.\n",
    "    \n",
    "    This environment simulates a single-product pricing scenario where:\n",
    "    - Customers have valuations drawn from a known distribution\n",
    "    - The probability of purchase depends on whether customer valuation >= price\n",
    "    - Each round represents one customer interaction\n",
    "    \"\"\"\n",
    "    def __init__(self, valuation_distribution):\n",
    "        \"\"\"\n",
    "        Initialize the pricing environment.\n",
    "        \n",
    "        Args:\n",
    "            valuation_distribution: A scipy.stats distribution representing customer valuations\n",
    "        \"\"\"\n",
    "        self.valuation_dist = valuation_distribution\n",
    "        \n",
    "    def demand_probability(self, price):\n",
    "        \"\"\"\n",
    "        Calculate the theoretical probability that a customer purchases at given price.\n",
    "        \n",
    "        This is the complement of the CDF: P(valuation >= price) = 1 - F(price)\n",
    "        where F is the cumulative distribution function of customer valuations.\n",
    "        \n",
    "        Args:\n",
    "            price: The price to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            Probability that a randomly drawn customer will purchase at this price\n",
    "        \"\"\"\n",
    "        return 1 - self.valuation_dist.cdf(price)\n",
    "\n",
    "    def simulate_round(self, price):\n",
    "        \"\"\"\n",
    "        Simulate one customer interaction at the given price.\n",
    "        \n",
    "        Args:\n",
    "            price: The price offered to the customer\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (sale_made, revenue) where:\n",
    "                - sale_made: 1 if customer purchased, 0 otherwise\n",
    "                - revenue: price if sale was made, 0 otherwise\n",
    "        \"\"\"\n",
    "        # Draw a random customer valuation from the distribution\n",
    "        valuation = self.valuation_dist.rvs()\n",
    "        \n",
    "        # Customer purchases if their valuation >= price\n",
    "        sale_made = 1 if valuation >= price else 0\n",
    "        \n",
    "        # Revenue is price if sale was made, 0 otherwise\n",
    "        revenue = sale_made * price\n",
    "        \n",
    "        return sale_made, revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c92ac7a",
   "metadata": {},
   "source": [
    "# Theoretical Optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68700f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the optimal solution\n",
    "def compute_clairvoyant(prices, environment, T, P):\n",
    "    \"\"\"\n",
    "    Compute the optimal (clairvoyant) pricing strategy with full information.\n",
    "    \n",
    "    This function solves the linear program that an oracle with perfect knowledge\n",
    "    of the demand probabilities would solve:\n",
    "    \n",
    "    maximize: sum_i gamma_i * price_i * demand_prob_i\n",
    "    subject to: sum_i gamma_i * demand_prob_i <= P/T  (inventory constraint)\n",
    "               sum_i gamma_i = 1                      (probability constraint)\n",
    "               gamma_i >= 0                          (non-negativity)\n",
    "    \n",
    "    Args:\n",
    "        prices: List of available prices\n",
    "        environment: StochasticPricingEnvironment to get true demand probabilities\n",
    "        T: Time horizon\n",
    "        P: Total inventory\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (optimal_distribution, optimal_value, demand_probabilities, expected_revenues)\n",
    "    \"\"\"\n",
    "    # Compute true demand probabilities for each price\n",
    "    buying_probabilities = np.array([environment.demand_probability(p) for p in prices])\n",
    "    \n",
    "    # Expected revenue per selection for each price\n",
    "    exp_reward= prices * buying_probabilities\n",
    "    \n",
    "    # Set up linear program (convert maximization to minimization)\n",
    "    c = -exp_reward  # Negate for minimization\n",
    "    \n",
    "    # Inventory constraint: expected consumption rate <= inventory rate\n",
    "    A_ub = [buying_probabilities]\n",
    "    b_ub = [P / T]\n",
    "    \n",
    "    # Probability constraint: sum of probabilities = 1\n",
    "    A_eq = [np.ones(len(prices))]\n",
    "    b_eq = [1]\n",
    "    \n",
    "    # Solve the linear program\n",
    "    res = optimize.linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq,method='highs', \n",
    "                          bounds=[(0, 1) for _ in range(len(prices))])\n",
    "    \n",
    "    gamma = res.x  # Optimal price distribution\n",
    "    optimal_value = -(res.fun)  # Optimal expected revenue per round\n",
    "    \n",
    "    return gamma, optimal_value, buying_probabilities, exp_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129cb10b",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec1ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulator(\n",
    "    T,\n",
    "    valuation_dist,\n",
    "    env_config,\n",
    "    agent_params,\n",
    "    n_simulations=1,\n",
    "    verbose=True\n",
    "):\n",
    "    selected_prices = []\n",
    "    revenues = []\n",
    "    sales = []\n",
    "    cumulative_revenue = []\n",
    "    total_revenue = 0\n",
    "    best_prices = []\n",
    "    first_inventory_empty = None\n",
    "\n",
    "    # === Compute theoretical optimum once before all simulations ===\n",
    "    env_theoretical = StochasticPricingEnvironment(\n",
    "        valuation_distribution=valuation_dist\n",
    "    )\n",
    "    opt_dist, opt_value, true_purchase_probs, true_rewards = compute_clairvoyant(\n",
    "        prices=agent_params['prices'],\n",
    "        environment=env_theoretical,\n",
    "        T=agent_params['T'],\n",
    "        P=agent_params['P']\n",
    "    )\n",
    "    optimal_idx = np.argmax(opt_dist)\n",
    "    #optimal_revenue = np.dot(agent_params['prices'], opt_dist)\n",
    "    #optimal_revenue = np.dot(true_rewards, opt_dist)\n",
    "    optimal_revenue=opt_value\n",
    "    for sim in range(n_simulations):\n",
    "        \n",
    "        print(f\"\\n=== Running UCB1 Pricing Simulation #{sim + 1} for {T} rounds ===\")\n",
    "\n",
    "        # Create environment and agent\n",
    "        env = StochasticPricingEnvironment(\n",
    "            valuation_distribution=valuation_dist\n",
    "        )\n",
    "        agent = UCBLikeAgent(**agent_params)\n",
    "        #agent = ThompsonSamplingAgent(P =agent_params['P'], T=T, prices=agent_params['prices'], rho_penalty=agent_params['rho_penalty'])\n",
    "\n",
    "        for t in range(T):\n",
    "            if agent.remaining_inventory < 1 and first_inventory_empty is None:\n",
    "                first_inventory_empty = t\n",
    "                print(f\"Inventory empty for the first time at round {t}\")\n",
    "                print(\"No more products in the inventory\")\n",
    "\n",
    "            price = agent.select_price()\n",
    "            price_idx = agent.current_price_idx\n",
    "\n",
    "            if agent.remaining_inventory <= 0:\n",
    "                sale_made = False\n",
    "                revenue = 0\n",
    "            else:\n",
    "                sale_made, revenue = env.simulate_round(price)\n",
    "\n",
    "            agent.update(revenue, sale_made)\n",
    "\n",
    "            selected_prices.append(price)\n",
    "            revenues.append(revenue)\n",
    "            sales.append(sale_made)\n",
    "            total_revenue += revenue\n",
    "            cumulative_revenue.append(total_revenue)\n",
    "                    # Print progress occasionally\n",
    "            if verbose and (t + 1) % (T // 10) == 0:\n",
    "                remaining_inventory = np.sum(agent.remaining_inventory)\n",
    "                print(f\"Round {t + 1:4d}: Revenue = {revenue:6.2f}, \"\n",
    "                    f\"Cumulative = {total_revenue:8.2f}, \"\n",
    "                    f\"Remaining inventory = {remaining_inventory:.0f}\")\n",
    "\n",
    "\n",
    "\n",
    "        best_price, best_avg_revenue = agent.get_best_price()\n",
    "        best_prices.append(best_price)\n",
    "\n",
    "        print(f\"\\nSimulation completed!\")\n",
    "        print(f\"Total revenue: {total_revenue:.2f}\")\n",
    "        print(f\"Average revenue per round: {total_revenue / T:.2f}\")\n",
    "        print(f\"Agent's best price: {best_price} (avg revenue: {best_avg_revenue:.2f})\")\n",
    "        print(f\"Theoretical optimal: {opt_dist} (expected revenue: {optimal_revenue:.2f})\")\n",
    "\n",
    "    price_to_idx = {p: i for i, p in enumerate(agent_params['prices'])}\n",
    "    price_indices = [price_to_idx[p] for p in selected_prices if not np.isnan(p)]\n",
    "    price_counts = np.bincount(price_indices, minlength=len(agent_params['prices']))        \n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n=== Simulation Results ===\")\n",
    "        print(\"\\nPrice selection frequency:\")\n",
    "        for i, (price, count) in enumerate(zip(agent_params['prices'], price_counts)):\n",
    "            percentage = 100 * count / T\n",
    "            marker = \" ← OPTIMAL\" if i == optimal_idx else \"\"\n",
    "            print(f\"  Price {price:2}: {count:4d} times ({percentage:5.1f}%){marker}\")\n",
    "\n",
    "    return {\n",
    "        'price_counts': price_counts,\n",
    "        'selected_prices': selected_prices,\n",
    "        'revenues': revenues,\n",
    "        'sales': sales,\n",
    "        'cumulative_revenue': cumulative_revenue,\n",
    "        'best_prices': best_prices,\n",
    "        'first_inventory_empty': first_inventory_empty,\n",
    "        'total_revenue': total_revenue,\n",
    "        'opt_dist': opt_dist,\n",
    "        'optimal_idx': optimal_idx,\n",
    "        'optimal_revenue': optimal_revenue,\n",
    "        'agent': agent  # Add the final agent state\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36927bd1",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d59893",
   "metadata": {},
   "outputs": [],
   "source": [
    "T=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f85655",
   "metadata": {},
   "source": [
    "# Low Budget 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f838b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ceece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Configuration\n",
    "env_config = {\n",
    "    'valuation_mean': 0.5,  # Average customer valuation\n",
    "    'valuation_std': 0.1,   # Standard deviation of customer valuations\n",
    "    'demand_noise_std': 0.005  # Noise in demand probability\n",
    "}\n",
    "\n",
    "## Create a normal distribution for customer valuations\n",
    "valuation_dist = stats.norm(loc=env_config['valuation_mean'], scale=env_config['valuation_std'])\n",
    "\n",
    "#valuation_dist=stats.uniform(0,1)\n",
    "\n",
    "\n",
    "inventory = T*budget # Inventory constraint\n",
    "\n",
    "n_prices=4\n",
    "\n",
    "# Calculate epsilon for exploration\n",
    "epsilon = inventory**(-1/3)  # Epsilon for UCB exploration\n",
    "\n",
    "agent_params = {\n",
    "    'P': inventory,  # inventory constraint\n",
    "    'T': T,  # number of rounds\n",
    "    #'prices': [0.0,0.1,0.2, 0.3, 0.4, 0.5, 0.6,0.7,0.8,0.9,1],  # set of prices \n",
    "    'prices': np.linspace(0.1, 0.9, n_prices+2),  # set of prices \n",
    "    'confidence_bound': 1,  # UCB exploration parameter (reduced for better performance)\n",
    "    'rho_penalty': 1,  # Penalty factor for inventory constraint (increased to allow more sales)\n",
    "    'use_pen_rho': False  # Use rho penalty for inventory constraint\n",
    "}\n",
    "\n",
    "agent = UCBLikeAgent(**agent_params)\n",
    "\n",
    "#print(f\"Customer valuation distribution: Normal(μ={env_config['valuation_mean']}, σ={env_config['valuation_std']})\")\n",
    "print(f\"Number of price options: {len(agent_params['prices'])}\")\n",
    "print(f\"UCB confidence bound: {agent_params['confidence_bound']}\")\n",
    "print(f'Inventory contrain: ',agent_params['P'])\n",
    "print(f'Number of rounds:',agent_params['T'])\n",
    "\n",
    "# set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "results = run_simulator(\n",
    "    T=T,\n",
    "    valuation_dist=valuation_dist,\n",
    "    env_config=env_config,\n",
    "    agent_params=agent_params,\n",
    "    n_simulations=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "selected_prices = results['selected_prices']\n",
    "revenues = results['revenues']\n",
    "sales = results['sales']\n",
    "cumulative_revenue = results['cumulative_revenue']\n",
    "best_prices = results['best_prices']\n",
    "first_inventory_empty = results['first_inventory_empty']\n",
    "total_revenue = results['total_revenue']\n",
    "price_counts = results['price_counts']\n",
    "agent = results['agent']  # Get the actual agent used in simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d4624",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, opt_value, _, _ = compute_clairvoyant(\n",
    "    prices=agent_params['prices'],\n",
    "    environment=StochasticPricingEnvironment(valuation_distribution=valuation_dist),\n",
    "    T=agent_params['T'],\n",
    "    P=agent_params['P']\n",
    ")\n",
    "\n",
    "baseline_reward_invetory = [opt_value * (t + 1) for t in range(T)]\n",
    "regret_invetory = np.array(baseline_reward_invetory) - cumulative_revenue\n",
    "\n",
    "t_vals = np.arange(1, T + 1)\n",
    "theoretical_bound = np.sqrt(np.log(t_vals) * t_vals)\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cumulative_revenue, label=\"Cumulative Reward\")\n",
    "plt.plot(baseline_reward_invetory, label=\"Baseline Reward\", linestyle=\"--\")\n",
    "t_values = np.arange(T)\n",
    "plt.fill_between(t_values, baseline_reward_invetory-np.sqrt(t_values), baseline_reward_invetory+np.sqrt(t_values), color='gray', alpha=0.3, label=\"Regret area\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(f\"Reward Over Time (UCB1 Agent), Budget: {budget*100}%\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(regret_invetory, label=\"Regret\", color='red')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.plot(theoretical_bound, label=\"Theoretical Regret Bound\", linestyle=\"--\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.title(f\"Regret Over Time (UCB1 Agent), Budget: {budget*100}%\")\n",
    "plt.legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68369b5b",
   "metadata": {},
   "source": [
    "# Mid budget 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a482d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "budget=0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ab137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Configuration\n",
    "env_config = {\n",
    "    'valuation_mean': 0.5,  # Average customer valuation\n",
    "    'valuation_std': 0.1,   # Standard deviation of customer valuations\n",
    "    'demand_noise_std': 0.005  # Noise in demand probability\n",
    "}\n",
    "\n",
    "## Create a normal distribution for customer valuations\n",
    "valuation_dist = stats.norm(loc=env_config['valuation_mean'], scale=env_config['valuation_std'])\n",
    "\n",
    "#valuation_dist=stats.uniform(0,1)\n",
    "\n",
    "\n",
    "inventory = T*budget # Inventory constraint\n",
    "\n",
    "n_prices=4\n",
    "\n",
    "# Calculate epsilon for exploration\n",
    "epsilon = inventory**(-1/3)  # Epsilon for UCB exploration\n",
    "\n",
    "agent_params = {\n",
    "    'P': inventory,  # inventory constraint\n",
    "    'T': T,  # number of rounds\n",
    "    #'prices': [0.0,0.1,0.2, 0.3, 0.4, 0.5, 0.6,0.7,0.8,0.9,1],  # set of prices \n",
    "    'prices': np.linspace(0.1, 0.9, n_prices+2),  # set of prices \n",
    "    'confidence_bound': 1,  # UCB exploration parameter (reduced for better performance)\n",
    "    'rho_penalty': 1,  # Penalty factor for inventory constraint (increased to allow more sales)\n",
    "    'use_pen_rho': False  # Use rho penalty for inventory constraint\n",
    "}\n",
    "\n",
    "agent = UCBLikeAgent(**agent_params)\n",
    "\n",
    "#print(f\"Customer valuation distribution: Normal(μ={env_config['valuation_mean']}, σ={env_config['valuation_std']})\")\n",
    "print(f\"Number of price options: {len(agent_params['prices'])}\")\n",
    "print(f\"UCB confidence bound: {agent_params['confidence_bound']}\")\n",
    "print(f'Inventory contrain: ',agent_params['P'])\n",
    "print(f'Number of rounds:',agent_params['T'])\n",
    "\n",
    "# set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "results = run_simulator(\n",
    "    T=T,\n",
    "    valuation_dist=valuation_dist,\n",
    "    env_config=env_config,\n",
    "    agent_params=agent_params,\n",
    "    n_simulations=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "selected_prices = results['selected_prices']\n",
    "revenues = results['revenues']\n",
    "sales = results['sales']\n",
    "cumulative_revenue = results['cumulative_revenue']\n",
    "best_prices = results['best_prices']\n",
    "first_inventory_empty = results['first_inventory_empty']\n",
    "total_revenue = results['total_revenue']\n",
    "price_counts = results['price_counts']\n",
    "agent = results['agent']  # Get the actual agent used in simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, opt_value, _, _ = compute_clairvoyant(\n",
    "    prices=agent_params['prices'],\n",
    "    environment=StochasticPricingEnvironment(valuation_distribution=valuation_dist),\n",
    "    T=agent_params['T'],\n",
    "    P=agent_params['P']\n",
    ")\n",
    "\n",
    "baseline_reward_invetory = [opt_value * (t + 1) for t in range(T)]\n",
    "regret_invetory = np.array(baseline_reward_invetory) - cumulative_revenue\n",
    "\n",
    "t_vals = np.arange(1, T + 1)\n",
    "theoretical_bound = np.sqrt(np.log(t_vals) * t_vals)\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cumulative_revenue, label=\"Cumulative Reward\")\n",
    "plt.plot(baseline_reward_invetory, label=\"Baseline Reward\", linestyle=\"--\")\n",
    "t_values = np.arange(T)\n",
    "plt.fill_between(t_values, baseline_reward_invetory-np.sqrt(t_values), baseline_reward_invetory+np.sqrt(t_values), color='gray', alpha=0.3, label=\"Regret area\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(f\"Reward Over Time (UCB1 Agent), Budget: {budget*100}%\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(regret_invetory, label=\"Regret\", color='red')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.plot(theoretical_bound, label=\"Theoretical Regret Bound\", linestyle=\"--\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.title(f\"Regret Over Time (UCB1 Agent), Budget: {budget*100}%\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52449ec2",
   "metadata": {},
   "source": [
    "# High Budget 80%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "budget=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Configuration\n",
    "env_config = {\n",
    "    'valuation_mean': 0.5,  # Average customer valuation\n",
    "    'valuation_std': 0.1,   # Standard deviation of customer valuations\n",
    "    'demand_noise_std': 0.005  # Noise in demand probability\n",
    "}\n",
    "\n",
    "## Create a normal distribution for customer valuations\n",
    "valuation_dist = stats.norm(loc=env_config['valuation_mean'], scale=env_config['valuation_std'])\n",
    "\n",
    "#valuation_dist=stats.uniform(0,1)\n",
    "\n",
    "\n",
    "inventory = T*budget # Inventory constraint\n",
    "\n",
    "n_prices=4\n",
    "\n",
    "# Calculate epsilon for exploration\n",
    "epsilon = inventory**(-1/3)  # Epsilon for UCB exploration\n",
    "\n",
    "agent_params = {\n",
    "    'P': inventory,  # inventory constraint\n",
    "    'T': T,  # number of rounds\n",
    "    #'prices': [0.0,0.1,0.2, 0.3, 0.4, 0.5, 0.6,0.7,0.8,0.9,1],  # set of prices \n",
    "    'prices': np.linspace(0.1, 0.9, n_prices+2),  # set of prices \n",
    "    'confidence_bound': 1,  # UCB exploration parameter (reduced for better performance)\n",
    "    'rho_penalty': 1,  # Penalty factor for inventory constraint (increased to allow more sales)\n",
    "    'use_pen_rho': False  # Use rho penalty for inventory constraint\n",
    "}\n",
    "\n",
    "agent = UCBLikeAgent(**agent_params)\n",
    "\n",
    "#print(f\"Customer valuation distribution: Normal(μ={env_config['valuation_mean']}, σ={env_config['valuation_std']})\")\n",
    "print(f\"Number of price options: {len(agent_params['prices'])}\")\n",
    "print(f\"UCB confidence bound: {agent_params['confidence_bound']}\")\n",
    "print(f'Inventory contrain: ',agent_params['P'])\n",
    "print(f'Number of rounds:',agent_params['T'])\n",
    "\n",
    "# set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "results = run_simulator(\n",
    "    T=T,\n",
    "    valuation_dist=valuation_dist,\n",
    "    env_config=env_config,\n",
    "    agent_params=agent_params,\n",
    "    n_simulations=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "selected_prices = results['selected_prices']\n",
    "revenues = results['revenues']\n",
    "sales = results['sales']\n",
    "cumulative_revenue = results['cumulative_revenue']\n",
    "best_prices = results['best_prices']\n",
    "first_inventory_empty = results['first_inventory_empty']\n",
    "total_revenue = results['total_revenue']\n",
    "price_counts = results['price_counts']\n",
    "agent = results['agent']  # Get the actual agent used in simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be3f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, opt_value, _, _ = compute_clairvoyant(\n",
    "    prices=agent_params['prices'],\n",
    "    environment=StochasticPricingEnvironment(valuation_distribution=valuation_dist),\n",
    "    T=agent_params['T'],\n",
    "    P=agent_params['P']\n",
    ")\n",
    "\n",
    "baseline_reward_invetory = [opt_value * (t + 1) for t in range(T)]\n",
    "regret_invetory = np.array(baseline_reward_invetory) - cumulative_revenue\n",
    "\n",
    "t_vals = np.arange(1, T + 1)\n",
    "theoretical_bound = np.sqrt(np.log(t_vals) * t_vals)\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cumulative_revenue, label=\"Cumulative Reward\")\n",
    "plt.plot(baseline_reward_invetory, label=\"Baseline Reward\", linestyle=\"--\")\n",
    "t_values = np.arange(T)\n",
    "plt.fill_between(t_values, baseline_reward_invetory-np.sqrt(t_values), baseline_reward_invetory+np.sqrt(t_values), color='gray', alpha=0.3, label=\"Regret area\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(f\"Reward Over Time (UCB1 Agent), Budget: {budget*100}%\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(regret_invetory, label=\"Regret\", color='red')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.plot(theoretical_bound, label=\"Theoretical Regret Bound\", linestyle=\"--\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Regret\")\n",
    "plt.title(f\"Regret Over Time (UCB1 Agent), Budget: {budget*100}%\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb9ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079b6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
